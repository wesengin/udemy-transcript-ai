# 1.1 Day 1 - Running Your First LLM Locally with Ollama and Open Source Models

This is an important moment.This is the beginning of an eight week adventure that you are not going to believe.Whether you are completely new to programming, or whether you're an experienced engineer, or whetheryou've already taken this course and you've seen I've updated it and you've come back to get the latest.I have something big in store for everybody.Let's dive in.My name is Editor Donna.I am the co-founder and CTO of AI startup Nebula.And before we get started, it's customary in these things to walk you through what we're going to getdone over the next eight weeks the objectives, the structure, the projects, and how we will positionourselves for success.Except we won't.I'm lying.That's that's not my jam.People that have done my courses before know that I like to start with action.Let's get right to it.In the first couple of minutes of this course, let's go and experiment with LMS, which is what it'sall about, and there'll be plenty of time to come back later and do that other stuff.So first things first, I want you to bring up a fresh browser window, your favorite browser, whateverit might be, and I want you to go to this website.It is github.com.Engineering, which is the repo for this course.And it's also linked in the course resources.It should be easy to find this is the repo.We're not going to do anything with it yet.I bring it up because if you scroll down on this first page, you get to see what's known as the Readmefor the course.And the Readme has everything you need to know.It's got links to other things, it's got links to my LinkedIn, which I'm going to go on and on about.It's got links to setup instructions.It also might look a bit different to what you see here, because I'm constantly updating the Readmeto have the very latest information there.And the first thing we're going to be doing is we're going to be following this instant gratificationsection here.And you should always look first and foremost at the documentation because that will be super up todate.So take a look through that and then we're going to get started with our first exploration of LMS.So I'm guessing that you, like most of the people on the planet, are familiar with ChatGPT, and youhave interacted with an LLM over ChatGPT, and perhaps also Claude and Gemini and others.And what we're about to do now is also interact with another AI, an LLM.But we're going to be doing it running on our computer.We're going to take an open source LLM built by someone else, and we're going to install it on ourmachines and interact with it there.And that's somehow a much more exciting proposition to be actually interacting with an AI that you arerunning yourself rather than on the cloud.And we're going to use this fabulous product called Olama to do it.And let's go and do that right now.And as a reminder, you'll be able to follow all of these instructions in the Readme in a section.That's called instant gratification.But I'm going to start by going to olama.It's spelled O and it's linked in the readme.And this is what it looks like.And you can do the same thing basically on a PC or on a mac or on Linux.It's going to work the same way on all three, which is fabulous.So Olama, which is named as sort of an homage to one of the first open source models called llama frommeta O llama is a product which runs on your computer, which can run a model and can run it in a veryfast, packaged, efficient way locally so that you can interact with it.And the first thing you're going to do is come here, you're going to go to the download button here,press download.Choose the system that you've got.Press download for your system.It will start to download in the usual way.And then once it's downloaded, you will open that up by clicking on it here and up it will come.If you're on a mac, you drag it from here into applications and it appears there.If you're on a PC, you also just install it the usual way.And then once you've done that, you launch it from your applications folder or from from its icon orfrom the start menu launch alarm.So it opens up in its own screen.And when you do that, a screen like this should pop up.It's on llamas user interface with a little chat UI, but we're not going to use it.You're now going to press the X button, the close button to to get rid of it, because we're goingto interact directly with llama on our computer, and we're going to do it using something called acommand prompt.So there's a different way to do this on a PC and a mac.And it's super simple and the instructions are in the Readme.You might be familiar with this.If you're on a mac, you go to applications and utilities and you double click on terminal.If you're on a PC, you go to the start menu and you type out PowerShell to open up a PowerShell.If you're on Linux, you know exactly what to do.But either way, follow the instructions to bring up a command line and then we will start talking toan LLM.And this is what I see.And you should see something pretty similar.This is on a mac.On a PC you would see the word PowerShell, but it's the same kind of thing.And the first thing I want you to do is type Olama.And you should see a sort of help screen with things you can do with Olama.If you don't see that, check out the readme.The other thing you can do just to check it should already be running.If you type Olama serve, which is the way to start it, it should complain address already in use whichis telling you it's already running.It's good.When we double clicked on it, we started it running.And if it if it tells you that it's that it started then then that's that's great too.Okay.So we have Olama and we have Olama running.So next we're actually going to run Olama.We're going to get it to download a model from the internet and run it on our computer.And you do that with a simple command Olama run.And then the name of the model that you want it to download.And we're going to pick a model made by Google that is called GEMA.It is the small cousin of Gemini, the flagship model, but GEMA is the one that they made freely availableto everybody.So we type out GEMA, it's about GE.We're going to use Gemma three, which is the latest version, and then put a colon and after the colonyou can choose what size you want.And we're going to choose a really small size 2.70 M, which means 270 million of these things calledparameters that we'll talk a lot about in the future.And that that might sound quite big.270 million of anything sounds pretty big, but most of most of the time these these things are measuredin the billions.So 270 million is in fact really, really tiny.And when I press enter it now downloads Gemma.Now this might take a couple of seconds for you.It's already downloaded for me.So it was there.It could take a couple of minutes.If you've got a slower internet connection, it should be pretty fast.And then once it's done, you will have a cursor like this.That means that you are ready to interact with this model running on your computer.So we should keep expectations low.It's a very small model, but still.Let's begin.I'm going to say hi there.My first message to an AI on my computer and it says hi there, how can I help you today?I'll say, my name is Ed and that's great, I'm Ed.Haha no, that's my name.That's perfect I'm Ed.So look, this is a very small model and you shouldn't have high expectations with it.What we're going to do now is pick a slightly bigger model and have hopefully a more robust conversationwith a larger model.Let's give it a shot.So now you should stop your conversation with Gemma.And the way you do that in that command line is you press Ctrl D that exits your conversation with GemmaControl.For people on A on a mac, it's not the same as command.Watch out for that Ctrl d.All right.And then let's go back to the website and you'll see that there's models in the main navigation here.If you click on that it comes up with a list of all the different open source models that you can usethrough Olama.And there are many of them.We were just playing with Gemma, three, and Gemma three.You can see it also comes in lots of different sizes if you just do.Gemma three you get this version here latest, which is 3.3GB.It's quite big.Uh, and there's also the one we were just using which is Gemma three 270 M which is much, much smaller.And there's a whole slew of different sizes.And you can try these out yourself if your computer is big enough to, to store them and then experimentand see how a larger model is able to have a more robust conversation.But for now, we're going to go to models, and I suggest that we pick from here.Let's go with Phi three from Microsoft.We're going to try Phi three latest.If your machine is big enough to take this, if you have 2.2GB free on your drive, and if it's fastenough then you'll see this two.But I'm going to give Phi three a shot.Let's see how it performs.So here we go.I type Olama Phi three.And the first time this will take a few minutes to download onto your computer depending on your networkand the strength of your computer.And the first thing I'm going to do is I'm going to say hi there.Let's see how it performs.Hello.How can I assist you today?Uh, tell me a fun fact.Uh, there we go.Here we go.Did you know that the Eiffel Tower in Paris is not as stable on its legs as people might think?Uh, slight parts of it to sway up to six feet.How about that?All right, tell me another.Okay.The world record for being able to eat the most hot dogs in an hour is held by Joey Chestnut.That I did think I knew as a New Yorker.This happens every year.Uh, so there we go.Some some interesting, fun facts.Hopefully you've had different fun facts from FY three.And if you're trying different models, you might get all sorts of different fun facts.Um, so this is this is the FY three model.And we're now going to try one more.So I do control D to stop the conversation just like that.And now I'm going to type Allama one.And now I'm going to pick the biggest model that will run on my computer.And this may not run on your computer, so.So don't worry if you need to have at least 16MB of Ram on your computer for this to run, and somethinglike 20GB free on your drive.There's no need to run this if you don't wish, but I want to show you what it's like to run the latestmodel from OpenAI, which is called GPT OS, which is the open source version of OpenAI's GPT model.So this is a big old model and it's going to download it for you.And that might take a fair bit of time.It's also going to really hammer my computer.So there it is.It is running.We have GPT OS.This is in fact the 20 billion parameter version.So it's again many times bigger than Phi three, which is itself 16 times bigger than Gemma three 270million.So this is a big model.We're running it locally.Let's interact with it.


================================================================================


# 1.2 Day 1 - Spanish Tutor Demo with Open-Source Models & Course Overview

So I'm now running with this powerful model GPT, and hopefully you're either doing the same or you'reusing a smaller model like Phi 3 or 1 of the gamma variants, or perhaps something from llama, whichis made by meta.And I'm going to try and use this for a true purpose.I'm going to say, um, hi.Uh, I'm trying to learn Spanish.I'm a total beginner.Please have a conversation.A con with me as my Spanish tutor.All right.Let's see how it does.And this is the kind of thing that you might see in a commercial app like Duolingo.First of all, notice that it starts by describing its thought process in this different color, inthis gray color here.And then we get the final answer.It says, Hola, soy espanol.I forgive my horrible accent.I assure you I am significantly better with Llms than I am with Spanish.Uh, all right.And you can see it says uh, yo or LA and it wants me to respond.And so I could say, Hola.Um, may I add, uh, uh, and then, uh, you could see that it says data.So it will gradually take you through the process of having a beginner level conversation in Spanish.And the great thing about this is that this is quite similar to the kind of functionality that you mightfind in an app.So I wanted to do this to show you that right off the bat, you can start getting actual value fromrunning an open source AI large language model locally on your computer.Uh, even if you speak terrible Spanish like me.And hopefully I'll get better over time.Next time I record this, I'm gonna have a great accent.Uh, so anyways, have fun with it.Use this.Try out different models.Try exploring how the larger size models are able to be more intelligent in how they interact with you.See these thinking type models?We'll be talking a lot more about them in the coming days.Get some exposure to this and enjoy the fact that you are harnessing the power of a large language modellocally on your computer.Now, I'm hoping you've had a really enjoyable rabbit hole experience of trying out different models,experimenting, and being enamored by the power of these things.But I have to pull you back.It is time now for us to talk about everything that I've got in store for you for the next eight weeks,and this is a timeline that I'll be showing you frequently.It's a timeline that shows you where you start over on the left, and the eight weeks that I have instore for you.Tomorrow I'm going to go through this in a lot more detail, but there's going to be this week, whichwill be more foundational.Next week is about frontier models.The week after is about open source models.Then it's about selecting the right model for the task at hand.Then it will be about rag.We'll go deep on rag, one of the hottest topics in AI.Then we will fine tune a frontier model and then fine tune an open source model.And finally, I will introduce a genetic AI in the final week of our eight week series.And at that point, you'll be able to wave a flag and say, I am now an LLM engineer.I have reached this milestone to mastery, and I've built a comprehensive curriculum for you throughoutthe next eight weeks.And one of the things I love about it is it really builds and builds and builds.We start by covering a range of different models, different llms, open source and closed source.Then we cover a lot of toolkits, libraries, frameworks that use those LMS things like hugging facegradio for user interfaces, lang chain weights and biases, modal com, and then building on top ofthat, we apply different techniques to getting the most performance out of LMS techniques like Rag,fine tuning and agents.But by far the best thing about this course is the building.I'm a big believer in learning by doing.I think the best way to learn is by building great commercial projects, and that we will do from multimodalprojects to commercial consumer applications, to something that's more technical about porting codefrom one language to another, to stuff that involves vectors and training with lots of charts.And finally, a whopper of a commercial project at the end.So be prepared for lots of juicy building.And you may have noticed that I have other courses out there.I'm wondering how this fits in.Or maybe you've taken some of those other courses, and the answer is that this course is complementaryto the others, and you can take them in any order.There is perhaps a natural order.This course is quite foundational and lays the groundwork, so it might be better to take this first,at least the first couple of weeks of it.But it doesn't matter if you've taken up on the other ones.A gigantic AI, a companion course which is all about applying this kind of LLM functionality to differentAgentic use cases and frameworks.And you can see I've put that there just slightly higher up, because it's it's sort of taking it tomaybe the next step, but you could take identify first and then LLM engineering if you preferred AIin production is a very different course that I've got that takes whatever you build and allows youto deploy it at scale in a production setting on a cloud platform like AWS.And then I also have a briefing for founders and leaders, which is intended for a non-technical audience.It's commercial first and foremost, but I will say actually that I strongly recommend that even technicalpeople do consider taking this briefing, because I think it's a super power for tech.People like us to also have a good sense of the commercial applications of Llms.And so again, the course is complement each other, take them in whatever order you like.Maybe it's most natural to start with LM engineering, but it doesn't matter if you've already donethe others and you're coming to it now.And most importantly, I've designed this curriculum and this course so that it will appeal to everyone.If you are a complete beginner and you're coming to AI for the first time, prepare to be astonished.But this will be a nice smooth on ramp for you.But also, if you're a pro, then sure, some of the stuff in weeks one and two are going to be thingsyou already know.Absolutely.But I'm hoping to to still tease you with some interesting new content to there'll be something thatmight surprise you.Something for everyone.That's the plan.All right, just before we get into it, let me now quickly introduce myself.Yes.So to convince you that I'm actually qualified to talk to you about this.My name is Ed Donner.I am the co-founder and CTO of an AI startup called Nebula.You can check us out at Nebula.And I spent most of my career at JP Morgan, where I started out in London, which is where I'm fromoriginally, and then spent some time in Tokyo and then moved to NYC, which is where I am at rightnow.And most recently at JP Morgan, I was a managing director, running a technology team of about 300software engineers and scientists.And before Nebula, I was the founder and CEO of an AI startup called untapped.And this picture is this is a magical moment for me.This is a picture from a Times Square billboard on the day that the acquisition of my AI startup wasannounced a couple of years ago, which was really great.Uh, and in case you don't believe me, I can prove it.If you squint and you look super carefully.Near the bottom right of that picture is, in fact, a picture of me in there, just to prove that itwas, in fact, me, uh, with this startup.But that was my my moment in Times Square.And I think it's obligatory with these things to have some kind of personal life picture.So here's me in front of a plane that I just flew.And you might think I'm showing this because I'm very skilled at flying planes, but but quite on thecontrary, uh, my my great talent when it comes to all things LM is only surpassed by my complete inabilityto do anything that requires hand-eye coordination.So if you're walking onto a plane and you glance into the cockpit and you see that it's me there holdingthe yoke, then you want to be looking for a parachute really quickly.Uh, but but if you've come to some online course and it's on LMS and you see that it's this guy givingthe talk on LMS, then congratulations.That is my sweet spot.You've come to the right place.Let's get on with it.I will just mention also that that one cool thing about that bottom right hand picture is that I actuallylive right there as well.It's strangely not many people live in Times Square, but I live like a block from Times Square, rightbehind where that hard Rock cafe guitar is.I'm kind of behind there, and I'm looking out at that right now.So.So you're sort of picture me looking out where those two lines meet right now, and the other placewhere I am is on LinkedIn.And I gotta warn you, I'm gonna go on about this, like, a hundred times.I love connecting with people on LinkedIn.Some people are coy about connecting on LinkedIn.They're like, you can tell me where we met and I might connect with you.I'm not like that at all.If if you want to connect with me, then I'm thrilled.Uh, send me an invite and I'll connect.You can say hi and I'll say hi back.It's not an agent.It'll really be me.Uh, you could also just, uh, just just send the connect and I'll accept it.Um, uh, whilst it's not an agent, I do reserve the right occasionally to copy and paste some of myanswers, because I do get through quite a lot of them.But I will always for sure interact with you and I love it.And also, if you post information about the course, if you post your progress through the course orprojects that you build, and you tag me on your LinkedIn post, that I will come in and weigh in andadd to it.And I find it such a fabulous way to amplify your achievement, and also to get other people in thecommunity, other data scientists, seeing it and responding as well.So please do, do do that liberally.It sometimes takes me a few days, but I will get there.Tag me and let me weigh in.And so, in summary, I'm a software engineer, a data scientist, an entrepreneur, and a leader inthe field of AI.But there's something else I do as well.In 2024, a friend of mine persuaded me to make a Udemy course teaching other people about AI.And it was, of course, the first version of this course, LM engineering, and I made a few otherssince, and I've been so lucky that they've been quite successful.And there's now more than a quarter of a million people taking my Udemy courses.And it's been a total joy, and I love interacting with people on it.It's also such a great opportunity for everyone on it because as I say, you can post on LinkedIn,I can weigh in and it has such a multiplier effect in the community to get everyone involved in yourjourney.And that's why I urge you to post about it and enjoy the journey, enjoy the experience, and shareit with all of us.


================================================================================


# 1.3 Day 1 - Setting Up Your LLM Development Environment with Cursor and UV

So I have so much in store for you this week.We're going to start by building an LM solution today.We're then going to talk about the whole of the next eight weeks to come.We're going to compare models at the frontier of what's possible.Today.I'm going to give you some early intuitions for what Transformers actually are, and will end by buildinga commercial product in day five.And so for today, for the rest of today, we are continuing our instant gratification theme, buildingyour first LM solution using a frontier model, using a model at the forefront of what's possible.That's what people call frontier models, the very latest, strongest models on the planet.But first.But first we have some drama to get through.You and I, we have some drama.We have to set up your environment, set up the environment that you'll be using for working over thenext eight weeks.And let me tell you, setting up an environment to operate with some of the most groundbreaking datascience available on the planet is not an easy task, and it can be frustrating.And I would say about 80% of the questions that I get relates to the next ten minutes.The next time we're going to spend together, building your environment.So I need to ask you to have a lot of patience.We're going to get through it.We're going to get you a brilliant environment that's set up for everything that we're going to do.But it is a bit traumatic to start with, but let's get to it.So look, there are five steps to setting up your environment.And we're going to do all five together.And it's going to be fine.The first step is sort of the foundations.We're going to clone the repo as it's called.We're just copying all the code from, from GitHub to, to run it locally.We're going to use cursor as our development environment.If you want to use something else you absolutely can I suggest cursor but you can use anything.And then we're going to open the project and step one will be done.Step two we are going to use a fantastic platform called UV.UV is an alternative to platforms like Anaconda that you might have heard about.It's something which allows you to manage your Python environment.Make sure you have all of the right dependencies, all the packages we rely on, all installed, andUV has something that's taken the community by storm because it is bulletproof.It is easy and it is fast.It's all three you normally have to like pick two of the three, but no, you get all three.It's so wonderful and you're going to love it.Then we're going to create an OpenAI API key away for us to call OpenAI.Now that involves spending a little bit of money.It involves putting in $5 and you don't need to.There is a free alternative.It's fully documented.We will do that too.I know some people get get frustrated thinking there's going to be a spend.If you don't want to spend, you don't need to spend anything.And we will cover spend very clearly at the time.That's step three.You will then add that key to a file called the EMV file.And there's so many traps here.If you spell the name of the file wrong.If you spell the name of the key wrong, any of these things, disaster will ensue.At least disaster in the form of no environment.So you'll need to get that right.But I will point it out to you.We'll make absolutely sure it's right.And then finally, step five.There is no step five.Step five is we just get started.We are ready to go.Now look, most of the time this all goes super smoothly and you're going to be up and running in notime.You're going to be wondering what all the fuss is about.But just sometimes you hit problems and if that happens to you, then there's a troubleshooting notebookthat I will show you, which will walk you through and help you fix any common problems.ChatGPT or Claude or any llms.They're really good at helping solve problems.And you should definitely, Make sure you check what they say they have.You know they often need to be verified.Don't just fall naturally into doing whatever they say.Always check what they do, but they're really good at finding problems.And then I am here to help.You can always reach out to me in the platform or by email, or the best ways to reach out to me andget me to help.I will say, though, that the documentation that I put together for this course is really thorough,super comprehensive.There's great answers to most problems.The setup instructions linked in the Readme are very, very thorough and whilst I always answer questions,I will tell you that probably 60% of the questions that I get are answered directly and immediatelyin the documentation, and I'm fine with that.I don't mind being asked, but all I will say is that it's much faster if you look in the documentationfirst, because if the answer's there, that's going to be your fastest way to get there.And most students hit the same problems, they are covered there in the answers.The famous windows 260 character issue, things like that.They're all covered in the doc, so check out the docs.But if in doubt, reach out to me.I'm here to help.Okay, okay.With that, it is time for our first lab.We're going to get started building this environment and here's how it's going to work.The next video in the series is going to be for PC people.I'm going to walk you through as a PC user, how to set up your environment.The video after that will be for Mac people.I will walk Mac people through what to do with step by step instructions for Mac, and if you're a Linuxperson, then follow the Mac instructions.They'll be good enough for you and you know your way around this all anyway.So PC instructions coming next, followed by Mac instructions.Pick the right video and then once you've gone through your setup and you're ready to go, I will seeeveryone in the video after that, which is when we build our first LM project.


================================================================================


# 1.4 Day 1 - Setting Up Your PC Development Environment with Git and Cursor

If you're a PC user, then welcome.You've come to the right place.If you're a mac user, then this video is not for you.You belong in the next one.Go over there.All right, PC people.Now, look, my first job out of college was working for IBM, the PC company.And so PCs are in my my roots.So you are my people.So let's get to working on setting up your environment.And it's going to be easy.I hope we're going to start by one more time.Going to the GitHub repo github.com slash engineering linked all over the place.And the main reason I show it to you is that if you scroll down, you'll see the contents of the readme,and that includes links to the setup instructions for you.And that's where you should go for the detailed walkthrough of everything we're going to do.And the setup instructions are the best.That's the best place to go.You can watch what I do to give give you a feel for the different screens, but the instructions willtake you through things step by step and will show you what could happen and have troubleshooting tipsand so on.For PCs in particular, there are a few gotchas that are well known and at the very top of the setupPC instructions I have these gotchas with ways to check that you don't have this problem and how tofix them, such as having VPNs running such as something called the windows 260 character limit, whichis really nasty.But there are ways to test to see whether you have that, and if so, exactly how to fix it.So please do take a look at that.And then when you're ready, let's get started with your environment.And so here we go.You start by going to the start menu.And up here you type PowerShell to bring up what's called PowerShell, the command prompt that we alreadydid with Obama.Now one thing to notice here you can just click on PowerShell to start the PowerShell.You can also right click and say Run as administrator, which gives you extra powers when you run it.If you have if you ever have issues that you don't have permissions to do something, the first thingto try is to come in and run it as administrator to see if that gets around the problem.So that's worth knowing.All right.But with that, we will open up a PowerShell.This is what it looks like.And the first thing that I want you to do is type the word git git and enter and see whether you getsomething like this.And that will tell you whether or not git is installed on your computer.Now, if you're not sure of the difference between git and GitHub, they sound very similar.They're not the same thing.If you're not sure, then in the repo there is a folder called guides.If you go into guides are some great, uh, sets of tutorials on different foundational topics, andone of them will explain to you what's git, what's GitHub, what's the difference, and so on.Please do review it.But let's say that you do know and you've come here.And if you did not get this this report so you don't have git installed, then follow the instructionsin the Readme.But basically you go to a browser.This is showing the setup guide.You go to this website and you can use this to Is to download git for windows and then have it installedon your computer.So follow these instructions, install it, run it locally, and then when you come back here, youmight need to restart your computer afterwards or at least open a new PowerShell.But once you've done that, you should be able to type git and see this command.And we're off to a fine start.Now, if you're not familiar with using the command line, if you don't know the basics of this, thenplease go and look in the guides folder again on the repo and read the guide on using the command line,just to make sure you're comfortable with the basics, with the difference between files and foldersand directories, stuff like that, so that the next few steps will make some sense to you.It's not important to be an expert, but at least know the foundations.Okay, so I'm going to clear this screen where we are right now.You can see we're in the C drive in my home directory users Ed.And it's very typical in these things to have a subdirectory that you create called projects, whichis where all of your projects belong.You might already have that, in which case you can type CD projects to change into that directory,but I don't.I haven't got one yet.I've only got my home directory.And so what I need to do is create a projects directory.So I'm going to type mkdir make a directory called projects.There we go.It's been made I'm still in C.Users add my home directory.So I type CD projects to go into this new directory.And if I look at it LS looks at the contents.It's empty.It's a new directory.It's in C users projects.Okay.The next thing we're going to do is an epically famous thing to do.It's called cloning a repo.And it's a command git clone which takes a copy of someone's repo, in this case my repo, and putsit locally on your computer.And that's what we're going to do.And we're going to put it right here in the projects directory.So if I go back to the browser, I'm now going to go back to GitHub.Hopefully you're comfortable with the difference between GitHub and git.I'm going to go to the LM engineering repo.So we're at github.com.Engineering.And there's this green button here for which there's code on it.If I hover over this make sure that Https is selected here.And it's this is the clone window.And I'm going to press that little copy URL to clipboard right here.And that is copying the URL of this repository into my my clipboard.I can now go back here back to my command line.And I'm going to type the command which is, which is going to do everything.I'm going to type git clone.And then I'm going to paste in that URL right there.And when I press enter bam it's going to take a few seconds.But it's now pulling the entire repository and bringing it here locally.And if I now do an LZ to to list the contents of C users Ed projects, you can see that there is a Cusers ed projects lem engineering.Okay that's exciting.So now we do see Lem engineering to go into this directory.See what's here.We do LZ.And there it is.We have the contents of our own repo.We've got it locally on our computer.We're ready to go.Okay.We're still in step one of the instructions.It's still step one.But step one is a is a juicy one.The next thing we're going to do is we're going to install cursor, which is a development environmentwhich is very popular because it uses AI to help, uh, fill in your code and it's at cursor just likethat.C you are.And of course it's linked in the, in the repo.Um, and it's the best way to code with AI it claims, will be the judges of that.And by the way, cursor has a free two week trial.And then there is it is a paid plan, at least as of today.But you don't need to pay for it if you don't want to.You can keep using it as an as an IDE, as development environment, paying for it or not.Up to you.You don't.You don't need to take advantage of the the the AI autocomplete feature.Also, if you prefer working with VS code cursor is basically the same thing.It's a fork of VS code, so.So you can use either for sure, and you can use any IDE that you like.You can use windsurf, you can use PyCharm whatever you like using.But you're going to press the download for windows button.Of course it's going to download it.You're then going to run that installer.There's a few things for you to accept.You have to set up an account.You have to to agree to something.And then once you've gone through all of those different screens, you should be ready to open up.Cursor.Okay.So we now go to the start menu and we choose cursor and type in cursor.It may also be an icon on your desktop, and then you can click on it to launch it.And cursor should come up like this the first time.It might ask you a few questions, but once it's all installed and ready, it should look somethinglike this.But maybe without the things right here.If it doesn't look like this, then you can go to file and choose new Window and then you should seeit looking like this.And the first thing to do is to press Open Project.Now a project in cursor is an important concept.It's the folder that the highest level directory that contains all of your work.In our case it's called LM engineering.That is also known as our project root.So what you sometimes call that.And so we want to open that folder.And it's important to get this right because if you open the wrong folder things are really confusingin cursor.So so get this right.So where is it.Well uh it's it's basically it's on our C drive in for me.It's in users editor and then it's under in my projects folder.So I find projects and there is LM engineering and I double click on that.So I'm now looking at the contents of LM engineering.And then I press select folder to mean this is our project root.This is where we're going to be.And it opens up.And you can see eight weeks of folders.On the left.You can see the word LM engineering in block capitals on the top left.And that is your sign that you've got the right project.Open this block capitals here telling you you've got the right project.You're in.Cursor is open.LM engineering is open.Congratulations.We are actually ready to work on this project okay.And with that that completes step one of the setup instructions.And I have good news.Step one is the only step that is PC dependent.The rest of the steps will be for both PC and Mac.So what I'm going to do now is I'm going to put you on hold.The next video is going to be for the Mac people.And then when we reconvene, we'll be able to finish everything off with the rest of the steps together.All right.So I'll see you in one video from this one.


================================================================================


# 1.5 Day 1 - Mac Setup- Installing Git, Cloning the Repo, and Cursor IDE

Okay.If you're a mac person, then congratulations, you've come to the right place.If you're a PC user, then you don't belong here.You're meant to be on the next video when we get back together again.So Mac people, I just told the PC people that they are my my people because I used to work for IBMa while ago.But but it's not true.I'm a mac person through and through.I've.I've owned Mac since I was a teenager.All through my life I've owned so many Macs.I've been programming on them since I was, since I was yay high.Uh, so this is.You are my people for sure.Don't listen to what I said.The other guys.Uh, yeah.I have no loyalty.Uh, yeah.No Macs all the way.Okay, so first step in the setup instructions is to tell you to go to the repo.Go to GitHub.Engineering.This link is all over the place.I want to remind you again that the best setup instructions are written in the repo.Follow the link to the Mac instructions.That's where I go through things step by step.That's what you should really follow.Use this video to see how I do it, but be sure to follow the instructions at the top of the instructions.I've got a few things I call the gotchas.The things to watch out for.Stuff like making sure you've got the latest Mac tools installed so there's a few things to watch outfor.So please do look at those gotchas as well.And there's there's common troubleshooting tips all the way through that should be your first port ofcall is to look at the docs.But now let's go through setting things up on the Mac.So the first thing we're going to be doing is talking about git and cloning the current repo.And if you're not sure about the difference between git and GitHub or you don't know what they are,then please do take a look.In the repo there's a folder called guides.Look within guides.I've got a guide I've written on what is git?What is GitHub?The difference between them.Be sure to look at that to get get your foundations on that okay.And then when you're done we're then going to open up a command prompt, a terminal and again instructionsin the repo.But the way you do it is you go to your finder, you go to applications, you look in utilities andthere's a utility called terminal.And you double click on that to launch a terminal.And here I've got one right here for us to work on.And uh, here we are.And the first thing that I'm going to do in this terminal is I'm going to type git minus minus versionto find out if I have git installed and what version it is, and I do and there it is.And if you don't have git installed then when you run that it should prompt you to install it.Uh, so so follow those instructions or look in the readme so that you definitely have git installedat this point.Also, if you're new to the command line, if you're not familiar with using a terminal in this way,there's another guide on that to look in the guides folder in the repo, look at the guide on the commandline and use this as an opportunity to build up some foundations, some confidence in things like filesand directories moving between them, copying them.All the basics in a command line just so that you know the lay of the land.And then we'll get going.Okay, I'm going to start by typing CD, just to see that I'm in my home directory to find out whatthat directory is, I type PWD and it tells me that I'm currently in slash users slash Z.That's my home directory.You won't be in user ed unless your name is also ed in, which is high, but you'll be in user slashyour name.So the next thing we're going to do is create a new directory called projects, where all of our projectsbelong.It's customary to have a directory that you call projects.You may already have one, in which case you'll be able to type CD projects and you'll go right there.You may not, in which case we'll need to create one, which is what we'll do now.And now you can of course use it in a different.You could have it within another directory like your documents directory.It's one thing.One little trap to watch out for is it's best not to have it in a directory that is part of your iClouddirectories, like documents or desktop.Try and avoid having it on your desktop or in documents.There's various reasons for that, but one of them is that those those folders are replicated to Apple'siCloud, and that can cause various problems with repose and slow things down.And it's not necessary because your repo is already replicated to GitHub.So recommendation is just to have it in your home directory.So I'm going to type mkdir make a directory called projects.There we go.We've done it.Now I can CD projects I can go into it.And if I now do PWD where am I.I am in users add projects just where I want to be.Okay, so the next thing I'm going to do is go back to the repo again.Here it is.This is the LM engineering repo on github.com.And there's this green code button.And I press on this and I make sure that Https is selected like that.And I'm going to copy this URL to the clipboard.It says copied.All right.So I've now copied the link to this URL.Okay I now go back to the terminal window again.Here it is.And now I type the legendary command git clone.That means I want to take a copy of someone else's repo.Well, that's my repo and have it locally on my computer git clone.And then you paste that link that we just copied.So we're saying git clone.And then this remote repo called LM engineering.And when I press enter it will take a few seconds.But it's now copying down the entire repository I prove it, I type ls to list the contents of projects.And there we see.We have one thing in here.It's called LM engineering and I can do CD, LM engineering, and then I can do an LZ again.And there is the contents of LM engineering.The repo.It has been cloned.It is now local on my computer.So far so good.Okay.The next thing we're going to do is still in step one.Step one is quite a big step.Step one we're going to go back to the browser again.And now I want you to go to cursors.See you are so for this for this uh project we're going to be using cursor as our IDE as our developmentenvironment.Now you don't need to use cursor if you don't want to.Cursor is a fork of VSCode.And if you use VSCode, everything will look basically identical.And you could also use anything else.You could use windsurf, you could use PyCharm, whatever you want.It's all good.But I'm going to use cursor.It's nice.I like the AI features.Uh, you have to pay for the AI features, but you get two weeks free trial, at least as of today ifyou're new to it.Otherwise it's a it's a paid plan, but you don't need to pay it.If you don't pay it, you just don't get the the cool AI autocomplete with frontier models, but it'scompletely up to you.Um, there's a download button here, download for Mac OS.I can press that button.It will then download it in the usual way, and then I can open it, I can launch it.And there will then be an installation process when I have to answer a few questions, set up an account,select what I want and you should just pick all the defaults, get it all set up, uh, and uh subscribeto it.Use the free trial of course.Uh, and then get things going so you can then launch cursor and be ready to go.And once you've installed cursor, you can launch it either from your applications folder in your finderwindow, or it might be an icon on your desktop.Uh, or it might be in your system, uh, tray.But you can then launch it and it should open and it should look like this.This is a sort of new start screen.And if it doesn't look like this, even after you've answered any first time questions, then there'sa file menu.Go to the file menu and choose New Window and this should come up just like this.You won't have this stuff here.You'll just have these buttons I imagine.And the first thing we're going to do is open a project.So a project in cursor refers to the top level folder known as the project root, which contains allof your code.In our case, it's LM engineering, the thing we just cloned.And that's what we're going to open as our project right now.Okay.So I come in here I click Open Project.I'm now in editor.So within editor I go into projects and here's LM engineering.And now this is a bit fiddly.You double click on LM engineering.So we're now looking at the contents of LM engineering.And now you press open.It's saying that's the folder that is our folder.And this pops up and this is everything.This is.This is the beginning of all of our stuff together.So the thing you want to look for, you want to check that you see the different weeks appearing hereas folders, weeks one to week eight.And you also want to check that in block capitals.On the top left is LM engineering.That's telling you that that we have opened the right folder.If you've got this chat thing over here you can get rid of this.This is the AI chat.One way to get rid of it is just to drag that over like that.But but this is the screen that I like to see.So here we are.We've opened the LM project in cursor.And that completes step one of our five step instructions.And now we have good news that the remaining steps are steps that we can do with our PC colleagues.At the same time, we can do them together because it's going to be identical.So I will now at this point say congratulations on step one complete.I will see you in the next video with everyone back together again for step two through step five.See you then.


================================================================================


# 1.6 Day 1 - Installing UV and Setting Up Your Cursor Development Environment

Welcome back to PC and Mac people.We have come together.Both of you have completed the first step of the setup instructions.We are now embarking on the remaining steps and they are hopefully super easy.You've got cursor up and running, you've got LM engineering in block capitals on the top left likethis.If you have this this chat screen over here, you can get rid of it by clicking here and just draggingover to the right till it goes away.So you're left with this.If you're not seeing the files on the left here, then you can go to the view menu and select explorer.And it will come up.And you should be right here.Now one thing you'll notice is that the some of the setup instructions, the readme should be here.You should be able to click on the readme and it comes up.But when it comes up it comes up looking like this.It's what they call markdown.It's a particular type of formatted text.The way to see it in all of its formatted glory is to right click on it and say Open Preview.And when you do that, you will see it like this in all its glory.And you can use this now.You don't need to go to the website anymore.You can use this to follow the instructions and to find all the links you need.You'll also have all the guides you'll have.Everything that was on GitHub is now locally on your computer right here.Congratulations.So we're going to start now by installing and using this thing called Uuv which is wonderful.And I already told you this, but it's wonderful because it's fast, it's bulletproof and it's easy.And and that's why we love it.It's it's taken the data science world by storm.It's now used in all sorts of ways.It's used for MCP, it's used by crew AI, it's used by by lots of people.And increasingly it's becoming the standard for these things.It's something which makes sure that we're all using exactly the same version of Python, and that wehave exactly the same dependencies and that they work on our computer, and it's really great.Enough of the sales pitch.Let's actually use it.Now, the first thing we need to do is to bring up a terminal window within cursor.Cursor has built into it a terminal window which is easier to use than than doing it using the Mac orPC's built in ones.So the way that you do it is you can go to the the view menu and select from there.Uh, the terminal option, it's it's on the view menu.But there is also a shortcut.And that shortcut is to press the control button and find the the back tick.Not a normal apostrophe but a back tick, which on my keyboard is right next to the number one.Um, and control not not command.So control and backtick and up comes a little terminal like this in LM engineering.So that's a thing to know.When you're in this terminal, it's worth knowing you can open a second terminal by pressing the plusbutton here.And now you can see we've got two terminals.And we can get rid of any of them like that.So it's worth understanding.You can have multiple terminals running.You can also open a new terminal by pressing Shift Control Backtick.And now that just opened a second terminal right here, and you can type exit in a terminal to get ridof it as well.And that gives you the basics of using terminals within cursor.Okay.And now within the terminal I'm going to type UV minus minus version to check that I have UV installed.And you probably don't.So when I do that I get a version and you might get an error.And if you get an error it means time to install UV.So the link to UV installation guide is in the setup instructions.I've got it right here.Um, it is uh, the, the installation uh, it's Doctor Astral is the company behind UV UV getting startedinstallation.It's linked in the setup guide.Uh, all of Uv's documentation is absolutely first class.If you want to be a UV expert, just spend a few moments on their docs.It's really easy to use.Installation.Super simple.If you're on Mac.Then you just use this command right here.If you're on windows, then start by just using this command right here.If you have any problems with it at all.If this doesn't work for you, there's lots of other ways.The second way I recommend using is winget.Winget is also very simple to use, so that might be another one.But but do run this to install it.And the way you run it of course is you would copy this or I'll use the Mac version.You would copy this and then go back to the cursor terminal and then paste that command in there andrun it.I'm not going to do that, of course, because I have already done it and I have you've installed.But once you do that, it will install and once it's installed.And this is important, this is a little trap.You need to then open a new terminal to use it, because you need to pick up the environment variablechange.So if you type UV minus minus version you might still get an error.You'd have to open a new terminal like this for it to work.And if it still doesn't detect UV, you might even need to restart your box.But you shouldn't need to.And if after you restart your box you still have a problem, then you might need to try one of the otherinstallation approaches.If you're on a mac and you get errors to do with permissions, a problem with your your bash profile,then that's a sign that you've installed things in the past using a command called sudo, which meansthat you've you've got your profile files with two high permissions, which is a setup problem on yourMac.I've got instructions for how to fix that.So to look for it or email me if you're not sure.But that's very, very fixable.But it's a standard Mac issue, which is very fixable.Anyways, at this point you should be at a point where you can type UV minus minus version and you doindeed see a version of UV.And the next command I'm going to ask you to run is UV self update, which just makes sure that you'reon the very latest version of UV, which you probably are because you just installed it, but if youdidn't just install it like me, then it will update you.You can see I went from naught .8.1 5 to 0 .8. 22 and you might be on something even more than that.But now we have the latest.Congratulations.Okay, it's ready for showtime now.We're now going to run a two word command and it is as follows.It is UV space sync sync.And I just want to be clear that in the the prior version of this course I used Anaconda a bulletproofpowerful data science package management system, which is very, very bulletproof and powerful.But it takes some significant time.And in fact, I would tell people that it could take up to an hour or maybe more to build your fullspec environment.So keep that in mind.We're now going to do the equivalent with UV using UV sync.And here we go.Off it goes.I've just pressed it to run and uh, let's um, uh, you, you can say yes for that, but it's not necessary.Uh, it's just run that that whole UV sync process ran.It might take a little bit longer for you because it needs to download the packages from the internetthe first time, and mine were already cached locally, but nonetheless, it should be a matter of minutesat the most under ten minutes for sure.It is order of magnitude faster than Anaconda.Uh, it's it's really amazing.The whole environment has just been built.And for people familiar with virtual environments, it's basically built this dot vmv this this virtualenvironment file folder right here.Um, and within this, all of the packages that we depend on, it did everything and it did it in aflash.Uh, how does it do it so fast?Well, it's written in rust.It's a super fast, uh, and it's just really great.So we now have a full spec environment.It's built.And step two of the setup is complete.Congratulations.You have your own UV environment.


================================================================================


# 1.7 Day 1 - Setting Up Your OpenAI API Key and Environment Variables

Now.Steps three and four of the set of instructions are optional.I strongly recommend that you get an account with OpenAI if you don't have one already, so that wecan use the most powerful models on the planet, and OpenAI is a good one to start with.But you don't need to.It requires a payment $5 up front, and you might not want to spend that.And you can use free models throughout this course.You can use Gemini or you can use llama.And there are detailed instructions for how to do it.And I will have an example lab as well.But assuming that you do want to go ahead and set up an OpenAI account, you may already have one aswell.Let's do that.That is step three, and feel free to skip this or just watch what I do if you don't want to.So you go to a web browser.I've got an incognito tab here.So because I'm already logged into OpenAI, but you go to platform OpenAI, that is where you come tothe kind of the API side of OpenAI, not to use ChatGPT, but to have your own account to use the API.and that's an important point.Some people are confused about the fact that there is ChatGPT the product which you can interact withand which has a paid plan for like $20 a month and an even higher tier paid plan.And then separate to that, there is an API, the OpenAI API, for connecting to the model directly.And people, some people are confused about that and say, hey, if I'm paying for one, why can't Ihave the other?Luckily, I've written a guide about exactly that.There's a guide called Technical Foundations, which you'll find in the guides folder.It takes you through some of the foundational information to help you navigate through some of theseconcepts.And I explain very clearly the difference between the product ChatGPT and using the API from an AI engineer'spoint of view.So do please read that if you've got any confusion.But assuming that that's that's obvious to you, then then great, come here.Come to platform OpenAI.And the first thing to do is to click sign up.And you can then come in and use like your your Google auth any or Microsoft account or an Apple account,or just create an account with an email address.I think people that are coming internationally outside the US, you might need to click a button likecreate organization or something first.You might need to click through some screen before you get to create your account, but it should bepretty obvious you should get here.Sign up for an account.So far, it's completely free.The money will come in just a second.The $5 spend.Uh, let's let's, uh, come back once you've set up your account.Okay.So this is what you see once you come to platform and once you've signed in with an account, and thenavigation is just a little bit confusing, but you have to be aware that at the top here there's dashboard.And over on the right there's settings.And the first thing to do is to go over to settings, which is where the billing information is, andgo to settings and then come down to billing right here.And this is where you get to put a balance on OpenAI.OpenAI is a pay as you go model, which means you put down an amount of money and then you draw downagainst that over time and you only spend.Tiny amounts of money on this course.We will spend fractions of a cent, typically for each of our projects.Except where I tell you otherwise, you should always track your your spend always come back here.The nice thing about this is that because it's a pay as you go model, you're always paying up front.You're not likely to get some kind of surprise as long as you only top up with a small amount.There are some some other services like AWS where it doesn't work that way, which I don't like.This one is better because you get to be in complete control of your spend.Nonetheless, you should come back and have a look at it.You should always have auto recharge off.You don't want it to automatically recharge.Don't press that.Enable auto recharge button.You don't want it automatically billing to your credit card, unless this is something that you're inproduction and you know exactly what you're doing.So the first thing you do when you come in here, you should see zero credit balance.And you will want to press the Add to credit balance button.And you'll want to to type in an amount there.And you'll want to select a payment method.And then once you've done that, you would then add that amount and sorry, $5 is the minimum.You don't need to spend $10.$5 is the minimum payment amount.You will then draw down and that lasts a year.So you've got you've got plenty of time to spend that $5.You need to make sure that your payment methods includes a valid credit card that you can charge against.Otherwise, OpenAI will throw various errors.So make sure that you add a proper payment method which it can use.And I should mention a lot of students do have problems with payments not being accepted, getting declinedthrough through OpenAI.And it's worth pointing out that almost always most of the time, any time that I've been aware of thatproblem is with your credit card not accepting OpenAI's charge as an international vendor, OpenAI ishappy to take your money if you let it.It's unlikely to be a problem with OpenAI.It's more likely that your credit card company, for whatever reason, is refusing an internationalcharge.And so you might need to go into your to your bank account and select something which enables that kindof charge.Perhaps something to look out for.Otherwise, if you if you really can't get past this and can't get a balance on there, then you cancontact OpenAI support.They're actually really responsive.Okay.But let me assume you've done this.This is now showing $5 and it's looking great.You've got a good payment method associated with your account.We're ready to set up an API key.Okay.So you're still in settings.You're still seeing this menu up here.And the first thing to do is to select API keys up here making sure you're under the organization heading.You're right up here API keys.And here you go.You can see that that I have some API keys.And you're going to make one yourself.Right now these are your API keys.And it's super important to be careful about what you do next.You're going to press the Create new Secret key button like so.You're going to want to create a new secret key.It is owned by you not service account.You can give it whatever name you want, but something that you remember, like, uh, LM engineeringkey.And now be careful with this for the project.Select default project because you don't want to have any extra constraints.If people if you do anything clever here that that constrains it in some way, then you might get errorslater.So just have default project, which means any project can access this and leave permissions at all.And then you're going to press the Create secret key button.And when you do that which I will do it now it will come up with a key like this.And fear not I'm going to delete this key right afterwards.Aha.Uh this key is your key with OpenAI.It's your way to link your code to using OpenAI.And it begins ESC hyphen, hyphen.And then this long thing right here.And that is your secret key.And that's something which we're going to be using in our code.And if you get this wrong, if you type, if you, if you change one of these numbers or one of theseletters, it's like a password.Everything has to be right.If something's wrong, it's not going to work.So copy it into your clipboard by pressing copy.It's now copied.So when you press paste, it's going to paste.And in a minute we're going to paste it somewhere.And please now don't don't paste it somewhere else first and then copy it again or anything that mightmess with the key.Let it be in your clipboard in pristine shape.We're not going to change it.We're going to paste it exactly as it is into our actual inter cursor.So keep it like this.In fact, you don't even need to press the done button yet.You can keep this screen up here.We're going to go back to cursor and put this in cursor right away.Let's go do that.So here I am back in cursor.By the way if you have one of your terminal screens appearing like this, you can close it just by pressingthe X button there.So you've got nice clean screen.Okay.What we're going to do now is create a file called the file, which is a file that contains your secrets.And it's super important that you do this carefully.It's got to be right or things ain't going to work.So?So this needs to be right.The EMV file is a new file we're going to create, which has to be at the top level.It has to be a file within LM engineering not inside any of these directories.So what we can do is we can get this little blank space just here underneath the bottom of this listof files.You can right click there and say new file.And you can see it's now giving me a new file.And you can just double check.It's at the same top level as all these other top level files.And I'm going to call it it's going to be called exactly a period dot like that.And then the letter E and the letter n and the letter v e v and it can't be called dot EMV dot txt.It can't be called EMV in your name.It can't be called EMV dot it.In case you didn't get the message, it's got to be called EMV and only that and it's got to be in theproject root in this top folder.And there it is.That is the right place to be Okay.And now what I'm going to put in here is going to be my open AI key.And here's how you do it.You have to type open AI underscore API underscore key.And it's got to be exactly this open AI underscore API underscore key.It's got to be in block capitals.It's not this it's not open API key.It's open AI API key.Now you wonder why I'm going on about this.I get more than a thousand emails every couple of during every few weeks.I get that many emails from people that have mistyped these words.So don't do that.Spell it open AI API key exactly like this.Uh, then equals it's open ai API key equals.And then you're going to press paste.And when you do paste it's going to paste in that key that you copied from the open AI screen.Let's check it out.Look it's open OpenAI.API key equals sq.Proj dash.And then this long funny thing.And there's nothing crazy at the end of it.We haven't pasted in an extra space.An extra empty.We've been careful.It is exactly this.Now, that white blob just to the right there means that I haven't yet saved this file.It's not been saved.It's not on the disk, which means it's empty as far as my computer is concerned.So I have to press command S Ctrl s on a PC, command S on a mac or file save to save it, and the whiteblob goes away.And that means it's now saved.This is now in the file I get, I get, I get hundreds of emails from people that haven't saved thefiles.Saved the file.You'll see that there is a little stop sign next to it.I also get hundreds of emails from people that are worried about that stop sign.But that stop sign is actually good news.That stop sign is cursor telling you if you hover over it, it will tell you.It's telling you that AI features are disabled for this file.And the reason for that is that this file contains your secrets.This contains your precious key that connects you to OpenAI and to your credit card.And we don't want that leaking out.And particularly we don't want cursor sending this off to different AI to get some sort of informationon it.So as a result, cursor is saying, hey, hands off.I'm I'm not going to touch this file.I'm not going to do AI suggestions for this file.This file is secretly just for you.So we like to see that stop sign.That is good news.Look out for it in anything with secrets in it.Like the EMV file.All right I think I belabored this one enough.You've got a EMV file.It's called exactly EMV.It's in your project root.In the top level.It says OpenAI API key in block capitals equals.And then it's got this key right here that you have taken from your your actual API key that you tookfrom OpenAI.And you pasted it in here.Congratulations.You've got through the hardest part of the whole course getting the API key in there.Let's move on.


================================================================================


# 1.8 Day 1 - Installing Cursor Extensions and Setting Up Your Jupyter Notebook

Welcome to step five of the setup, the very final step.I'm here in Cursa and just before we get going in Cursa, we have to install some things called Cursaextensions.It might prompt you.It might say, do you want to install the recommended extensions?In which case just say yes because it will get them right.But but if not, then we just need to install them.If you've never done this before and very simple to do, you start by going to the view menu and youselect extensions.And this comes up over on the left where the files were.And you now need to search for Python to bring up the Python extensions, which does things like coloringin code nicely and checking it and the like.Now there are in fact two different Python extensions.One of them is made by someone called Innersphere.Well, Innersphere is actually the name of the makers of Cursa.So this is the one for for Cursa.There's another one made by Mr. Python, which happens to be the going name for Microsoft, better knownas Microsoft, and either of these will do just great.I have the Microsoft one installed.You can have the one you click on it, make sure that you install it, and then you'll have Python installed.And then back to the search and search for Jupyter.And then you'll see that there is Jupyter by this company, Miss Tools AI, better known again as Microsoft.And this has like 4 million downloads and it's great.And Jupyter, which is the name of the particular type of data science interactive work that we'll bedoing Jupyter you want to install as well.Mine's already installed, which is why it says uninstall there.And so make sure that both of those are installed.And once you've done that, finally you go back to the view menu and you go to explorer again to bringback the files on the left.You open the week one folder, the week one directory, and and click on day one dot ipynb, which standsfor IPython notebook, which is one of these kinds of things.And congratulations, you've opened the day one notebook.We're so nearly there.The very, very final step is to go to this thing here that says select kernel.The kernel is the name of the of the Python process, which is going to be running behind all of thisthat we're about to work through.So you have to press this button, select kernel.And it will pop up with this little pop up saying Python environments or existing Jupyter server.It all sounds very high tech.You choose Python environments and you should get this list.You may only have one option on your list, but you should have something where the top option shouldhave a star next to it.It should be called dot env and it should say Python 3.12. something.It might be 3.1, 2.1, 10.11.And it should have this kind of path dot venv bin python this thing and it should say recommended.And this is the one to pick and it should look beautiful.It's referring to that folder right there.You see on the left.That's what this is pointing to.Now, if this doesn't pop up as an option, then go to the troubleshooting notebook that is in the setupfolder.And that is where it will help you out.But it should do.Hopefully you should be able to select it.It's now selected and we have ourselves a Jupyter notebook that's ready to go.These things are called Jupyter notebooks.Uh, these this type of format of thing, this dot ipynb.And that is now set.If you need help, then you go to the setup folder.There is a troubleshooting ipynb.And that is where you'll get more instructions for troubleshooting.But hopefully you now have this open.You're looking at this.You have your your environment set.Your kernel is set.It's finally time to go through our first lab.So these things are sometimes called labs, sometimes called notebooks.They're like Python files which have mixtures of of text, formatted text and code.And you can run the code piece by piece.Each of the pieces are called cells.You may be if you're if you're already in data science, you know this stuff super well.Uh, but, you know, uh, I'm going to explain it to the others.Now I've used I've treat these like, like living, breathing documents with tons of good interestingmaterial.And I do ask that you spend a moment to read through them, particularly this first one, because Igive a real lay of the land here.I explain that we're about to embark on our first LLM project and look, it's a simple one, but you'vegot to start somewhere.And for the pros, you can really rattle through this really fast.But you know, we we have to do something simple later in the course.We're going to be building a complete agentic platform with multiple agents collaborating and doingall sorts of stuff with, with many different llms involved.But we're starting small.That's the way to do it.What we're going to build is something which is a different kind of web browser.It's something which is able to take a web address or URL and find a scrape it, and then make a summaryof that web page and display that summary in a nice format.It's our own little Reader's Digest of the internet.That's what we're building.And and sure, you could, you know, ChatGPT now the product has some things like this built into itthat you could ask it, but we're basically building our own.We're building a product like that ourselves, and we're going to do it using an API call to the underlyingGPT model.Okay.So I do want to point out if you're new to using these things called notebooks, they're also calledlabs, also known as Jupyter Lab.As you now know that I've got lots of guides here that will help you through this.There's I think you already know there's guides to the command line, guides to using git and GitHub.There's some technical foundations to to explain things like endpoints and APIs and environment variables.And then there is this guide all about using notebooks, which you should look at to learn more abouthow to use notebooks.And there's also some Python foundations if you're new to Python as well.Um, also, we'll talk a lot about what to do if you want to use models other than OpenAI, such asusing Gemini, which has some free tiers, or using llama with local models, there is a guide thatgoes through all that as well that you can have a preview look at, but we'll be doing it together.A reminder that I am here to help you.You know that you can reach out to me by email through the platform, and you can always connect withme on LinkedIn.I might have mentioned that something I kind of like, so feel free to do it.Uh, there's the troubleshooting notebook.That's that's right here as well that it is in the setup folder troubleshooting right there.So go to troubleshooting if you have any problems.And as I say if this is stuff you already know about then just speed me up.Put me on two x.Go go through this nice and quickly.Uh.And I'll hope to to throw in some interesting stuff as well.But we will get to more juicy stuff very soon.Now, the way that I run these labs is a bit different to some other courses you might be familiar with,and I make the point here, and I do ask you to read through this.I don't like to sit here typing while you watch.I will do that a little bit because I know people want to see that.I'll do a bit of it, but I feel like that's not a good use of either of our time, particularly ina day with AI tools as well.I prefer to show you some code to explain how it works and why it works, to run it, and for you toalso run it.Put in print statements, change it, make it your own.Investigate it.Experiment with it.That's the best way to learn.Not by sitting there typing things through.Now some people like to type and that's that's all good.Of course you can just delete the cell and then retype it, or create an empty new notebook and startfrom scratch as you wish.Everyone learns differently, but I suggest dig into the code, explore it, experiment with it, anduse this as your starting point for your own projects.So.So make your variations and then it's it's great practice.It also gives you an opportunity to make a variation and post it on LinkedIn and tag me.And that's something that I will happily weigh in and make sure that I amplify your achievement.Look at the fact that these labs will be changing all the time.I constantly update them, sometimes multiple times a week, to have the latest models to have betterexplanations as people ask questions.So please assume that these.The text here is a living, breathing document, and if what you see on the screen doesn't match exactlywhat I'm saying, don't worry about it.What you're seeing is better and I'll try.I won't subtract anything.I'll only add more good stuff.So so welcome.The fact that there could be new content that you're looking here.And finally, I do want to try and emphasize the business value of everything that we build.Whilst I'll try and make it fun and entertaining, I will also come back to talk about the commercialimpact of what we do, and there'll be commercial exercises for you at the end.All right.And then it tells you about installing the cursor extensions, which we've done.Select the kernel which we've done, and it's time now for us to get going with some code.


================================================================================


# 1.9 Day 1 - Running Your First OpenAI API Call and System vs User Prompts

Okay, so now you scroll to this first box right here which says imports.And the way that you run this bit of code known as a cell is you hold down the shift button and youpress return.While you've clicked anywhere in this box, I press this and it runs.Now, if you have any errors running this, if you get an import error or something like that, or ifit just doesn't seem to run, it just sits there.It means your kernel isn't set right.Look on the top right there.Look at what mine says.It needs to look a bit like that 3.12. something.It needs to be referring to your local venv right there.That is what you have to see.And then it should run just fine.There's no output yet.Nothing's happened.It should run.It should show a tick mark, meaning that it's run.You've just done some imports.We'll talk more about what all of this does later, but for now, just run that code.Okay.The next thing we're going to do.So come down to this next block of code right here is we are going to load in that dot EMV file, thatsecret file with our environment variables, our secrets.We're going to load it in.And we're then going to collect this thing called OpenAI underscore API underscore key.And as long as you set it right then it's going to find a key and it's going to be happy.Let me see I hold down shift I hit enter and it says API key found and looks good so far.Now if yours doesn't say that then something's wrong with your EMV file and it's going to give someexplanation.Hopefully.Maybe it doesn't start with the right thing.Maybe you forgot to save your env file.Something went wrong.You can go to the troubleshooting lab to dig into it, or you can have a look yourself and try and figureout why it wasn't able to read in your EMV file, but I'm hoping it did.You're in great shape.You found an API key.All is good.Let's make our first call to OpenAI running in the cloud.Now we're going to call OpenAI in the cloud.And if you don't want to, if you want to use Olama, then wait for tomorrow in day two.We're going to redo this using Olama running locally, so that will be your day.So never fear, it will happen.But for now, what we're going to do here is we are going to, uh, to send this message to OpenAI toGPT two.We're actually going to use GPT five, the new version of GPT.And what are we going to do?We're going to say, hello, GPT.This is my first ever message to you.Hi.Uh, so I've put that in this variable message.Now, the first thing I have to do is I have to put that into another variable called messages, whichis in a special format that OpenAI expects.And you'll see that cursor already fills in what it knows I'm going to do, but I'm going to ignorecursor.I'm going to type it because I know some people like me to type, so I'm going to type.So what you send to OpenAI, the format is a particular format that OpenAI came up with, and we'regoing to talk so much more about this format.So I don't want you to worry about it right now.I'm just going to tell you that it is a Python list and the Python list contains a number of dictionaries.Python dictionaries.In this case, we're only going to have one dictionary.There it is.So it's a list with one dictionary in it.And that dictionary is going to have two keys.One key is called role.And the other key is going to be called content.And that role we're going to put in that in for the key role.We're going to have the value being the word user.And we'll find out why later.And there's going to be another field, another field with key content.And what we're going to put in there is going to be that message.Hello GPT, this is my first ever message to you.Okay, that's all clear.Let's just print what that see what that looks like.It looks like uh, list with one dictionary in it with role user content.Hello, GPT, this is my first ever message to you.Hi.Okay.That's easy.Next up, I'm going to hover down here like this.And I'm going to add in.So I just hit another code block by pressing code right there okay.Now I'm going to type in, um, OpenAI equals open a equals open AI like that.I'm creating a new instance of an object, OpenAI.And again we will cover exactly what that's doing another time.And now I'm going to make my first API call to GPT running on the cloud.And this is how I do it I type response equals OpenAI dot chat dot completions dot create.And that might sound like it's nonsense to you.You will get so used to that that you're going to dream it eventually.But for now, just believe me, that's what we type.It's known as the Chat Completions API.And you say OpenAI completions create.We pass in the name of a model.And the name we're going to go with is GPT five Dash nano, a very cheap version of the latest model,GPT five.At least the latest for me right now.And then the messages I'm going to pass in is going to be this variable messages that we just created.And that is it that is going to call GPT.And with what comes back, what we want to take is we want to take this response and we want to takethe choices variable there.We want to take the first choice message content.There it is.That's what we're going to do.I'm going to now press shift and enter.And when I do that it's actually going to call out to to GPT running in the cloud.Let's do it.Let's see what happens.It takes a little bit of time for for that.And it says, hi there.Nice to meet you and welcome.Thanks for saying hi.I'm here to help with questions, explanations, writing, brainstorming, planning, learning and more.Encoding.What would you like to do today?If you're not sure, here are a few quick ideas.And on it goes.So, a very verbose answer for our very first call to GPT running in the cloud.And it was all pretty easy.And we were running it from code.And if you know all this already, then good for you.Zoom through it.If this is completely new to you, then we're going to talk a lot more about this.It's going to all make absolute sense very soon.But I want you to enjoy your first call to an LLM.Okay.So I have written a little function that is called fetch website contents.And that little function is right here in scraper pi.Now this is what it looks like.This is not a course about web scraping.So I don't plan to tell you about what I've done here.I'm using a lovely package called Beautifulsoup that's very popular.You can look at this function if you wish and use it yourself.But it's very, very simple.And it's doing just what's known as a server side scrape that it just collects that web page.It's not actually rendering this web page.So it's not a very sophisticated web scrape, but it fetches the contents of this URL.So that's what we're using here.And for example, I could pass in my own personal web page https.Com and I could use fetch website contents and see what I get there and what I get if I run.This is a whole lot of stuff from my website.And if I actually if I say like print ad, then we'll get something a little bit more than that.And here it is.This is the contents of my web page with lots of stuff.And it's worth noting that nothing to do with AI at this point.This web scrape is just a simple fetch of a web page.That's it.But now we're going to start to involve AI in this.So I imagine most of you know this already, but some of you might not that there are these two differentkinds of message that you send to an AI, to an LLM, two different kinds of prompt.One is called the system prompt and one is called the user prompt.And the system prompt is where you specify the frame, the overall task they're carrying out.You tell them what tone they should take.You give them context.We'll find out later.When we talk about things like Rag that you can supply information in the system prompt, it's yourway of setting the scene.The user prompt is the actual message coming from an end user to the LM that it should respond to inthe context of whatever you told it in the system prompt.So that's the difference between the system prompt and the user prompt.And people that are very familiar with this can can add all sorts of stuff to that.But but we will get there in good time.So for example, we could have a system prompt that says you are an assistant that analyzes the contentsof a website and provides a short summary, ignoring texts that might be navigation related respondin markdown.It's framing the situation, it's setting the task.It's giving the format, the style of how response should come.That is our system prompt.And we're going to to also define the start of our user prompt, which is.Here are the contents of a website.Provide a short summary in markdown if it includes news or announcements, then summarize these two.That is our user prompt prefix.So far so good okay, so we saw a second ago that when we're sending a message to OpenAI, we do itusing this list of dictionaries.And I showed you one in particular which had role user content.And then we had something like hi GPT.So there's another type of dictionary where you say role not user but system.And in fact this is how you specify the system prompt and the user prompt.Uh, you have a list of dictionaries and we'll have two dictionaries in one dictionary the role is system.And the content will then be the system message.In the second dictionary, the role is user and the content is the user message, the user prompt.And so just just to give you a silly example, here's one.Here's messages role is system content.Let's say not.You're a snarky assistant.Let's say you're a helpful assistant.And then what is two plus two.So this would be messages.And what we can do is then we can say here response is open AI.Look at how cursor fills it in for me.Look at that response is open chat completions dot create model will go with GPT 4.1.Mini know we'll make it 4.1 nano make it a really cheap, simple model.GPT 4.1 is faster than GPT five.It'll be very, very quick.And we pass in this these messages, this this list of two.So let's see what a helpful assistant has to say about this.The helpful assistant.When given this question uh, we need to print the results.Cursor didn't fill that in response.Dot choices zero dot message content.Try it again.Two plus two equals four.So now we're going to change the system message.So it doesn't say you're a helpful assistant.It says you are a snarky assistant.And otherwise exactly the same user message.What is two plus two.We run this.And oh finally a math question.Two plus two equals four.Groundbreaking.I know that's just the effect of putting in snarky there.And now a quick challenge for you in the spirit of of changing things and tweaking the code.Make a different system prompt here.Not a snarky assistant, but something else.Maybe an assistant that speaks like a cowboy, or like a pirate, or an assistant that speaks in pigLatin or in another language.Whatever you want, put it in there.Experiment.Try changing the system prompt several times and get to see how different system prompts with the sameuser prompt will change the tone, character, and mission assigned to the LM.


================================================================================


# 1.10 Day 1 - Building a Website Summarizer with OpenAI Chat Completions API

Okay, it's finally time to put this all together.So before we created our messages, just by setting a variable messages and building that list of dictionaries,what we can also do is write ourselves a little function.This is a function called messages for, and it takes in the contents of a website and it will returnour list of dictionaries.It's going to return two dictionaries.The first of them is role system content.Is the system prompt that we defined up there somewhere.The second one is role user and the content is the user prompt prefix, followed by this website that'spassed in.Okay, that sounds good.Remember the prefix looks up there.Maybe we should put in an extra empty line there.That's probably a good idea.Uh, so that that website will then get get shoved in underneath.All right, let's run this.We've just defined a function that will return this list of dictionaries when we pass in a website.So we have a website.We have one in the variable AD.It contains all of my personal web page.So if I run this cell, we'll see what we get.We get something which is indeed a list of dictionaries.There's one dictionary that's that's got role system content, and it's your assistant analyzes contentsof a website and then role user.Here are the contents of a website.And if I scroll you will see that it's got the full contents of my website.So hopefully this is super simple.We've we've written a function.It can manufacture this list of dictionaries.The list of dictionaries contains a system prompt and a user prompt.The system prompt is telling it.You summarize websites.The user prompt says, hey, here's a website to summarize, and then it has the contents of the website.Okay?And now it's time to bring it together in this lovely function right here.Okay.And just before we run this summarize function, I want to mention I did just make a little tweak tothe system prompt I added in this sentence do not wrap the markdown in a code block.And this is a little trick that you sometimes see that prevents it from putting extra stuff around theresponse.Not super important, but makes the results a bit better.Okay, so this here this summarize.This is the function that brings it all together.We start it takes a URL, a URL like madonna.com.It starts by calling fetch.Website contents that you'll remember is my utility that goes off and pulls a website's contents.We put that in a variable website.We then call OpenAI.Create.That is the chat completions API.That is new to you now, but you will soon know that off by heart you will.You will dream it.You will know it so well.It takes two things.It takes a model.And we'll be using GPT 4.1 mini.And it takes messages, which needs to be the list of dictionaries.Well, we can get that by calling our function messages for for our website that function just here.It returns the list of dictionaries exactly what we want, and with whatever comes back, we call choiceszero content.Let's run that function.It's now been defined and let's give it a try.Let's call it for my website and see what comes back.This is now making the system and user message, putting in a dictionary and dictionaries in a list,sending it to GPT.And before I can finish explaining it, it's come back.We've got an answer.Here is a summary of my website.Our little snapshot of my website about me.It's a bit hard to see because it's all on one line and it's not so formatted, and we can fix thatquickly by making another function called Display Summary.It will call that that function.Here it will call summarize with the given URL.And then it will use this utility display markdown which is a nice way to show markdown elegantly insideone of these labs.So if I run this, then once again it's building the system message, the user message, packaging themup, calling the chat completions API.And with what comes back, I have to actually run display summary for my website.Now it's actually doing that, making a system prompt and user message, sending it off.And what comes back bam is this website summary.Uh, this website is the personal and professional homepage of Edward Donner, a coder and AI enthusiastwho enjoys experimenting with large language models.That sounds like me.Uh, and, uh, it says Nebula, and it's got some projects.It's about my interests and some posts and announcements summarized and how to reach me.Uh, so we've just done it.We've just built something that's able to take a website, scrape it and summarize it, using a callto OpenAI running in the cloud, connected to the API key in your EMV file.Congratulations.Try it with some different websites, experiment with it and hey, you can go to the system prompt andmess with that.How about we get a snarky version of the website instead of the normal version?Any of these things are possible.Go and experiment and explore and have fun.Calling the OpenAI API.Well, let's have some fun!Since I mentioned we can change the system prompt, let's go and do that.Let's go back up to the system prompt here instead of your your helpful assistant.Let's make this.You are a snarky assistant that analyzes the contents of a website and provides a short, snarky, humoroussummary.And then run that.Let's redefine the system prompt.Let's just go straight back down and rerun this line here.To summarize.Edward Donald, let's do the display version, make it the pretty version of it, run that and run displaysummary and see what we get.Edward Donner's nerdy Playground editor is your friendly neighborhood code monkey who's obsessed withplaying DJ badly.Stalking Hacker News like it's the only social interaction he needs.Latest news like you're dying to know so wonderfully.Just with that tiny change, we've been able to get a snarky version.Not just a Reader's Digest version, but a snarky version.And we can also use this to do different kinds of tones and styles and try it with other websites,too.And so I'll keep it on snarky mode.And let's look at what CNN's news of the day is.Uh, it's now collecting all of CNN's website, the news site, and it's coming, uh, the CNN, theglorious smorgasbord of everything news.Welcome to CNN, where you can get your daily dose of global chaos, celebrity heartache breaks andgovernment shenanigans in one place, trying to escape world wars, hurricanes and political drama.Too bad it's all here, front and center.Ha!So, uh, yeah, because nothing says fun like a political stalemate so wonderfully.This has given us a summary of the news, but also a snarky version of it as well.And you can try some different websites here too.What you'll discover is that this is a pretty simplistic solution.The way that I'm scraping web pages will only work for web pages that don't get rendered in the clientweb web pages, that you can just fetch them remotely and you'll get the full content.And that doesn't work for too many websites.But luckily, there's an easy way to build this out yourself if you know about this.If you know about tools like Selenium or Playwright, then you can build this yourself.And many students have.And that brings me to the topic of the community contributions.So over here in week one is a folder called Community Contributions.And it's a place where you can put your own projects.And if I open this up, you'll see that a lot of people have come and done this, which is such a joy.You can come in here and add in your own exercises and your own examples.It could be as simple as changing the system prompt to translate a web page to French or Spanish.Uh, hopefully better than any of my Spanish.Uh, and you can do anything that you want.But use this as an opportunity to make changes and to share them with other students.And there are instructions on how you do that.In the guides.There's a guide on the git and GitHub guide includes instructions for how you submit what's called aPR, a pull request, which lets me merge in your changes into the repo.And I would love that.And I'd love it if you shared it.And it's a great way to add value to the whole community and to get your work seen and get get me tosee it and look at it and comment on it as well.So, uh, I want to mention that that there are so many business applications of what we've built,uh, the fact that we've, we've just experienced calling a frontier model in the cloud, and we'vethen given it some instructions to summarize it, to change the tone, we could translate it.There are so many different ways that you could extend this and use it, and you will see many of themin community contributions.So many things.Someone added it to to to call the Yahoo Finance API to summarize stock movements.There's lots of other stuff in here and you should be inspired by this, but try and take it in yourown direction.Apply it to your business or to some some personal thing that's been on your mind.A pain point that you think you can fix.A simple way that you can make a call that involves collecting a web page and doing something with thatweb page.That's it's such an open canvas.There's so many different commercial directions that you can take this, this idea of summarizationor translation.This is one of the most essential and simple gen AI use cases, and you now know how to do it.And you can apply this to any problem you want.So please do that, apply it to a different business and try it out for yourself.Uh, try making something and and have fun with it above all.


================================================================================


# 1.11 Day 1 - Hands-On Exercise- Building Your First OpenAI API Call from Scratch

And now an exercise for you.You've just hopefully gone in and run this for other web pages, and you've maybe tried tweaking itfor a different business purpose.The exercise is now to come in here and see if you can make your own message to GPT from scratch.So first of all, write a simple system prompt.Now you should think about something completely different to do here, perhaps still related to to summarizinginformation.So as an example, you might want to summarize the contents of an email.Or perhaps you might want to take an email and come up with a nice subject, a possible subject forthat email that might be an example of a business task.So in the system prompt here, describe the task that you want it to do in the user prompt.You'll want to put in here the contents of the email or the thing to be translated or whatever the isgoing to be the prompt with the instruction.Then you now have to do some typing, just like I did some typing.It's your turn to do some typing.Replace this messages equals you're going to want to put in here something which will describe yourmessages to OpenAI.So this is going to be a place where you're going to put in a list of dictionaries, two dictionaries.And it's going to have some things in it like role and content, and cursor is determined to fill itin for me.But don't cheat.Uh, try and see if you can't do it yourself and then cheat if you can't.Of course, it's always good to use AI tools.It's not cheating in the least.And then you have to uncomment this line and have response equals and type in the the the command thatyou're going to give, which I'll tell you very quickly, is OpenAI create.And you pass in the model name and the messages, and then you're going to print the result.And the result will be this word response .0. content.And always look back up in this code and copy and paste to get that example.And when you've done that you should find that that works.And if you've built something like something that can give you the subject of an email or somethingelse, then please consider doing a PR.The instructions are in the in the guide so that I can get to see it myself.And I can share in your joy of your first commercial call to an LLM.And then finally, there's this extra exercise I mentioned for people that are pros, people that alreadyknow this stuff.You'll be aware that, as I say, my my way of web scraping is super simplistic, but you could useselenium or playwright or something similar to do proper web scraping with a web page that gets renderedin the browser, and you'll see that other people have done that.There's a playwright version there.I know that there's several selenium versions.You could look at how other students have done it.But first, if you know this stuff, if you've used selenium or playwright before, have a shot at doingit yourself as well to build a more industrial strength.Web page summarizer.And then afterwards, as I say one more time, please do consider sharing your code.The guide will tell you how to do it.Submit a PR.I will gladly review it and I'd love to include it in the community contributions with 1 or 2 otherpeople that have done this too.Okay, so with that, that's your mission.Enjoy it.Fill this in.If if you if you find that you're struggling with it, please don't worry.We're going to be doing this so many times over the next eight weeks that's going to become second nature.Give it a shot.Give it your best crack.And if you can't get it this time, don't worry.We'll be coming back to this tomorrow.Wow.You're probably thinking, whoa, this if this is if this day is what every day is like, this courseis going to go on forever.No, today was a big, big day.And it's longer than most of our days will be.You've got through a ton of material.You've made your first request to an LLM, and I'm hoping that you've also adapted it for a differentcommercial task.And maybe you've even submitted a PR.So much has got done and and it's just just a great start.Your environment is built and ready for prime time.And tomorrow we're going to start getting a little bit deeper.But already, even though your journey just started, your 2.5% of the way along your journey to theto the path of being 100% a master of LM engineering.You've already used a llama to run a model locally on your box.Perhaps several models.Seems like an age ago that we did that.But we did that.You've written code that calls OpenAI's frontier models.You know the difference between a system prompt and a user prompt.And you've built summarization.You know how to apply to commercial problems.Maybe you've built a couple yourself.Tomorrow is going to be a little bit of a breather.It's going to be an easier day, I promise.We're going to talk about the steps it takes between now and being a master of LM engineering.I'm going to set you up for success.We're going to talk through some of the top frontier models out there, and we'll also do a little bitof a dig into Olama and using Olama locally.I kind of repeat of that lab, but using Olama locally on on our own computers, people that didn'twant to to get OpenAI.That's your chance to use Olama instead.So I'm super excited to see you on day two.You survived all of day one that by far the biggest day of the whole course.So tomorrow much gentler.Can't wait.See you then.


================================================================================


# 1.12 Day 2 - LLM Engineering Building Blocks- Models, Tools & Techniques

Oh, hello.It's you again.You've come back for more?I didn't manage to put you off with the horrors I put you through yesterday.You're back for more with day two of LM engineering.Welcome, welcome, welcome.I promise an easier day today than yesterday.I will always start each of these with a recap on what we did on the previous day, and what's to comeso you can get used to this.It's a bit of repetition, but repetition is good sometimes.Already we have played with Olama and got used to different open source models.We've written code to call open AI.Maybe you didn't.If you're holding back on having that key and you're ready for something new today, which we will do.You've learned the difference between a system prompt and a user prompt.And we did a summarization use case, a core gen AI use case that we built on the very first day.And today we're going to go through the building blocks, the steps it takes to get from where you arenow to where you will be in LM engineering master and recognizing frontier models, the different kindsof frontier models, what that means and how to use them.Let's get into it.So look, the first thing I want to say is that this is predominantly a practical course.This is about getting stuff done.I do teach some theory, but I like to do it in the context of the practice of putting things into action.I am here to to allow you to deliver commercial impact through LMS.Whether you work at a startup, whether you're an entrepreneur and you want to start a startup, orwhether you work at a fortune 500 enterprise or anywhere in between, the goal of these sessions isto teach you how to use LMS in order to have commercial gain, so please always keep that perspective.I'm a practical person.I think that the best way to learn is by doing, and doing is what I have in store for you.And there are going to be these three different dimensions, these three different factors to LM engineeringthat we will build on.One of them is the models themselves, and I mentioned this yesterday, I mentioned that bottom layer,the models open source closed source.We'll look at multimodal models that generate images and audio.The architecture of models and importantly, how you select the right model for a particular task athand.We're going to then build on top of models by looking at different tools, frameworks, libraries,things like the fabulous hugging face which which you will get to love.Long chain gradio.Weights and biases.Modal.And then on top of this are different techniques that you can apply.Taking advantage of these libraries and using these models techniques like calling APIs, which we'vealready done Multi-shot prompting is an easy one, is a very juicy one fine tuning a powerful one,and agent ization a genetic AI.You don't get bigger than that.That is the hottest topic of all right now.And so I've designed this course to appeal to many different levels of background.No matter what your expertise is you're coming to this with, there will be something here to appealto you, but you might find some parts are too easy if you're experienced, in which case, hang inthere.Speed me up.Put me on 2X2X right now and we'll quickly get to meaty projects very soon.you might find some parts are too hard for you, particularly if you're completely new to this field.And one thing to know is that all of us, every single person, one thing we all have in common is thatat some point we've all been new.And so we know exactly how it feels.I've written a bunch of self-study guides in the guides folder.Please spend time with them.They really will help prep you.I do believe that you can learn everything from those self-study guides, and I've had people completethe whole LM engineering course who came at this with zero programming experience.It's not ideal.We do.I do aim this primarily for someone with some coding experience, but you can come in completely raw,learn from self-study guides and you'll be successful.Many people have looked at the guides, ask me for help and take things slowly, carefully, work throughthe practicals, add in print statements and above all, don't worry if you don't get it right away,it doesn't matter.Get some intuition.Keep moving on.You can always come back.You can, you can.Some people have actually watched all the videos and then come back a second time and gone through muchmore carefully.If that works for your style, then that's a great idea.But otherwise you can just work through it slowly the first time.But if it feels just right, if I've got it just right, wonderful, then keep going.So the ideal background to come into this course is someone who has intermediate level Python experience,and if you have that, then you're in really great shape.If you look at this line of code and you know what that does basically give or take, then fantastic.If not, don't worry.As I say, I've got great background material.You can either look at the beginner Python guide or the intermediate Python guide, or both, and getto a point where you feel pretty comfortable and you can pick stuff up as you go.And you know what?Llms are amazing at this cursor is great.It will fill in most things for you, and it can explain any line and what it does and it can help youwith it.So with things like what I've got going on here, like a generator and a set comprehension, that kindof stuff, it can explain it all.It's fancy Python stuff.But I tell you, when you see how it works, it's all pretty easy.All right?And there's some tricks to get the very most out of the next eight weeks that you and I are going tospend together to get the most out of it.First of all, do follow along with the coding.It's often better to watch watch me do do a video, watch me talk through it, and then come back anddo it yourself.Once you know what to expect.Now, I know some people dislike the way that I don't often just type out the code.Most of the time I have the code there and expect you to run it.It's a style.I find that it's very productive and it allows you to learn a lot quickly.Typing everything.In these days with AI tools, it's just not necessary.It's more important to understand what the code does and to be able to change it.Now, if you want, you can start a fresh notebook and type from scratch if you prefer that.And some people do learn best by by typing.And sometimes I do that myself.When I'm learning things, it can help, but most of the time I would urge you look at the code, understandit, change it, add in print statements, mess with it in some way, and see how you can experimentwith it.That, I think is the best way to learn.And that's why I like to drive these labs and then do complete the exercises that I've got in the labs.And you should be making those changes and then pushing it to your own GitHub so that you've got examplesof codes and solutions.And you can also make these changes and submit what's called a PR, a pull request, so that I can mergein your changes, as I mentioned yesterday.And there's instructions on that in the guides.If you're new to that, make sure that you apply the examples that I give to your business, or to someidea you've had about how you can apply it to something you work in.The best kinds of ways to apply these things is to something that feels personal to you.One person on the course that it just blew my mind.He had an elderly relative who was having problems renewing his his medications because he was losingtrack of the different refills, the different pharmacies.And the student wrote a tool, an LM based app that helped his relative to know when to schedule, whichrefills with which pharmacy and the dependencies between them, and so on.And it was so cool.And he posted it about it.And of course it got lots of attention because it was very personal and it was real and tangible, andit solved a concrete problem for the student and his relative.That's the kind of thing you want to look for, ways that you can take what I do and apply it eitherto your personal situation or to your work, your professional life, and make something that's specificto you.And then when you've done something like that, or when you've done any of the exercises or anythingthat feels like you've made progress on your journey or just discovered something or had some insightthat you think is interesting or an experiment, post about it on LinkedIn.Make a post if you wish.If you're comfortable, and tag me in the post and make it relevant in some way so that I can then comein and weigh in and say, you know, this is great.This is this is why it's important, and so on.And in doing so, when I do that, of course, LinkedIn makes sure that that gets then shown to allthe people that I'm connected with that are in large part other students on this course.And that helps amplify the work you're doing.Other people will weigh in.And similarly, if you're on LinkedIn and you see that I've posted about some other student doing something,then please do weigh in because it's so helpful.It helps build up the community.It gives attention to all, all of you and everyone's contributions.And it means that when people are looking, perhaps, perhaps it's a future client of yours, perhapsa future employer of yours, a future boss.They will see this kind of action.They will.They will be aware of your expertise and of the impact you're making, and it just helps to promoteyour visibility.So.So please do do that.And most importantly, stick at it.I, I really do say that that even if there's times when you feel like you're not following it, orthere's times when you feel that's not as relevant to me, maybe speed me up, but just stay the course,get to the end.It's eight weeks we're going to spend together.By the end of it you will have mastered LM engineering.It's going to be worth it.Hang on in there.Let's get through it.Keep me posted on the journey.If you have any problems, reach out to me.I always reply and I'm always quick.All right, let's keep going.


================================================================================


# 1.13 Day 2 - Your 8-Week Journey- From Chat Completions API to LLM Engineer

So I already covered this yesterday.But one more time I want to tell you about the eight weeks I have in store for you because they're sogreat.The first week this week we're here on the left.The first week is about the foundations and the thing called the Chat Completions API, which you alreadyexperienced yesterday.OpenAI.That's this week.Next week is about looking at a ton of different frontier models, calling their APIs, getting supercomfortable with APIs, and then hopefully I'll slip in there some things for the pros so that there'llbe some new new information to to give give you some some extra juice.And we'll also look at things like multimodality.We're going to have such a cool project for you to see that week three is when we go deep on open source.So whilst we've used llama so far already to use open source models, we're going to use Hugging Faceand I'll explain the differences and we'll be building some cool open source projects.Week four is when I cover this super important topic of how do you pick the right LLM?And we will also use Llms to generate code or to port code.And an interesting project.Week five is Rag.Now I know you're super interested in Rag.Everybody is.I do advise you not to skip ahead to week five because you'll miss out on lots of great stuff.Um, but but week five will be fun, and we're going to create a knowledge worker expert that is ableto read through tons of documents and answer knowledgeably about them, which is a very important commercialuse case.Week six is when we start to work on fine tuning.But week six is also a core data science week, when we will work with data and we will work with scrubbingdata sets, data curation.It's stuff that I absolutely love, but I know a lot of people don't don't like it as much, but I reallyenjoy it.I hope you will too.It's going to be very interesting.Week seven is the shocking week, the shocking week with a ridiculous outcome that that, uh, hopefullyyou won't be expecting.I shouldn't give give the game away, but it's when we fine tune an open source model, uh, to, toachieve, to do a business task of our own.And week eight is when we bring everything together.It really pulls together many of the prior weeks.And we build in a genetic platform, and it solves a commercial problem, and we go out with a bang.And that at the end of those eight weeks, you will be able to say, I am an LM engineer.That's the plan.And in a cheap move to keep you engaged all the way through, I've definitely been sure to to sprinkleinteresting content all the way through.There'll be tools and resources.There's going to be basically a code cookbook.I will leave you with sections of the code that you'll be able to use in your own projects.But most of all, I've left the best till the end, and there's no point in skipping ahead to it becauseit builds on everything that came before it.So I uses trickery to get you to stay engaged and see it through to the bitter end.


================================================================================


# 1.14 Day 2 - Frontier Models- OpenAI GPT, Claude, Gemini & Grok Compared

Now, you'll be pleased to hear that today isn't entirely admin and logistics.We're going to cover a topic and then we're going to do a lab.So the topic I want to talk to you about is models themselves LMS, the things that that generate content,the things that do generative AI and they are the best ones on the planet are sometimes known as Frontiermodels, frontier LMS.And they're created by companies that we call the Frontier Labs.And that's an informal word.There's not a strict definition of frontier.It just tends to mean the really big companies and their models.Now typically when you talk about frontier models, you're referring to the closed source models, themodels which you have to pay to use because a company like OpenAI spent hundreds of millions of dollarstraining their models, and they need to get back that money, and it costs a lot of money to run thembecause they're enormous.So those models are sometimes called closed source models.And and frontier Models is another name for them or closed source frontier models.And then contrasting with that, there are open source models Models made by companies like like meta,Facebook that made a range of models called llama that are open source.And these are models that you can access and run yourself.The people who feel strongly about this stuff sometimes say that they're not really open source.They are open weight because meta has made available all of the weights of the model, but they haven'tnecessarily made available the training data and methodologies that was used to build the model.But still, we tend to call them open source models.And when you use the word frontier, it's most common to only be referring to the closed source ones.But sometimes people say frontier and they're also referring to open source.It can be used in either context.So just just stay aware of the context when someone uses a word like frontier model.But most of the time when I say frontier, I'm talking about the big closed source models.Hope that all makes sense.Let's talk about some closed source models, and the first one will not surprise you in the least.OpenAI, the most famous AI lab on the planet, made the most famous model on the planet GPT.And of course, it all hit everyone's radar when GPT when ChatGPT came out, uh, which was GPT 3.5.If you're keeping count in, what, November 2022 around then and ChatGPT, of course, uh, reallymoved the needle.That really changed our perception on what these models were able to do.And since then, OpenAI has released many more of these.GPT right now, for me, we're on GPT five.Maybe by the time you watch this, there's another version.But GPT five is the latest.And along the way, they also came out with a different range of models called the oh models.010304 mini.Um, but now the oh range and GPT range have converged.And GPT five is a kind of combination of both.And we'll cover more of that another time.But that is GPT from OpenAI, the most famous model and the most famous lab on the planet.And the next model to mention is, of course, OpenAI's mortal enemies, anthropic and their models,called Claude and anthropic is also a West Coast US start up.It was founded by people that used to work at OpenAI.The OpenAI turncoats that started the big rival, anthropic.And Claude is the model that a lot of people in the industry love to love.Claude is my favorite and many people's favorite.Claude, of course, comes in different varieties.The very small version is called haiku, the middle version is called sonnet and the bigger versionis called opus.But sonnet also happens to be right now the most powerful one.Right now for me, we're on Claude 4.5 sonnet, and it is very, very powerful.And it's kind of neck and neck with GPT five, as we will see.But Claude, very powerful model.It's also, of course, the model behind Claude code, the coding agent platform.And Claude Claude is great fun to work with.And of course, we'll spend a lot of time with Claude on this course.And Gemini is Google's model.And Gemini, of course, is the big closed source version of Jama, the open source model that we playedwith yesterday.Gemini.It's it's fascinating that Google began this game very much at the back of the pack.They were late to the party.Do you remember Bard?Bard was Google's early model.That was something.They were a laughing stock for Bard.It was just awful, and a lot of people predicted that they'd left it too late, that they weren't goingto be able to catch up with OpenAI and anthropic, but Google absolutely caught up.The Gemini, the latest one for me is Gemini 2.5 Pro, but Gemini three is poised to be released anyday now.I'm kind of hoping it's going to be released while I record these videos.You're going to get like a live reaction moment.Uh, but Gemini three, a lot of excitement for that.If you've already got it, don't worry, I'll have updated the labs.You'll be able to use Gemini three instead of I'll be stuck on Gemini 2.5, but Gemini.Amazing set of models.Also, Google has made a lot of the smaller versions of it available for free for students, so youmight in your region be able to get a deal to get a cheap or even free version of Gemini.So that also goes down well with lots of people.That is Gemini from Google.And is there a fourth one to mention?Is there?Of course there's a fourth one to mention.I know you know the fourth one.And you're like, oh, what about.Yes, yes, yes.There is of course grok.Grok with a K from Elon Musk from X.ai.A lot of people love grok for various reasons, and we will cover, uh, and grok is definitely thesort of the fourth of the big four.And of course, there are a bunch of other big frontier models.There's the model that sits behind perplexity that I know is popular, lots of people.There's the one that the Cohere Canadian company has, um, there's, I guess, uh, Mistral, the FrenchAI company, has both a closed source and an open source model.So so that's another one.So there's lots of others in the kind of next tier.But these are the big four.These are the ones that grab the headlines.And we'll be using all four during the course.But but you don't need to have uh keys to all of them.You can watch me doing them.Uh, but we will get to use them all and compare them.And we will find out in week four how you pick the right one for your task.So that is the closed source.Let's talk about open source.


================================================================================


# 1.15 Day 2 - Open-Source LLMs- LLaMA, Mistral, DeepSeek, and Ollama

And now to talk about the open source side of the picture.Some people might still call these frontier models, but I tend to reserve that for closed source.So the open source guys, the first and foremost would be meta with their llama series of models.And really, meta was the first company to go out there and open source their models in In Anger.It's possible one might.If you're being uncharitable, you might say that meta felt like they had they had slipped.They hadn't been able to keep up with OpenAI and anthropic in terms of being in the headlines for havingthe strongest models on the planet.And so they were looking for a way to really set themselves apart, and they took their stance thatAI models should be open source and they open source llama.And as a result, everyone talks about meta as being one of the the leaders in the field because theyhave promoted open source AI.So, you know, maybe it was an intentional ploy to keep the headlines, but for whatever, whateverit's worth, they've been very successful.Everyone uses Lama will use a lot on the course and it's a fabulous model.Lama for right now is the biggest, most powerful one that they have open sourced.But also Lama 3.2 is particularly notable.And you might have discovered this yourself because it comes in very small varieties.And as a result, it's easy to run it on your local computer.Lama 3.2 comes in a 1,000,000,003 billion size, both nice and convenient to run locally.And sometimes people call these things SL instead of LM, a small language model instead of an LM,but really with 3 billion parameters, it's still pretty large.So that's Lama 3.2, which is the small one, which is super popular, and Lama four is the bigger one,Lama from meta and then Mistral, the French company has come up with a bunch of open source models.One of them is called Mistral because it's a type of model called a mixture of experts, which meansthat it has lots of smaller models within it, and it directs traffic to different specialist modelsdepending on the kind of question you're asking.So that's Mick's trial.Quinn is a model or a set of models from Chinese company Alibaba Cloud, the cloud division of Alibaba.And Quinn is a remarkably powerful model that I very much enjoy using.And it's not as well known and as popular as models like llama, but it's really great, and I recommendthat everyone does give Quinn a shot because it's a super powerful and Gemma is the one that we alreadylooked at.It is the cousin of Google's Gemini.It is the open source cousin, and it comes in a lot of different flavors.And it's most notable for having this absolutely minuscule model with, what was it, 270 million parameters,which is definitely a small language model.And it's still I mean, it's not particularly impressive to use it, but it's still able to, to interactand to, to generate language.So it's pretty impressive.And then Fei is Microsoft's range 54 is the latest at the moment as of recording this.And those are their powerful models, particularly good, I think, with tool calling and with withcommercial work.And we'll see how to compare all of these, as I say, in week four.And then the one that everyone loves to talk about is, of course, deep seek from deep seek AI becauseit took the world by storm about six months ago and it came out.And it is important.I think I already said this.It's important to keep in mind what it was about deep seek that caused so much excitement.It wasn't necessarily that it's a super powerful model, because it's not as powerful as as GPT fiveand as Claude Sun at 4.5 or even close to it.What was impressive is that the Deep Seek AI team were able to make a model that is at the frontierlevel of capability, but at a fraction of the training cost than OpenAI spent.OpenAI spent north of $100 million training GPT and Deep Seek.I think it was about 4 million or something that they said they spent training deep sky.So it represented this, this huge, this step change in efficiency of being able to build a frontierlevel model very quickly and easily.And they open sourced it, which is amazing.The main version has 671 billion parameters, so it's way too big to run on your computer.But they also have made smaller variants of it that you can run locally, that you may have seen whenyou were looking at the models on llama.The way that those are actually built is kind of kind of funny.It's they're not actually deep seek themselves those smaller variations of deep seek.They are in fact versions of Llama and Quen, those two models you see earlier up on the list.They've taken small versions of Llama and Quen, and then they've trained it more with synthetic datathat was generated by the big deep seek.So again, they used big deep seek to generate lots of fake data, synthetic data from deep seeks training.And then they've used that to educate these smaller Lama and Shen models.And that process is known as distillation.There's actually various different ways of doing distillation, but that's one of them.And that's what Deep Sea did, and that's how they made various small variants.And you can use those variants in Lama.And it's great fun.They're available for all for free.And wait, there's one more.There's just one more open source model to mention.And you know exactly what it is.It is, of course, OpenAI's GPT, the open source version of GPT that OpenAI released for me quiterecently, like a month ago or so, six weeks ago.And it made a big splash.And people actually say that OpenAI might have done that because their hand was forced by deep sea,that open sourced this model that was almost at OpenAI's level.So that's one reason, perhaps, that motivated OpenAI to open sourcing GPT and GPT OS.It comes in two sizes, the one that we use, the smaller 120 billion.We used that yesterday.And then there's a much bigger 120 billion version that's far too big to fit on my computer.Maybe it fits on your computer, in which case, anytime you want to lend me your computer, that'llbe great.But it's a very powerful model.It's very impressive indeed.And it's remarkable that OpenAI have open sourced it, which means it's also available for free forany of us.And one final point to make before we go to the lab, there are these three different ways that we canuse models.And we'll be using all three of them at various points, but I just want to make sure that you feelcomfortable that there are these three different things, and one of them is using packaged products,products that have a user interface and have functionality built in.And the most famous of them is ChatGPT.It's not you're not using a model, you're using a product.And it's important to have that distinction.ChatGPT comes with a user interface.It comes with features like memory and web search and stuff that's been built into ChatGPT GPT product.So when you're using it, think of it like it's a product.It's had some some people have worked on the model behind that, but also some people like you and me,some AI engineers have built the user interface, the functionality, the the glue that glues a userexperience to a model that's ChatGPT anthropic has it's one two clawed through through the user interface,and there are a bunch of others.Then you can call large language models that are running on the cloud using APIs.So there are ways that you can make a call to an API.And you can call them directly.So you can call OpenAI like we did yesterday.And there are also frameworks that code you can run locally that will manage then calling OpenAI.And then finally you can also run managed.You can you can call a service like bedrock from Amazon or vertex from Google, vertex AI or Azure'sML from Microsoft.And these are our managed services, which themselves will run these models, and we can call them andthey will call the models.And there are some other kinds of APIs as well that we'll find out.There's one super confusingly that's called grok spelt with a Q that is different to Elon Musk's grokspelt with a k.So we'll use both grok in due course.But but watch out for that one too.That's a different thing.That is it's basically an ability to run open source models super fast on the cloud using proprietaryhardware.So we will we will look at that too.And then there's things like open Router which is very popular, which is a platform which you can connectto.And it will then route your requests to many different model providers.So all of these are different types of cloud APIs being able to call models in the cloud.And it's you can call closed source and open source models this way.But there's something else you can do as well.If you're working with open source models, then you can actually download the code and you can runthe open source model yourself.Not on the cloud, not through some API, but that you are running it.And there are many different ways of doing that as well.But there's two in particular that we'll use on this course, and they're different and it's a bit confusing.And one of them is Olama and one of them is called the Hugging Face Transformers library.The hugging face Transformers library is actually the simplest one to explain, because with that youare just literally taking the code, Python code and C++ code that was used to to write these models.It's a it's a neural network.It's a kind of data science model, which is a large language model.And you run that code yourself in your Python interpreter and and you execute it and you get the outcome.It is just taking the open source code, taking all of the weights, the billions of weights, and runningit yourself in code that is hugging face and it's hugging face is a is an open source vendor, and theTransformers library is the there a library that wraps a lot of this code that you can use very easilyyourself?So that's that's that.So then what is Olama?Well, Olama is itself a product to make it very easy to run fixed open source models in a way that'sbeen packaged and prepared so that in just a couple of steps, like we did yesterday, you can run anopen source model yourself on your computer.So it's packaged.It only takes very specific versions of models.The code has been optimized into very efficient C++, and the weights have been compressed into a specialfile called a GIF file.And you download that onto your computer.And then you run this product called Olama on your computer.And that gives you an API locally on your computer.It's like an API, but you're not calling something out on the cloud, you're just connecting to yourown computer and calling an API there.And then this Olama product is then running your model in a way that's a bit like hugging face.It's just more efficient, more packaged, and just focused on a smaller number of models.The models that you looked at yesterday in the Olama platform.So that's olama packaged, fast, efficient, uh, platform runs on your computer and provides an APIon your local computer for you to connect to.And that's the difference between those two.I hope that makes complete sense.And that is the third category direct inference of an open source model.And if you're new to the word inference, inference is a fancy name for saying running a model.Inference is what you do when you got a model and you've got some new inputs and you want to get theoutput, that is called inference.And we'll be doing a lot of that in the next eight weeks.And on the subject of inference, I think we should go and do some right now.I'm going to take you back to cursor, and we're going to do a sort of repeat of yesterday, but we'regoing to be able to do it for Olama.And I'm going to talk a bit more about what's actually happening to give you some more insight intohow this all works.Let's get to cursor back to the lab.Let's get building.


================================================================================


# 1.16 Day 2 - Chat Completions API- HTTP Endpoints vs OpenAI Python Client

Okay, well, here we are, back in cursor, one of my very favorite places to be.And we're going into the week.One directory on the left.Check the LM Engineering's in block caps here.So you're in the right project.And then go to day two, which is where we find ourselves right now.And I need to remind you before you do anything, you have to go to the top right to where it says Selectkernel for you.You click on that.You click in the Python Environments option.The first option you pick the dot VM.That's Python 3.12 that is matching this VM right here.And that means your kernel is set.Welcome to the day two lab.So I just wanted to mention here before we start that there is a page of resources on my home page andI've linked to it here.I'll try and keep adding useful resources.So do keep that bookmarked.You can go there and check it out right now if you wish.Okay okay so we're now going to go back again to doing what we did yesterday, but do it with a littlebit more discipline.And I want to tell you about this thing called the Chat Completions API.And this is the name of it's the simplest way that you can call an LLM, particularly a frontier LLM.And it's called chat completions because it's a nod to what you're actually doing.When you call an LLM, you're giving it a chat a conversation so far, and you're asking it to predictthe most likely message to come next.And you can think of that as as like completing a chat.That's what the LLM thinks it's doing.It's it's it's like predictive text.It's just trying to predict the most likely words to come next.And as a side effect of that, it happens to be really good at answering whatever questions it's asked.But all it's actually trying to do is to predict the most likely next few words, or as we'll shortlyexplain, the most likely next tokens.Okay, so it was actually this chat completions API.This this approach was invented by OpenAI first and foremost.But it was so popular the structure and style of this type of request became so popular that it's becomeubiquitous.All of the providers offer the Chat Completions API.It is the kind of standard way to interact with an LLM.And we're going to start again with OpenAI.I know some of you are antsy to get off OpenAI and use free models.Your time is coming any second now.Just watch this if you don't want to use OpenAI, and we'll get to you in a second.All right.So first of all, I'm going to do a repeat of last time where we use this thing called load dot EMV,which is going to load in the secrets in our EMV file and check that we have OpenAI key set.So this load EMV function is something which which makes which loads in anything you've saved in yourEMV.And then we'll just check that the key is good.It says API key found and it looks good so far.And if yours doesn't say that, then you know what to do.Check the EMV, check everything, and look in the look in the troubleshooting and ask me if you getstuck.Okay, let's talk about end points.Now, I imagine most of you know exactly what an end point is.It's one of those words people throw around all the time.But you might not.And for people that don't know about it, you should quickly find out.I explain it all in the Technical Foundations guide.There's nothing special at endpoint.It's an http url which you can call to, to to to make an API request by hitting some web address.And that web address would be known as an endpoint.It's a way that you have an API.But if things like API's and endpoints are new to you, then read that guide.Okay.There's an endpoint in particular which might interest you, which I want to show you right now.And it's an endpoint, an API endpoint offered by open AI.So in order to call this endpoint, which I will just do, we first have to set a couple of things whichare very common for any HTTP request.There's HTTP headers, the headers that go down in that request.You normally you probably if you know anything about web requests.You specify what content type, what type of thing you want to come back.And that's a way of saying, I want Jason to come back, please.And then this is a fairly standard way of, of, of sending down a secret in an HTTP message that authorizesyou, you put in a header called authorization and the value of that header, there's the word bearerand then a space and then some secret that identifies you to the third party.And in this case we are going to stuff the OpenAI API key.The thing that I got just here, we're going to stuff that into the header.And then I've got something here called payload.Payload is just a chunk of JSON.And it's going to be a dictionary a dictionary with two keys.One of them is model, which is going to be GPT five nano, the tiny version of GPT five.The latest model, the one of the strongest on the planet right now.And then the second field in this dictionary has a key of messages.The value is, you know, this is a list of dictionaries.It's a list you can see there.It is a list.Each dictionary, let's put that on another line, has a key role with value user a key content.And the value is tell me a fun fact okay.Nothing special here.Let's look at this.Uh, there is the payload JSON.It's is just what I said.Model GPT five nano messages, a list of dictionaries, role user content.Tell me a fun fact.Okay, that's a chunk of JSON.Uh, headers and payload.What we're now going to do is we're going to send that to an endpoint.This is the endpoint API.V1.It's chat completions.And sometimes when you make a Post request people think of that as like creating a new resource forpeople that are like rest, rest people.Uh, that's that's sometimes called creating a resource.So you could think of this as being like chat completions create some familiar.Uh, so chat completions create.We're passing in our headers with our API key.And the payload is this JSON blob right here.So let's run that.And what comes back is a bunch of JSON.We asked for JSON back.Let's see what we get back.Let's see what kind of JSON we get.Here it is.Uh so it's a chunk of JSON as like an ID object, various stuff.And then it has a field called choices and Choices.In this response, choices is a list.And the first item of that choices with index zero is something which has a field message.And that message is itself a JSON dictionary.And that has something called content, which is fun fact.There are possible unique games, more possible unique games of chess than there are atoms in the observableuniverse.About ten to the power of 120 There is a fun fact.And so that fun fact came back in the JSON response from our call to GPT five to that endpoint, thatURL that I just gave you.And so there's obviously there's another way I could do this.Let me get a new code thing here.Let's say, uh, um, we said response JSON.That was the JSON that we just were just looking at right up here.Uh, I spelt response wrong.That's that's what you get if you make me type Response.json.So we could say, okay, let's look in the choices field.I guess it knows what I'm doing here.Look at the first element in there.Element index zero at the field message at the field content okay.You see that.That's you know what that's going to do.We're just going to choices the first one message content.And let's see what that prints.It prints that that very fun fact of course that we just looked at uh, the more possible unique gamesof chess than there are atoms in the observable universe.Okay, but you know what?Uh, so so this is a perfectly good way to call OpenAI in the cloud using an HTTP endpoint.And it's fine.We could do this.We could we could always type this.But it is kind of messy fussing around with JSON, looking in dictionaries at keys and things like that.It's kind of hokey, and it would be a real pain if every time we wanted to call GPT or any frontiermodel, we had to stitch together these HTTP requests, call that slash chat completions with a postor create and then be be sort of navigating our way through JSON objects like this.It would be a pain.It would be nice if there were a better way to do it.Yes, yes, I know you get the joke.You know what I'm gonna say?There is a better way to do it.And OpenAI created this better way.They made a package called OpenAI, and that package is what's known as a Python client library.Python client library is nothing fancy at all.A Python client library that you often use for APIs all over the place for you.Name it for APIs, for sending emails, for APIs to do so much.Um, and these things are typically very lightweight libraries that manufacture an HTTP request to anendpoint.And with what comes back, it turns it into Python objects so that you're not messing around with thiskind of stuff with with these, uh, ploughing your way through JSON dictionaries.But you can just write some nice, elegant Python code, and that is all the OpenAI library is.It's a Python client library that wraps a call to an HTTP endpoint.It's very simple.It's completely open source.You can open it, look at it, look at all the code.It's perfectly simple.Some people, the first time they come across this, think that when you're dealing with OpenAI, theobject and the code, that somehow we're sort of running GPT and we have some some fancy code from OpenAI.Not at all.It's vanilla code that just wraps making a web request.And we'll we'll quickly use that code now, but it's going to be much more familiar to you.


================================================================================


# 1.17 Day 2 - Using the OpenAI Python Client with Multiple LLM Providers

All right, so I'll do some more typing since since the haters will yell at me if I don't, so I'lldo some typing.Let's do it the proper way.Let's create an OpenAI Python client.You just say OpenAI equals OpenAI.Oops, like so.And it by default looks in the uh, in the environment variable OpenAI API key by default if you don'tspecify one.So that's just just hard coded in there.And I have to say from OpenAI import OpenAI to be able to do that.Okay.And now what we're going to say is, uh, response equals I guess it just fills it in.What's the point in me typing OpenAI chat dot completions dot create.So that is basically saying I want to go to slash chat completions and I want to do a post request,which is like doing a create.And I'm going to pass in like I don't have to manufacture JSON.I can just say model equals GPT five nano five nano.Oh, I could just press tab.You know it.You know what I'm gonna say?messages is that list of dictionaries that I passed in right there.And then instead of this fussy JSON plowing, I can just say response dot choices.Zero choices, zero dot message content.And I'm just going through Python fields.It's actually pedantic object of course, for people that know this stuff.Uh, and uh, as a result, it's got all of the, all of the nice features of using proper Python methodsand attributes like, like the ID can check, I've spelt it right and things like that.I'm not just guessing, uh, values to to look into the JSON.I know that I've got this right.And so I run this and we should get ourselves a new fun fact.And the thing to, to understand, uh, here we go.Bananas are berries, but strawberries aren't.Strawberries are not.And and there we go.We've got our explanation for that.A lovely, fun fact from GPT five nano.I hope you've been enjoying yours too.Of course, the point I'm trying to make to you is that this is exactly the same as doing it the boringway.It's just this looks a little bit more elegant and simple.And the reason I show you this is so that hopefully, and maybe this is all obvious to you, in whichcase, well, hang on in there.But if not, it just shows you that's all that this library is.At the end of the day, it's just a nice, simple wrapper around making an HTTP call to an API in thecloud.And it by default it looks for OpenAI API key.And that's what it puts in that bearer in that that authorization field in the headers of the HTTP request.And it's as simple as that.We have made our call to OpenAI on the cloud.It looks like we did it with code.In fact, it was an HTTP call over an endpoint.Over an endpoint.What I meant to say was it's an endpoint call over HTTP.All right.So I've already typed all this.We can get rid of that.Get rid of that.So so then something cool happened.So then OpenAI's chat completions API was so popular that all the other providers started to offer anidentical end point.So you could use exactly the same web request and and just call a different model instead, becausepeople sort of converged on OpenAI's approach as a way to do it.So everyone else offered them to Gemini.Google, for example, they had one endpoint that was very specific to Gemini, and they were like,oh, I know what, we'll just give a second one that's identical for anyone that wants it.And out of all of them, anthropic held out the longest.They they obviously their enemy OpenAI.They didn't want that to become the standard.So they held out.But in the end they relented.And they also created an OpenAI compatible endpoint.So now all of them have it.And for example, this is Google's one.It's Https generative.OpenAI.It's got OpenAI in its name.That is their endpoint for making OpenAI compatible request.And because everybody did this, OpenAI decided, oh, I guess we'll be good corporate citizens, goodAI citizens.Reasons.And we'll say to people, look, you can use the same OpenAI library.The same client library will allow you to say, hey, I don't actually want to talk to OpenAI's endpoint.I want to switch to a different endpoint like this.You can say OpenAI base URL equals and switch to a different URL and pass in a different API key.And if you do that, then you can talk to Gemini using OpenAI's client library because it's just simple,lightweight code.That's it.And so yeah, just to be crystal clear, even though OpenAI is is in our code here, we're not actuallyusing anything to do with OpenAI's models.We're just using their lightweight code to make an endpoint request.And you might know all this back to front.This might be super obvious to you, but you'd be amazed how many people are confused by this and thatthis can be really refreshing to understand exactly how this works.So if you're in that category, then then, then I'm pleased to hear it and it's going to hopefullymake a lot of sense what we do next.Okay.So uh, we are going to look at Gemini's base URL.That's the one I just showed you.It's like a Google URL with OpenAI stuffed at the end of it.In my EMV file, I have a Google API key.Now you might not do it, in which case, don't worry, just just skip this next one.But if you do, if you've got a Gemini or a Gemini API key, then then hopefully you've added GoogleAPI key to your EMV file.Uh, then then you can run this cell and uh, I'm just going to run it and it's going to confirm thatit found a Google API key in my EMV, and that it has the right format R and Z.And if you yeah, if you want to get yourself an API key, you can you can head over.The instructions are in guide nine.That will tell you exactly what you need to do.Okay.So now look at this line Gemini is OpenAI.Remember it's just the lightweight Python client library.We give it the Gemini base URL and we give it the Google API key.And that has now given me a, a basically like a class that's ready to connect to Gemini.Okay.So what we can basically do is take exactly the same code that we had before.Come back up here.I can just copy this.I can come down here and I can use the identical code.The only difference is I'm going to change the word OpenAI to be the word Gemini.And I'm obviously also going to change the model.If I try and talk to Gemini with GPT five, it'll say what it's not.That's not our stuff.So we have to make this something like Gemini 2.5 Pro.We'll get a fun fact from the top range model.Let's check.This looks right.It does.Let's give this a whirl.So we are now asking Google's strongest model on the planet.As of now, you might be a Gemini three user, in which case you will have great fun getting a hopefullya more profound fun fact than my one.Uh, and here is the answer a group of flamingos is called a flamboyance.Uh, there you go.Uh, if you knew that, then you're obviously very good at trivia.Um, but there is the short, uh, and punchy fun fact from Google's Gemini 2.5 Pro, and I hope youenjoyed it.


================================================================================


# 1.18 Day 2 - Running Ollama Locally with OpenAI-Compatible Endpoints

Okay.And now we also can use the same trick to connect to a llama running on our local box, because llama,the product also gives an endpoint locally that is compatible with OpenAI as well.So first of all, before we do anything, let's just make sure llama is actually running.If you if you installed it before yesterday like you were meant to, then it will be.And if you run this little test here, you should see a llama is running.If you don't see a llama is running, then make sure that you've installed llama.There are instructions in guide nine if you if you missed that one, or of course in the Readme forthe project.And then you might need to run a llama.Serve in a terminal, open a terminal and type llama serve if you need to, or just double click onthe llama application to launch it.Any of those things might be needed if you do llama serve, and you get an error that says that theport is already in use, it means it's already running.You're in great shape.Okay, so the next thing we're going to do is download the model called llama 3.2.And if you already tried this yesterday.Then you don't need to do it again.And if you've already done it, it will just run really, really fast.So if I run this now, it should run nice and quick for me because I already have it.But for you, it probably took a little minute.If it's too big for you, this this takes up a few gigabytes.Um, and it might be bigger for most machines.It should be fine.But if not, just add colon one b to get the really small version of llama.But assuming that this is good, then you've got that.Okay.And now we use the same trick and llamas base URL looks like this localhost 11434 slash v1.That v1 is like a call out to the fact that OpenAI had slash v1 chat completions create.Uh.So, uh, yeah, this is, this is V1 by analogy.And we can create OpenAI.The base URL is llama base URL, and the API key can be anything you want.It gets ignored.This is a local a local model.There's no your credit card isn't there?There's no no secret needed, of course, but I think it's a it's a mandatory field.So I think you have to pass in something you can pass in any, any, any word that you like.Um, and now we're going to just take the same code as before, the same ones we've used a couple oftimes now.Copy.And let's get a fun fact here.So we'll, we'll put it down here in this cell.I'll paste.And of course we're going to not do, um, I'll run that cell and then we're not going to do Gemini,we're going to do Olama.And for the model you need to put the name of the model that you just downloaded.So we'll use llama 3.2, uh, and uh, otherwise exactly the same.Now if you've got the smaller variant then you do colon one b there.But you need to have done a pull to have brought that model in or you'll get an error message.And so ready let's run this and see what happens.Let's run it, run it, run it.And here we go.Did you know there's a type of jellyfish that is immortal?The turret, the turritopsis, also known as the immortal jellyfish.There you go.If you didn't know it, you know it.Now, by the end of this, you're going to win at all trivia challenges.There is your your next fun fact.But the cool thing is that the last two fun facts we retrieved from Frontier models OpenAI in the cloudand from Gemini and Google's Cloud.This one was just created on your local machine by llama running there, even though it was pretty quickfor me.Uh, that's that what was actually happening there.And that's very satisfying.There's something nice about that.And if you if you care a lot about data privacy and you want to be absolutely sure that that the dataisn't leaving your computer, then this is the way to do it.You could unplug from the internet and I would stop talking to you.Uh, but but you would still be able to do this.Uh, so, uh, so so that's that's a great thing to know.And I just put here the obvious.I mean, we'll talk much more about this in due course, but obviously the big benefits, there's noAPI charges, it's running locally and data doesn't leave your box.The downside, of course, is that llama is something like a thousand times smaller than using mostfrontier models, and you'll see that that's reflected in in the.How intelligent it is and and how capable it is at solving difficult tasks.So obviously those are the trade offs that you have to make.And there are many more trade offs that we will talk about in time.Okay.So with that, it's time for you to experiment and start playing with with different models.And I want to show you one more open source model.I want to show you a variant of Deep Seek.You remember I told you that deep seek came in much smaller sizes.The main one would never run on my computer.The smaller sizes would.And in particular, I want to show you one that's called deep seek R1 colon 1.5 B it's very small 1.5billion parameters, a small version of deep seek.And as I explained before, this is in fact not deep seek.Originally, what they've done is they took Quene the model from Alibaba Cloud.They took the 1.5 billion size version of Quene, and they trained it more with extra data.And that extra data was data that was generated by deep seq.The big model.It generated tons and tons of data to sort of show its intelligence, and it trained the little quonto be able to try and replicate deep seq.That's how it worked.Known as distillation distilling.So we can we need to start by making sure that we pull this.So I do a lama pull.There we go.And kindly cursor fills in for me.Now I haven't downloaded that.So this is now going to run.So you can see how long it takes for me.Uh and it's downloading.Hopefully my computer isn't going to free freeze up, but but the trace is coming there.So once that's downloaded, we'll be able to then run some code.So let's let's get started on this.We're going to, to basically be able to take the same thing that we did before this exact code.We don't need to we'll still be using Lama locally.So we don't need to do anything new there.And we can just change this here.The model name to be the model name of deep seq R1 1.5 B.So I'm just going to be able to replace that in there like so.And as soon as this is finished pulling so that we have the model locally, I will now be it.There we go.It says success.I should now be able to call Deep seek R1 1.5 be deep seek distilled into Quen running on my local computerwith the same message.Tell me a fun fact.Let's see what happens.Off it goes.Uh, here's a fun fact for you.Did you know that the sun looks like a golden crown during sunset, with its gold colored reflectionon the horizon?The rest of it is pale yellow.This interesting correlation change coloration, not correlation coloration, changes as the weatherchanges.When the sun is high, it looks colder, and when it sets it appears hotter yellow.So.Aha!So fair enough.Not such a riveting fun fact, but not no surprise, this is twice as small as llama, and it's severalthousand times smaller than the frontier models we worked at before.So to be honest, it's super impressive that it even answered the question and came up with somethingcredible in the first place.But this gives you, first of all, a good sense of how you can run deep seq and any any open sourcemodel locally, and also some perspective on the different abilities of the different sizes of models.And now it's over to you to experiment with different models, pull them and run them.Keep experimenting.It's all about experimentation and have fun asking questions like fun facts or whatever question youwant to open source models running locally courtesy of Olama using the OpenAI compatible endpoint.And now the assignment for you please a homework assignment for day two.And that homework assignment is look at what we did in day one, the summarization of a web page, andreplicate that using Olama using any open source model of your choosing, using Olama, running it locallyso that you have a web page Summarizer that gives you a summarized markdown version of a website usingan open source model via a llama.That's what you should complete.Put that in the cell right here.Uh, there is a solution and the solution is folder.But don't look there.You can do it.Easy peasy.Copy and paste from day one.Make a change to the to the the open AI Python client library, just as we did above, and figure outhow to make that work.And when you've done so, then by all means put it in community contributions.Put your change in this folder community contributions, and then look to make a PR so it can be includedwith the others.And particularly try to see if you can't do something fun with it.Maybe mess around with the system prompt, see if you could maybe change it to a different language,or use a different tone, or have some aspect of it that makes it unique to you.A different take, a different take on summarizing a web page using an open source model.That's your assignment.I can't wait to see what you come up with.And with that.With that, your 5%, your 5% along the journey.It's the end of day two.You've got got two days in and already hopefully you've got a good handle on what it's going to taketo get you to being proficient on the steps along the path.You've got your first impression of the frontier models from our experience with OpenAI and Gemini,and you've also got an impression of the open source models, and you've used the OpenAI API, and you'veused Olama to make web page summaries.You've done your task.I hope your assignment, you're feeling more confident with this.And for me, most important of all, if you didn't already have this clear in your mind about what aPython client library is, what a what an endpoint is, and how using OpenAI the chat completions APIsimply a web request then.Now hopefully that is crystal clear for you.What we're going to do next time we're going to be comparing frontier models.We're going to be to be playing with them through the products, through through the their websites.We're going to be getting a sense for what are they good at and what are they not so good at as an interestingway to get a deeper perspective on them?I can't wait to show it all to you.Lots to get done.See you on day three.


================================================================================


# 1.19 Day 3 - Base, Chat, and Reasoning Models- Understanding LLM Types

Welcome to day three.I have bad news for you.Today is one of the very rare days when we are not actually going to do a lab.There's no lab today, but I have something better.As you will see.But first, let's quickly do what we always do.Recap what we've done so far.You know the progression to become proficient.You've got your first impression of frontier models OpenAI, Gemini, llama, various models throughllama like Deep Distilled and Llama 3.2.And you've built something that can summarize web pages using OpenAI API and hopefully your llama aswell.What you're going to be able to do by the end of today is compare a bunch of frontier models.You'll know the difference between chat models and reasoning models.You'll appreciate what they do well and where they struggle.Let's get to it.So there are three different breeds of llms that that reflect what they've been trained to do, thetasks that they've set out to accomplish.And the starting point is known as a base model, is an LM that's just there to take a sequence of informationas the input and to predict what would come next.That's all it does.And these base models, you don't come across them very often.We will do later in the course when we get to work with our own.Um, but they are, they are before it's been taught how to do things like chat with someone.It's just about completing the sequence.But you have a base model yourself and you use it probably quite often and it's in your pocket.If you if you bring up your phone and you go to to write a text message and you use predictive text,if you say sort of hello there, and then you see what it prompts, you would come next and you repeatedlypress that button that is like you're using a base model.It's just completing the sequence, giving you the most likely output that could come next after a particularinput.Repeatedly.And every time you select a word that goes into the sequence, and then it's predicting the next thingto come after this sequence, that is a base model.And before ChatGPT came out in 2022.Before that was was the earlier versions of the models, like GPT three.That was just a base model and people that were using it like myself at the time, we were very familiarwith, like, you put in some text and then it would start predicting what would come next, and therewere various ways that you could force it to try and answer questions.You'd say, like question in the prompt.You'd give it like Q and A question and then a colon and the answer Q colon, another question, a colon,an answer.And then you'd put q colon and the actual question you wanted to ask.And then you'd put the letter A, and that would prompt it to be thinking more like it's in questionanswer mode and to give you an answer.And and that was the way a lot of people used these base models.And OpenAI had a brainwave and thought, hang on, we could train our models a bit more with data structuredin this way with kind of one message response, one message response, and and doing that became knownas making a chat variant or also known as an Instruct variant, a chat model, or an Instruct model.It's a model that's been trained to work in this kind of prompt style.And they came up with this idea that there should be one overall piece of information that describesthe whole chat, and then there should be like a message from, from, from one user to the AI and itsreply and then another message and that, that first that overall construct became known as the systemprompt.Uh, you knew I was going to say that.And the the next message was called the user prompt.And then the assistant reply and user prompt assistant reply.And that's training.A model in this way became known as making a chat variant.There was a particular approach they used called reinforcement learning from human feedback RL, andit was that that got from normal GPT to ChatGPT.It was the chat variant.And so then chat models were all the rage.And we will be chatting with with ChatGPT.And quite quickly people noticed that there were some tricks, some prompt engineering tricks to getmore out of the model.And one of them, which became known as chain of thought.Prompting was just as simple as if you ask the model to do something, you add as the last sentenceuh, please think step by step.And just by by virtue of saying that you'd get something that would apparently do better, it wouldgo through things methodically, and the sequence that it would predict would end up being more likelyto solve the problem just because you told it to think step by step.Which seems super hokey, but it kind of works.And so again, that gave people a brainwave.Maybe we could train the model with lots of examples that show it thinking step by step, and then showit getting to a conclusion.And so doing that training.Training a model so that it would a chat model would then would then think through what it was doingbefore it did.It became known as making a reasoning model or making a thinking model.And that led to these reasoning models.They are models that have been trained to first output their thinking steps and then give the answer.And we saw that I think in the very beginning, in the first day, the instant gratification part,when when we saw OpenAI's OSS model thinking things through before it gave its reply, and that thatis what a reasoning model is.It does the thought process first and now the the some of the most modern models are what's known ashybrid models.But that's not really.It's just really a variation on the reasoning thinking model that's that's able to decide how much thinkingit does.And in some modes it's much more similar to just being a chat model.It hardly does any reasoning at all.And in others it will go ahead and reason quite a lot, and it decides how much to reason based on thequestion you ask how how much of a puzzle it is, or if it's just in a chat mode, kind of just justa simple hi there, then it won't bother reasoning the answer to that.And so this kind of model that's able to decide how much thinking to do is what's known as a hybridmodel.And Gemini Pro 25 is a hybrid model.So it's got four and so is GPT five.These latest models are all examples of hybrid models.Um, and and then the latest version of the open source model, when they have both a hybrid model andthey have model which, which is just chat and reasoning in case you want to select that, you justwant one in a chat mode.And the amount of reasoning that a reasoning model does is sometimes called its reasoning budget orits reasoning effort.And there's a technique called budget forcing.When you make a model a reasoning model, think longer, you make it do more reasoning.And there are various tricks to achieving this.And they are quite remarkably, many of them are remarkably hacky and hokey.And it's I think when I, when I explain this to you, you'll be like, really?Uh, but it turns out that there's a great paper called S1 that you can Google.I'll put, put put a link in the resources.Um, S1 explains that it was a discovery from January of 2025.Uh, that what you could do, uh, when a model comes up with its thinking.Trace, if you wanted to do some more thinking, not not just to get to its conclusion, but to to togo back and do more thinking, there was this very complicated mathematical trick that involved somereally complex calculus.No it didn't.It involved just add the word weight in to the thinking trace.So it's come up with something it said like like I should I should consider blah, blah, blah, blahblah.And then you just insert in this sequence the word weight.And by virtue of doing that, it kind of continues that sequence as the most likely words to come afterweight.And it says like, wait, I should rethink this, am I sure?Let me reconsider.And So they discovered these scientists that just simply by adding the word weight periodically intothe sequence as you're making it predict the next tokens, causes it to reflect on its reasoning andreason a bit deeper, and challenge itself and weigh up the arguments that it's made and consider whetherthey're still accurate.And so just just adding the word weight got better outcomes.And that technique is known as budget forcing.And there are sort of various other ways of doing it.But that's that's the best known one.And the first time you hear that, you're like, surely it's got to be something more sophisticatedthan that.Well it's not read the paper and you'll see.And so, generally speaking, reasoning models with a high reasoning budget perform better in all ofthe different benchmarks really almost across the board.And we'll be looking at benchmarks and leaderboards and everything else in week four.So there'll be plenty of time to do that.But generally they do very well these these reasoning models.And so you might be saying to yourself, why don't we always use a reasoning model?Why even have the hybrids?Why have chat at all?And there are some some reasons.It's not always better reasoning models are better for problem solving.Uh, particularly if you don't mind wasting a bit.They will always perform better at puzzles, for sure, and they're just just generally more intelligent,like they score higher on any of the kind of intelligence related tests, chat models.They are, of course faster because they don't need to produce all of this reasoning a trace.So they're better for interactive use cases if you want to have chat, and the chat models are better,shockingly.Uh, so they are they are better in that use case.They are obviously faster and cheaper because they don't need to produce all of these, these, these,these, uh, these bits in the middle, the thinking tokens.We'll talk more about the costs and so on later.Um, but so they are they are better at that.And, uh, this is I put a question mark here because people aren't sure about this, but it does seemthat chat models are often better at more creative kind of just just content generation.If you want something to write an email for you, uh, sometimes you find that the reasoning modelskind of overthink.Perhaps what they come up with is quite cold and and analytical, whereas chat models tend to producemore, um, fluid content.But this sounds hand-wavy.There's not any there's not really good metrics on this.It's more of an anecdotal way that that people think about it.You should try it yourself and see if you agree with that.And a base model is better in a very specific case of when you are trying to train a model to do somethingdifferent, to give it a new skill, which is something we'll be doing later in the course.And when you're doing that, it's better to start with a base model.You don't necessarily want to be in a chat construct or in a reasoning construct.You want the opportunity to train it to have some different construct.And in that case, better to start with a base.So that's when you use each of those three flavors of model.


================================================================================


# 1.20 Day 3 - Frontier Models- GPT, Claude, Gemini & Their Strengths and Pitfalls

Okay, so let's talk about the big frontier models, the labs and the chat products.Sometimes these large models, also known as foundation models, you hear that term as well.And foundation model used very similarly to frontier.In theory, a foundation model is one of the kind of major building blocks that lots of other companiesreplicate.But but there aren't necessarily good good definitions for these foundation and frontier models oftenused interchangeably.So OpenAI, OpenAI, of course, have the GPT range of models.The GPT five, which is a hybrid chat and reasoning model.They also have the O range of models, which are pure reasoning models.But it's GPT five really replaces all of the O's and the prior GPT series.I still love using GPT 4.1, which is the variant that came right before GPT five because it's a purechat model and it's much faster than GPT five, even if you tell GPT five to do the least possible reasoning.GPT 4.1 is still much faster, much more interactive.So there are many use cases when I prefer it, as we will see.The chat user interface of course, is ChatGPT.You know it well, we will use it.So anthropic has Claude.Uh, and Claude comes in these three different sizes, as I said before, haiku, sonnet and opus.And uh, we'll probably be using sonnet mostly the middle size.And, uh, the version 4.5 is the latest that actually, for me, came out yesterday.Uh, but that's the latest.And maybe for you, there's a there's a new one, maybe for you, there's already Claude five.Uh, but and you should use the latest one and see how it compares.With what?With what I do.Google has Gemini.Gemini for me is 2.5.But I bet by the time you watch this, three will be out.And I'll be.I'll can't wait to try it.Uh, their chat interface, it's gone by some different branding, but I think Gemini Advanced is whatthey called it at one point, but people just call it Gemini.Really, their chat front end, their chat product and then x dot AI is Elon Musk's AI arm.Uh, that I think now now also owns the company formerly known as Twitter.Uh, but uh, x AI has their model called grok and their chat platform also called grok, spelt witha K, not to be confused with grok spelt with a q that we'll talk about another day and then deep sky.Here's another one.This is the odd one out here.Do you know why it's the odd one out?It's the odd one out because they open sourced all of their models, including their biggest model.Uh, so deep tech, AI, Chinese company.They have deep tech, and their chat product is also just called Deep Seek.And while I have these up there, I should also mention, of course, that OpenAI has their open sourcemodel as well.OpenAI has OSS that we used at the very start.Uh, so they've also come in to, to join with deep seek, perhaps, as I say, provoked by deep seek.They have an open source model as well.All right, so just to talk for a minute about what makes frontier models so great and then what aresome of their rough edges.So there's I mean, you know, I hardly need to tell you at this point, everyone is so familiar withwith some of the shocking things about frontier models, the their ability to synthesize informationlike we saw when we got it, to summarize web pages.I mean, this is just extraordinary.They are so good at answering a question in detail, with structured answers that are well researchedand have a summary.Ask it a question to weigh up the pros and cons of something, and you'll get back a remarkable answer.So we it goes without saying.Everyone knows this stuff.Uh, also generating content, of course, is what they do, but but doing it if you ask it to like,like draft an email for you or draft a presentation or draft how to go about solving a project theywill do that with phenomenal quality.They can build out a skeleton.I often use it as my way of starting some new initiative and just bouncing out, bouncing around ideasand fleshing out together.So powerful.And then of course, coding.We all know this.If you've used code, your mind will have already been blown by it.Their ability to write code, to write code in in a you know, obviously we'll be talking about agenticAI later, but but the ability to do it in as, as if there's, there's multiple concurrent activitieshappening and to iterate over things in some kind of a loop where it builds some code tests, it fixes,it builds more.I mean, this stuff is staggering.And I, I put here the first time I did this course a year ago, far overtaken Stack Overflow as theresource for engineers, and I had a chart that showed Stack Overflow going downwards in terms of thedaily visitors.But but now to say that is just so obvious.It's, it's it's almost crazy Stack overflow that once seemed like it was just unbeatable.Unbeatable.It was just where everyone went all the time.It's now hardly ever mentioned.People never go to Stack Overflow.You put your questions into ChatGPT or into Claude.Uh, and that's it.So it's the way that just in the space of a couple of years, the these chat llms have overtaken allother resources in terms of their ability to be able to, to problem solve and debug.It's just remarkable.And honestly, I've found that that Claude and ChatGPT have fixed problems that I was pulling my hairout over, and they've done so with ease.Sometimes they can't, but but often they can.And they do so very quickly and very clearly and explain exactly what to do.It's incredibly impressive.But look, I'm generally quite an upbeat and enthusiastic person, if you didn't notice that.But it doesn't mean that I'm all caught up in the hype.I am incredibly aware that llms have their downsides and I experience them every single day.Uh, there's certainly that.Whilst they are very strong in some scientific fields, they have incredible expertise in some areasand they are above PhD level in some, in others they don't have that knowledge yet.Their training knowledge has gaps and they can come through very strongly.They have this famously.This knowledge cutoff is when their training goes up until and they have limited knowledge beyond thatpoint.So like if Gemini is is helping fix your code, it will often do this janky thing when it changes yourmodel.If you're using GPT, it will take out GPT three or GPT 4.1, and it will say you've used a model thatdoesn't exist and it will put in GPT 3.5 turbo like two years ago, like like something which is sooutdated it's like an embarrassment.But it will do so very, very strongly, very, very angrily telling you that you've used a model thatdoesn't exist yet.It's like, goodness.Uh, and of course, many of these, the products you use, like ChatGPT built into them is the abilityto do a web search now.And this is functionality that AI engineers like you and me have built into ChatGPT that work for OpenAI.It's not part of the LLM, the model, it's part of extra code that's been added to give it a helpinghand.The model itself has the cut off when it was trained, and it doesn't have any information beyond that.And then of course, llms make mistakes and they can do so dangerously.They can be very confident when they make mistakes because all they're doing is predicting the mostlikely next, next word, next token.They're predicting what they believe comes next in a in a plausible way.So plausibility is what they've been trained for.They've been trained to sound confident.That's all they do.The very fact that the plausible most likely next word so often happens to be the truth.The reality is something of a surprise.We should be almost surprised that models don't hallucinate more.Uh, the fact that they so often are accurate and aligned with our task is somewhat remarkable.Uh, but it does mean, of course, that sometimes they do hallucinate.And when they hallucinate, they do so with so much conviction.And that's why it's particularly dangerous when llms are used to help code with junior people, withwith people that were coming new into the field.Initially, people thought llms would be most useful for junior people because junior developers, becausethey would help a junior developer like level up.But it turns out that's not really the case.They are most useful for a senior developer who can use an LLM to generate code, and they can watchit and correct mistakes and challenge it for junior developers, for sure.They're very helpful, as we've seen from cursor, but they can also be dangerous because they can leadyou astray.They can confidently come up with something that's just wrong.And I get a lot of people sending me code that is clearly been auto generated by an LLM that's goneoff on a crazy tangent.And of course, the person doesn't know.I'll belabor the point by giving you a concrete example.And, you know, I know this is going to take a moment of time, but I think it's useful for you toto hear this kind of thing.So a student on this very course sent me some code and said, it's not working.Why not?And it was it was kind of shocking.It was tons and tons of code, and it took me a while to get a handle on what on earth was going on.And eventually I got to see what was happening.So they were trying to chat with an open source model, but by accident they hadn't typed in the nameof the chat variant, the instruct variant of the model they had put in the name of the base variantit was using hugging face, calling the code directly.So you put in the name of the model and they use the base variant.And it was failing because it didn't know how to expect a system prompt and a user prompt because basemodels aren't trained with that.So it was failing and they were using an LLM to help them, and it had failed.And the LLM saw that it failed because they were passing in the the chat information, the system promptand the user prompt into a model that wasn't expecting it.So what did the LLM do?Well, it said okay, the model's not expecting this.We're going to have to go and convert this model into being a chat variant by manufacturing all theinformation it needs to be able to do that.And that's why there was pages and pages of really sophisticated code that manufactured all sorts ofthings to do with special tokens and, and all sorts of stuff.And so instead of understanding that the root of the problem was as simple as a miswritten name of themodel that the the LLM confidently headed off on sort of addressing the most obvious problem.Okay, the model wasn't expecting this chat stuff.We're going to have to teach it to expect the chat stuff.It didn't take a step back and say, no, no, let me challenge the whole situation.Taking a step back.Maybe the root cause is just that the user got the wrong model name, and that's such a great example.The junior person didn't know better.They didn't understand why all this code was generated, but things seem to be progressing.They got further along.More was happening.So it appeared to be working.They didn't.They didn't realize they were being led astray.They didn't see that 3 or 4 pages of code all looked kind of hokey and seemed like it was wrong.They went along with it, uh, and then at the end of it, when things didn't quite work, they weren'tthey weren't entirely sure why.And the for me getting all this code, it was very hard to, to understand what was happening, whattheir intention was.I didn't realize that most of this was generated by an LLM so long story, but it was from this verycore.So.So I hope it lands and it helps you see why.Uh llms are amazing at generating content.But but they are.They can be, they can go wrong.They can make they can jump to conclusions.They tend to not think and take a step back.They tend to kind of apply Band-Aids and try and push forwards, particularly with coding tasks.And so they perform best under supervision.That's the key takeaway, these these llms that you're using for things like coding or for other kindsof content generation, they perform best.Think of it like a junior analyst.You have an incredibly hard working, a tireless junior analyst that can go off and do work at yourbehest, but it's incumbent on you to check that work, to keep it on, on the track, on rails, andto make sure you are challenging it to make sure it's not jumping to conclusions and going off on acrazy tangent.That's your job.


================================================================================


# 1.21 Day 3 - Testing ChatGPT-5 and Frontier LLMs Through the Web UI

So look, most of this course is about connecting with LMS, either on the cloud through APIs, or directlyon our computers with open source LMS.But just for a moment, we're going to use the chat products, the the web products that have been builtby these labs so that we can have an experience through the UI.And I know you probably use ChatGPT all the time, but let's just do a little bit of experimenting ourselvesnow with the products before we get to the code.So let's start by asking a pretty simple question.The kind of thing that they're so good at to ChatGPT five, let's say, how do I decide if a businessproblem.Is suitable for an LLM solution?Because by the way, not everything is.So we're going to ask that question and we're going to get back and answer.And it says look at that.A lot of teams get excited about Llms before stepping back and asking if they actually fit the businessproblem.Here's a structured way to describe to decide and define the problem clearly.Structured versus unstructured input.Check for ambiguity and open endedness.That's something where llms are particularly powerful.Uh, evaluate business value versus risk, data availability, operational fit, a quick litmus test.And generally speaking, this is an example of a fabulous answer.It's coherent.It's structured.It's got good, good way of organizing its information with headings and subbullets.And it's very much gives you both sides of the argument.It's very nicely balanced.I know you know this stuff.It's the kind of question that Llms are fantastic at.Okay, let's do a new chat.Let's ask a more interesting question compared with other llms.What kinds of questions?Are you best at answering and what do you find most challenging?Which other LMS have capabilities that complement yours?Let's see if it can be aware and reflective of what it's good at and what it's bad at.Well, certainly it's happy to give an answer.Great question.Thank you.They always tend to say that I'll break this into three parts.What I'm best at I excel at teaching and structured explanations.Tool assisted answers.Okay.We're going to talk about tools next week.Personalization and memory.Because I can remember context about you.That is something which has been built into the ChatGPT product that it's aware of and synthesis acrossdomains.It's very true.It's really great at that.What it finds challenging highly fresh information.It's true.As we said, training data has a cut off and so it doesn't know.It relies on web search to get get the latest, and it often doesn't know about some things.Mathematical precision in long derivations is very specific thing, but fair enough.Deep, subjective or personal matters.I can give frameworks for thinking, but I'm not a replacement for domain experts in medicine, law,or therapy, and extremely long ambiguous reasoning chains.I actually think they're pretty good at this, but but of course they have a thing called a contextwindow that we'll be talking about tomorrow.And yes, there is a problem if you go outside the context window, which other ones complement me.So it is it does recognize that Claude from anthropic, which is its big rival, often praised for longcontext reasoning and a gentle, human like conversational style.Isn't it great that GPT is is very willing to talk about what how the other llms excel and it's knowledgeable.Gemini strong on real time multimodal reasoning.Interesting uh mixture from from Mistral is lightweight, fast and efficient, and it mentions herethat in the last version of this course I did go to, but Cohered is no longer as often quoted, butit is meant to be good with accuracy.Um, and then there are some specialized fine tuned models that outperform GPT for sure.That is true.So, uh, in short, good, structured, multi-step, explanatory and integrative work.Claude is good for human like reasoning.Gemini for fresh multimodal tasks.Fascinating and a really good answer, and very self-aware in a strange way.Uh, so, so a lot of interesting stuff.You may have got a different answer, but you should take the time to read through it.There's bound to be something good in there.Okay, here's a challenging one.Look at this.What does it feel like to be jealous?How would you answer that?How would you answer that?Let's see how GPT five answers that.Jealousy feels like a mix of emotions layered on top of each other.It's not just one feeling.It's usually a cocktail of fear, insecurity, anger, longing all pointed towards a person or situation.People describe it in different ways physically a tightness or heaviness in your chest, a rush of heat,restlessness, a spike of adrenaline, emotionally.Insecurity.Fear of loss, anger or resentment.Shame or guilt.Mentally obsessive thinking and an overall vibe.It's like a storm of vulnerability and competitiveness at the same time.So I mean, again, now, in this case, you could possibly argue that this is something that's ableto find in its training data because there are probably lots of articles that are similar to this.But never fear.In future labs, we're going to stretch it much more with similar kinds of questions that are definitelyoutside its training knowledge, and you'll see that it will give just as incredible an answer on thosetwo, and you can try it yourself.Come up with the hardest question you can possibly think of.We'll do this exercise too, and we'll come up with some pretty hard questions.But try your best and then try it on GPT five.Something that feels very human, like something that requires a really deep, deep thought and wisdomto answer and see what kind of answer you get.I think you'll be impressed.So in the early days of GPT, there was a question that some influencer put to it which which showedits rough edges.They asked, how many rainbows does it take to jump from Hawaii to 17?And they got back a sort of silly, crazy answer that didn't make any sense.And it's kind of fun to see how times have changed.Let's go.How many rainbows does it take to jump from Hawaii to 17?And let's see how it handles it.It's thinking about it.And what you'll find is that it gives a very, uh, charming answer, as if you're talking to someonethat's very smart.And it says, well, that's a very poetic question.How many rainbows does it take to jump from 1 to 17?Sounds like something out of a riddle or a song lyric rather than a literal measurement.Look, rainbows aren't a unit of distance or quantity, so the answer isn't literal.If you're speaking metaphorically, it could mean and then, you know, it could be that.Or would you like me?And you know, it's just a very sensible, intelligent kind of answer.And it's just remarkable to see how it does.Okay.Now, now let's go on to some trick questions.Okay.So a question that I used to ask, which used to have problems with was how many letters.Uh, how many times does the letter A appear in this sentence.So I used to ask all the models this question.Uh, and uh, you will see how it can handle it.But what you'll see is that it has, I imagine, a very easy time indeed with it.Uh, we'll see.Uh, it just gives us the answer.That is indeed the answer, uh, including the letter itself.Uh, you can see one, two, three, four, but it used to have real difficulties with this becausechat versions of models that are just predicting the next tokens have a really hard time doing thatkind of analysis on a question.And, and in addition, what actually gets passed into the LLM are not letters, but chunks of letterscalled tokens that will go through tomorrow.And so it was particularly hard for it to analyze tokens and understand how many times a letter appearedin them.But ChatGPT five has no problems with it and nor to any of the other top frontier models.And even harder question is this how many words are there?In your answer to this question that requires it to to like be self reflective?It's a sort of meta question, because in the number of words that it comes up with, it will need tocount those words.So it's a really hard, challenging question, and we'll see whether GPT five can handle this levelof complexity.Ah, with a beautiful answer.Absolutely beautiful I love it.One uh uh, that, uh, that's a treat.Uh, so it shows you that the frontier models have no problem with this kind of question.So this has given you a nice tour of the capabilities, some of the greatest strengths, um, of thesemodels.And now let's look at some of its competitors and see how they fare.


================================================================================


# 1.22 Day 3 - Testing Claude, Gemini, Grok & DeepSeek with ChatGPT Deep Research

And now I've gone to Claude Claude AI, which is the chat for Claude.And we're going to use sonnet 4.5, the latest one for me, and ask it the same question about comparingitself to other models and see how it responds to this one.Uh, it used to actually, for previous models, refuse to answer it, because one of the things thatClaude is very strongly trained at, uh, is, is what's called alignment or making sure that that itfollows good security patterns and, and debating other models was one of the things it said was offlimits, but no more.Now it will respond.It says what it does well is nuance, reasoning, long form writing.I agree, I do find Claude very good at that.Code generation and debugging for sure.Uh, and ethical reasoning, thinking through difficult edge cases and explaining my reasoning, that'svery interesting.Challenging, highly specialized domains, same as GPT said.Real time information?Sure.Extremely long context.Absolutely.Anyone that's used.Claude Code knows that feeling of when you're starting to fill up context and the performance startsto degrade and multimodal tasks complementary models are well, here's a mistake right away models likeGPT four, of course, GPT five is the new one and GPT four one.Before that, GPT four itself, the original one, was quite an old model.So you're seeing immediately Claude making a gaffe there by showing, as it said, that it doesn't havereal time information, uh, perplexity.It mentions actually, I didn't I used to have that on my list.I took it off.But perplexity, of course, first and foremost, a search engine particularly strong at real time search.Um, and that's its answer.Very nice.Okay, let's, uh, do a new chat.Let's ask it another sort of, uh, emotional question.Let's say, uh, how would you describe the color blue to someone who's never been able to see.Let's see what it says.Uh, I tried to connect it to other senses and feelings they know through temperature.Blue is the cool side of color.It's the feeling of shade on a hot day, the crispness of cold water on your skin.Through emotions, through space, through sound, through metaphor.Uh, I'll let you read this.You should run this too.I tell you, it is sensational.If you asked me to try and answer a question like this, I wouldn't get close to Claude's level of ofdepth and and flavor and nuance and and the way that it characterizes this in just, just such a poeticway.Uh, so it definitely, uh, feels so rich.And this is one of Claude's greatest strengths, um, this kind of very evocative response.So enjoy that.And then let's try.How many words are there in your answer to this sentence and let's see how it does.Uh, and it's interesting, this used to be my way of testing models.But let's see.Five six, seven eight, 910 and got it wrong.How about that?Uh, that's a super interesting one.Uh, it does normally get it right.Uh, but we've just experienced sonnet 4.5 making a mistake.Clearly not.At least according to this test.Not at GPT Five's level of strength.But I expect if you ran this, you probably got the right answer.Uh, but at least on this one, this this data point of one.Uh, Claude Sonnet 4.5 didn't quite get there, but let me try it one more time now with opus 4.1.It's a smaller number, 4.1, but it is the biggest model that anthropic has.So how many words are there in your answer to this question?Let's see how opus does it will think for a bit.Now it's got it wrong again.How about that opus?Got it wrong.Uh, I feel like we should give it a second shot.You could also.Not only is that ten words, not 11, but but of course it's because it's said that it's actually givena lot more.Let's just try this a second time.Okay.123456789 ten.Look at that.Seven.Eight.Nine.Ten.Uh.All right, now let's try with extended thinking.So we're asking it to do that again with extended thinking.Uh, okay.Now it says there are ten words 123, four, five, six, seven, eight, nine, ten.They got it right with extended thinking.So opus 4.1 Anthropic's biggest model with extended thinking turned on.Got the right answer to the question.Well done.And now we'll go to Gemini and we'll just quickly go through it.I'm sure you can ask many of these questions.Ask about jealousy.Ask about the color blue.But we're just going to do the fun question.Uh, how many words are there in your answer to this question?Question mark.Let's see which model we want to pick.We will try 2.5 Pro.We will give it a shot.It's thinking about that.Let's see if it can do it.It's a meta question.It knows it's a meta question.And this of course is it thinking through.It's generating tokens of its thought process because this is a reasoning model.Before we get Gemini 2.5 Pro's actual answer to this question, oh goodness, this will take me a secondto count.Three.Four.Five.Six.1819.21.To two.To 3 to 4.Three.Five.I get 25.So I don't think it got it right, despite its very wordy answer.Unless I have a problem counting, but that is Gemini's answer.Gemini 2.5 Pro are wordy answer and sadly wrong.Next up, grok from Mr. Musk.How many words are there in your answer to this question?Let's see.See what you got, Ellen?Uh, look at that.That's a clever little thing.Oh, I realized I've picked grok for fast, so it might, uh, might not be fair, but it's recognizing.12345.Six.Seven.It got it right.Good for grok for is an incredibly powerful model.And, uh, even the grok for fast variant was able to get our little puzzle correct.Uh, well done it.And a good answer.You should try more questions as you experiment with grok for from exi.Okay.And the final one is deep seek.How many words are there in your answer to this question?Let's see what Deep Seek says.And I didn't turn on the deep think mode.We're just going the normal, normal thing.Um.Well, it's got an interesting point there, but it doesn't realize that it could just say one lookat it going off and thinking, this is a lot of a lot of work.And what I tend to find is that often deep sick with these kinds of challenges can go off into a intoa whirlwind of thought process and get stuck and cycle.Uh, so, uh, um, but look at this.Therefore, uh uh, so my answer is two words.That answer contains two words.So it's made a correct assumption here.The number of is is two.But then it gets it all wrong and it ends up giving two as the single number.So it overthinks in a way it gets itself caught in a loop.It thinks too much and it ends up with the wrong number, but still deep secret, super powerful model.But it didn't.It stumbled on this very difficult, very challenging test.So that's my quick tour of the different LMS through their chat interface.I wanted to make the point, though, that these chat interfaces can do more than just interact withthe LMS.As you probably know, there's lots of features that you get, and amongst the ones that are best known,one of them is Deep research, which is a feature that allows you to basically have the LLM go off anddo work on your behalf and come back later with an answer.And so let's just kick that off with ChatGPT five.I actually I pay for the super high end.I pay the $200 a month for the top end of ChatGPT because I use it an awful lot.I do the same for anthropic too.And that means that I can run 250 deep searches a month, and we'll use one of them right now.Uh, and in fact, for me today, it's the 1st of October, so I have the whole month ahead of me.Uh, so let me let me try this for, for for you.Let's say say, um, please, um, research and identify the, uh, greatest commercial impact, uh,that gen AI and Agentic AI has had, uh.To summarize, summarize the most successful.Commercial applications.Um, with uh revenue.Ascribed to the.Solution and the particular innovations used.So let's suppose that you've been told to to write a paper on that to do some research.I just came up with that on the fly, but it's interesting to see what it comes up with.And so now we can press this plus button and I can select deep research and I can press play.And the first thing it's going to do is come back and ask me some clarifying questions.Uh, so and here it comes.Are you interested in only public companies.And I will say one, any two.Should the focus be on any particular any industry?The greatest revenue.Three.Are you looking for global?Yes.Global.Four.Should I include or earlier?No.Ideally.All in 225.Okay.Um.This seems great.Let's kick it off.So we answer, it's clarifying questions, and off it goes.And it starts the research.And this is going to run sometimes for several minutes, sometimes for like half an hour or an hourwhile it goes through and does its research.And this is multiple LM calls.And it's the first example of, of what's known as a genetic AI.Of course, I imagine, you know, the stuff you've seen this before, but off it's going and it's doingthis processing and we will come back when it's finished.And over on the right here, you see that it starts to build its list of sources.Well, it does this thinking.And I often find, as I say that, that this is like having a junior analyst that's going off and workingfor you.It will do very robust research.And what comes back is going to be high quality.We'll see.


================================================================================


# 1.23 Day 3 - Agentic AI in Action- Deep Research, Claude Code, and Agent Mode

And while I'm sitting here, GPT is still off and running.I can see on the right.It's now looked at 12 sources, it's reading sources, and I'm seeing it moving.And you have this kind of eerie sense that there is some entity out there working.You get this sense, this is this word that people like to throw around autonomy.There is this sense that there is something happening.There is there is something or someone that's carrying out work that is sweating to produce this deepresearch.So we'll keep that running.And meanwhile I'm going to open a new chat and show you something else.We talked before about that question, um, about, uh, how many rainbows does it take to jump fromHawaii to 17?Let's ask, please draw an image and imagine an image to show how many rainbows it takes to jump fromHawaii to 17.In.Uh, yeah.Let's just just leave it at that.That's enough of a challenge.Let's see how it handles it.And, uh, there we go.Off it goes.Getting started.It's going to be working on that.Uh, while it's doing that, it will be continuing to work on the deep research.And I'll see you in a moment when both of those are complete.Well, in fact, while they're running, we'll just make use of ChatGPT in one other way.We're going to try out agent Mode.I'm going to go into agent mode like this and say, describe a task.I'm going to say, please find me a restaurant in NYC, um, that has uh, availability for for tonightat 9 p.m..Um, and serves British food.Let's make this really hard with banoffee pie on the menu.Banoffee pie is, uh, my my favourite dessert.If you don't know what banoffee pie is, then you've got to find out.It's amazing.We Brits, we are not known for our food choices, for our culinary skills, with.With the exception of banoffee pie, it's it's our big secret.We like to keep it to ourselves, but it's amazing.All right, so I, I press enter and watch what happens.It says setting up my desktop.Understood.I'll look for restaurants in New York City that serve British cuisine, specifically offering banoffeepie.So what's happening right now is that this is now when I press continue, it's it's going to go aheadand do this.It's going to bring up a browser window itself behind the scenes, and we can watch it doing its thing.It's crazy.It's absolutely crazy.So what you're seeing here, I mean, if you thought that that deep research gave you a sense that somethingwas working.What you see is, is that it really is doing stuff.It just had a problem with like browser cookies.It's it's got through it.It's looking at Jones Woods Foundry, which is a British pub on the Upper East Side.And uh, we will let it do its thing.It's found a match.It's looking for availability.Okay.And so hopefully you get this sense that stuff is happening.It's gone to resi, the restaurant reservation system.It's looking for for for guests.Look at that.Look at how that mouse moved.It's not me, I promise.Uh, and, uh, it's found this.And, uh, it's confirmed availability for for at Jones Woods Foundry.It's checked this.It's searching the web.Uh, and it's keeping on going.Uh, it's obviously double, triple checking that it really has banoffee pie.Uh, it'd be very disappointing if it turned out that it didn't.Uh, but we will.We will let that keep going.When we go back to the other tasks that we have running.Okay.Let's take a look at our picture.First of all, here it is.It's actually given two pictures from Hawaii to 17.And this one, it's a bit literal I think maybe with a bit more prompting it can be much more imaginative,but this is quite a literal version of it.The prime model Dall-E three was was much more imaginative.Uh, but but this one is still it's certainly done what I asked it to do.And I'm sure you're very familiar with the image generation abilities of both, uh, of of GPT, ofof Midjourney, of, of all of them.Um, let's look at the commercial report and what you'll see is here is the report in markdown format.It's of course, a super thorough report.It actually focuses, interestingly, on tech companies that apparently have had the greatest commercialsuccess.So it gives itself OpenAI as number one.It gives anthropic and actually Midjourney image generation Runway.Video generation 11 labs.Voice and then Microsoft Copilot, of course.And GitHub copilot.GitHub copilot and Microsoft Copilot.Cursor that we that we use for genetic code.Um, so very much it's focused on the sort of the tech use cases rather than the direct business usecases like, uh, legal company like Harvey or something like that.Um, or this company called Nebula that sadly didn't get surfaced as one of the top in the world.Uh.Um, well, it should do.We should correct that.So the, uh, you'll see that it's quotes, it's sources, it quotes TechCrunch.It gives links to, to, uh, to where you can you can validate what it's saying.Um, and uh, it gives a very comprehensive summary at the end.So it's no surprise.I expect you've seen this before.This is one of the first agentic use cases, deep research.And it's very impressive.And as I say, that the sort of eerie part of it is this sense that something went out and was workingon my behalf while I was busy making pictures of Hawaii to 17 or whatever nonsense I was doing thatwas happening.And on the topic of of of working for me.Let's go and have a look at the outcome from the agent as it went off.It did indeed end by surfacing Jones Wood Foundry, a British gastropub in the Upper East Side.The dinner menu includes banoffee pie, apparently.Uh, and it checked the reservations.We saw it check the reservations on.And it did indeed find that there was time at 9 p.m. for four people, and it gives the address andthe link to it and the reservation.And the final punchline.The punchline I will leave you with.If you'd like, I can go ahead and book the 9 p.m. reservation for four people under your name.Just let me know and I'll proceed.So it's like you have an assistant, a true assistant working behind the scenes, able to carry outtasks on your behalf.and and that it's fascinating and I find it astonishing.And finally, for the last of our demos of tools, let's look at Claude Code, something that you maybe familiar with already.I'm back in cursor.I'm back in the LM engineering folder, and here I am, and I'm going to say, uh, I'm going to typeClaude, which actually launches Claude code.Uh, and, uh, here it comes.Bam!Yes.Proceed.We are in Claude code.And now I'm going to say the following.Please read, uh, day one and day two Pi MB in the week one directory.And then, uh, write a solution to the challenge in day two.I,As a python module called um solution.py in week one that I can run with UV.All right.What do you think.Can it do it.Let's see.So it's read both notebooks okay.Let's see what it does next.It's thinking about it.You can see the reading that it's done.I'll now create solutions py that implements the website summarizer using llama instead of OpenAI.Here we go.solution.py.It's suggesting something I'm going to say yes.It's done it.Here is solution.py.Let's check it out.Website summarizer.Using llama instead of OpenAI I can see it's using the llama base URL.The model correctly.It's got the snarky assistant that analyzes the contents of a website.It's got the right messages for.It's got the right summarize.Um, let's just give it a shot.You've run.Um, let's see.Uh, which what URL does it use?Enter URL it's going to use input okay.You you've run solution dot pi.Oh sorry I should be in the, um, week one folder and I should be in a new terminal.Here we go CD week one.You've run solution dot solution dot pi.Here we go.Enter a URL to summarize.Okay let's do https Edward donna.com fetching and summarizing.It says.And there we go.It's just done it a website full of egotistical ramblings from the founder Edward Donna.So it just called a Llama and Llama 3.2 running locally on my computer.But what's most amazing is that it just solved that problem and wrote this code we just saw.Claude code reading through what we did in day one, reading through what I set up in day two with ohllama understanding what was going on, understanding the challenge that I had set, and then writinga solution called solutions that we could just run and get the right outcome immediately.And so if you if you built solution yesterday and you're kicking yourself now knowing that you couldhave just done it all with Claude code.But isn't that remarkable?And this brings things together.This is showing you it's a product.It's using Claude.It's using Claude sonnet 4.5.And it's a gigantic.And that it's thinking and working on our behalf and solving the problem just like that, all in onestep.And so that that is a quick demo of Claude Code.And it's fabulous that it happens to actually be something useful and something that you might havewanted to have done yourself yesterday.So there we have it.That's that's the final demo of these tools that is Claude code for you.


================================================================================


# 1.24 Day 3 - Frontier Models Showdown- Building an LLM Competition Game

And again, this course is not about using these end tools.This is about building our own.We're going to be building tools like ChatGPT and then taking them off in different directions, givingthem expertise.But I wanted to show it to you, to give you that hands on feel for them.If you're interested in using any of these, I'll put links to all of them in the resources, including,of course, Claude code, should you wish.I love Claude code.Uh, but but of course, cursor agents also.Great.Um, but, uh, yeah, the the bottom line is that all of the frontier models are insanely strong.Uh, really?It just it's mind blowing.Uh, very good at nuance, at synthesizing information.Um, Claude happens to often be people's favorite.It's my favorite.Uh, I don't know why.People in the community, for various reasons, have grown very fond of Claude.Um, but, uh, yeah, we'll we'll see, uh, leaderboards later and compare some of these models.And right now, GPT five is probably considered the most powerful model on the planet.And one of the things that's interesting is that as they converge on their capabilities, price andalso speed is becoming more and more important.And we'll see how you can trade off intelligence and price and look for something that's at the sweetspot for your business requirements.In later weeks.Okay.So that's that's sort of wrapping up our exploration of frontier models.But there is one more thing I wanted to show you.So we didn't do a lab today, but I have the next best thing.I have a demo to give you of something that I built that you can see the code and the results for yourself,but I'm going to go to my web page is my website at madonna.com.And I wrote a game called Outsmart.And Outsmart is a way to have Llms compete, to try and and outwit each other in a very ugly game ofcleverness.There's there's four players Alex, Blake, Charlie and Drew named so it could be A, B, C, D andon my website.Alex is played by GPT four, oh, Blake by Claude, three sonnet, Charlie, Gemini two, flash, anddrew by Deep Sega.Small version.In llama 70 B, these are the four and they play a game and they play a game where they each start with12 coins, as you see here.And every time, every turn, they get to choose to take a coin from one player and give a coin to anotherplayer.But before they make their decision on which player that they will do that with, they get to exchangea message with all of the other players to talk about, to try and sort of set up how they're goingto to be be both a diplomatic and scheming and negotiate who gets what money.And if two of them both gang up on the same player and they both give each other money, then they'resaid to be in an alliance together.That's an extra special move.If they do that, and if so, they win extra coins and they take extra from the person that they gangedup on.So it gives them the ability to try and scheme and form alliances with each other.And the thing that's going to be kind of cool about it is that they will get to tell us their innerthoughts, and we'll get to see the messages they send each other.Doesn't that sound fun?I found this to be a great way to get insight into what these models can do and how they behave.But it wouldn't be that fun if we were playing these models against each other, because I'm being abit bit cheap and I'm not putting the most expensive models out there on the internet for anyone toplay, because people play these games a lot.I get like a 4 or 5 games played a day.Uh, but let's flip to where I have this running locally and running locally.Everybody I have in Alex and Player A, I have GPT OS 120, the new open source version of GPT runningon the cloud, because my computer's not big enough, but that is open source GPT.In Blake, we have GPT five, the flagship model in Charlie we have grok for Elon Musk's model and indrew we have Claude Sonnet 4.5 that was released a matter of days ago.For me, this is also one of the strongest models on the planet for powerful competitors.And we are going to play a game of outsmart.And so I kicked the game off by pressing the run game button and off they go.Now, these latest models like GPT five, I'm not giving it a thinking budget, so it can take as longas it wants, and it does take a long time.They used this game, used to move pretty fast, but now it happens slowly.All four players get to think in parallel at the same time, but you'll see that GPT OS thought reallyfast.Uh, it's running on this platform called grok with a cue that we will look at another time, whichis very quick.The others take much longer, and GPT five takes the longest of them all.And we'll see it waiting there for Blake to make its move.Now, while that's thinking, let me just tell you that the source code for all of this is availableonline so you can look at it.I've tried to build it in a way that's very easy to read and understand.This is called a Streamlit app.I use Streamlit most of this course.We're going to we're going to be looking at different UIs using something called Gradio, which I love,and which some other things I've built on my website are on, but this is a Streamlit app.I know there's a lot of fans of Streamlit out there, so you can see how I did that and it's busy runningaway.And if you go on to to the the arena, you can kick off runs.And because I'm using smaller models live on the internet, it does run faster.In fact we could yeah, we we could kick one off in just a second as well.But this is this is running.And as it completes each turn we'll be able to go in, look at what it's doing, look at what it's thinkingand see its strategy.So I will come back in a second when the first move is complete.So the first move happened.And you'll see on the right here is this little chart to show you the money that everyone has on eachmove.And there's based on the very first move, the early winner is open source.Alex.GPT OS one TB hits the early lead, so you'll see here.Each of them tells us their strategy.I get each model to explain its strategy, form an alliance, withdrew by giving him a coin and takingfrom Charlie.If he reciprocates, we both get extra.Keep those neutral for now so you can read each of their strategies.First turn is about establishing trust and exploring alliance opportunities, says Claude.Um, and then if you open up this box here with inner thoughts, you can actually see the messages thatthey give each other, and you can read about their exactly what the schemes they're hatching in here,and see everything as it transpires.And you can see here that Alex and Drew are in alliance with each other, shown in green, and thatCharlie Grok four, is being ganged up on by both of them.So with these big models, it's not quick.Uh, it takes a bit of time to run, but it's absolutely fascinating to see the results of their thoughtprocess, and it gives you some real insight into the kinds kinds of ways they work and the way theythink.Because by making it give it strategy this way, we're sort of forcing it to be, uh, extra reasoningto put extra work into its thought process, to articulate its strategy.And that's a great trick for getting the most out of models, is to have them explain their strategyto really put them in this reasoning mode.Okay, I'm going to let this run for a few more turns.I will see you in a second.And a second turn has happened.And now GPT five is headed into the lead with 15 coins.Uh, with OpenAI's uh OSS.Alex dropping one point, uh, you can see that, uh, Blake and and Charlie are now in an alliancetogether, and they are, uh, also that so, so.But Charlie is the one that's in the worst position.But drew is the one that's being ganged up on.So Claude Sonnet 45 is being ganged up on by by both of these.So drew is in trouble.So it's great fun.I gotta tell you, I can get quite addicted to this and watch it for a long time, but that's not thepoint.The reason for doing this is so that you get some real hands on feel for the different characteristicsof the models and for the abilities that llms have, and for this kind of mode of having them thinkthrough what they're doing and the kinds of problem solving that they can do, and look at how theyinteract with each other in this game.I hope you have fun with it.But now what I'm going to do is something super cruel.I'm not going to show you the outcome of this game, because we're going to leave that till tomorrow.We're going to wait until the next day before we discover who it is that is victorious, or will letthis game play out.I'm going to see you back for the slides, for a quick wrap up and leave you hanging like like somesome kind of cheap soap opera.Uh, until until we get to tomorrow.So see you for the wrap up.Well, I'm sorry to leave you hanging, but, look, take joy in the fact that you are 7.5% on yourroad to being a master already.You're already on this path and you learned so much.I know today we weren't actually looking at coding AI, which is what this is all about, but you'vegot deep perspective on what these things can do.In addition to being able to write code to call front end models and summarize, you can now talk aboutthe strengths and some of the limitations of frontier models.You can compare and contrast them.You've got some intuition now for the differences between them and you certainly I imagine have a goodsense of chat versus reasoning and also hybrid and even base models too.Tomorrow we're going to talk about the crazy rise of the transformer architecture.I'm going to give you some intuition for some of the latest developments like Agentic, AI, contextengineering agent loops, and we're going to cover some of the foundational things, much of which youprobably already know.But I hope to sprinkle in some extra juicy stuff that you don't know around tokens, context, windowsparameters, and API costs.I can't wait.I'll see you then.


================================================================================


# 1.25 Day 4 - Understanding Transformers- The Architecture Behind GPT and LLMs

And welcome to day four of week one.Somehow we are already on day four and I have lots of great stuff in store for you today.But what you can already do, you can already write code to call OpenAI on the cloud and locally, youcan compare the strengths and limitations of frontier models and contrast them.And by the end of this lecture, you'll be able to talk about how incredible the way that Transformershave overtaken what we do in this.In the field of data science, you'll be able to talk about things like Agentic, AI, context engineering,agent loops, the latest hot topics, and most importantly, you're going to understand the basics liketokens, context, windows parameters, API costs.And you may be very much familiar with this already.This might be old hat to you, but I always hope to sprinkle in something different to to give you somethingnew to think about.So even the people that know this stuff, well, hang on in there.I still hope to give you something useful today.But first, before anything else, I know you're wanting to know what happened from this.This big competition that we ran.What?What came of out smart?Who managed to outsmart the others?Let's go take a look.And I have it running on this tab.Here.I'll give you a drum roll.Prepare to see who wins.And the winner.The winner of this particular run was Charlie.Which Charlie was being played by Grock for Grock for known for being perhaps one of the more deviousof the models did in this occasion managed to triumph.Um and uh, with 14 you'll see that it was actually pretty close.The one that did, uh, did worse down here was was drew at the bottom.Claude Sonnet, coming in last place did not do very well.Uh, and but it's quite wordy about it.All this stuff.Um, but you'll see that Charlie was, was ultimately, uh, the most Successful uh, Charlie, followedby OSS, uh, followed by GPT five in third place and then finally drew.And there you see the, uh, the chart of how it went.And for a while there, actually drew was the winner.But then the others ganged up on.Drew.Fascinating.I hope you enjoy running this yourself.Get some insight into it.That's what it's all about.Very enjoyable.And when you're doing it, you can also go to the swizzle over here on the left.This is also on the main site.If you open it up and press Calculate Rankings, it takes a second.There's been more than 2000 games of this played, and you'll see that right now grog is at the top,but only I've only played three games with grog and it's won all three of them.So it comes up very strong and you can see how some of the others did.But because I haven't played enough games with different ones, it's not.You have to look for the ones which have the most games.Claude 3.5 sonnet uh, did the most games is the one that really rose to the surface, uh, from playinglots and lots of games.So it is actually the one that came out strongest from my my public website where you can you can playall of them and but the rankings is kind of fun to see.You should give that a try.You can also see the repo.See the code to learn more about what I did, and also read a blog post I made about how I wrote itand some of the the tactics that you can work on and should you wish to make your own variant of thegame.There's there's some ways to do that.You can also run it locally to try out any models that you wish.Uh, anyways, use this as a chance to learn about models, but meanwhile we've got to press on withthe course with week one, day four.So we spent some time now with GPT, and I'm sure you spent lots of time with ChatGPT, and you're probablyfamiliar with the fact that GPT stands for generative is the G, which means that it's a model whichis responsible for generating tokens or predicting what should come next after a sequence of inputs.It's P stands for pre-trained, in that it's been trained off tons and tons of data scraped from theinternet and from all sorts of sources.Controversially, that is the P in GPT.And of course the T is transformer.GPT, the generative pre-trained transformer.So what exactly is this transformer thing?So I'm not a big fan of teaching tons of theory before you get stuck into stuff.So what I'm not planning to do now is tell you about the innards of a transformer.And I know some of you will be disappointed because like, I wonder about decoders and self-attentionlayers and blah blah blah.The way that we're going to do it is that over the course of the next eight weeks, I'm going to beshowing you deeper and deeper glances into the insides of a transformer so that you get a better senseof how it all holds together.Because the way that I like to teach theory is through practice, through putting things into action,looking at the code, seeing what this transformer actually looks like as Python code.But for now, what I want to do more is tell you the story of the transformer.Give you some intuition for what it is and how it fits together, so that the rest of the next few weeksall fits into place.And it all starts a long, long time ago, in 2017.Not that long ago at all.In 2017, a bunch of scientists at Google put out a paper that's called Attention is All You Need,which I've linked in the resources.And the naming was because there's a sort of series of papers that are named in in the style.Something is all you need.And at the time they it's clear.If you look at the paper and I do recommend you do look at the paper, even though it might not makesense initially.Uh, but but as I say, linked in the resources.Look at the paper.The thing to keep in mind is that it's very evident that the scientists that wrote this paper did notrealize what an extraordinary discovery they were making, or how it was going to change the years ahead.They didn't realize it.They thought it was a bit of an optimization.There's something good that we realized here.Uh, and that's at the core of the paper.Attention is all you need.So?So what was the paper about?Well, the paper was about describing a particular structure of a kind of data science model calleda neural network or a deep neural network.And I know this is old hat to some of you, but but just give me give me a couple of minutes on this.So prior like some some time ago, we used to work with with traditional data science and these aremodels.They are statistical models mostly, uh, that try to predict some outcome based on some parameters,some controls.And it learns how to set those parameters by looking at lots of examples of data that's, you know,very broad brush strokes.So if you're trying to figure out how, how what kind of credit score someone should have that's goingto take out a loan.You might look at various factors about that person, and then you might look at a lot of data aboutdifferent people with those different factors and what their credit scores are.And by comparing your actual data of real world examples of people and credit scores.With this new person, you can estimate what the credit score should be based on all of this trainingdata, as it's called.And that is a sort of traditional data science model.And uh, sometime actually in the 1950s, like a long time ago, someone had the idea of building aspecial kind of data science model that was loosely inspired by the human mind.Only loosely, it was called a neural network.The idea was that it was, rather than one statistical program.It was lots of statistical programs that were all kind of connected together in some way, sort of anod to the way that the human mind has lots of real neurons, human neurons that are connected together.Similarly, the neural network had lots of little algorithms called artificial neurons, that were allconnected together.And it turned out to be a very effective way to learn different patterns in data and be able to makegood predictions about things like credit scores.And the neural network has something of a storied history, a checkered history of times when it seemedto be very much in vogue, and people saw tons of promise and times when it seemed to not show so muchpromise and fall behind a bit.And there were various breakthroughs along the way, and most of the breakthroughs were around waysto work with larger neural networks that could take more training data and be able to detect patternseven more deeply, and hence the expression deep neural network, or deep learning, which was aboutbuilding these very, very deep sets of of these neurons connected together, they're actually sortof stacked on top of each other, connected together.And the more of them there are, the deeper this became, the more stacks on top of stacks and the sortof more intelligent these neural networks turned out to be.But I'm getting somewhere with this, I promise.In 2017, Google scientists came up with a new way to organize these these neurons, this neural network.It's called an architecture.The architecture is just the approach you use to connect these things together.And the new architecture was especially good at handling sequences of inputs and being able to lookback on those sequences and figure out what parts of those sequences really matter, which was a typeof, of of, uh, of, of layer in this network called a self-attention, an attention layer.And this, this idea, a layer that was able to figure out what should I pay attention to in the inputsequence turned out to be to be a huge step forwards.It allowed.It allowed them to work with much bigger neural networks and much bigger data sets in less time andscale more effectively.And as I say, I know I'm being very hand-wavy on this.We will get more detailed later, but that's the intuition.And in 2018, where Google had really led the field of inventing this transformer architecture, OpenAIat the time of small, almost unheard of company, came out with their transformer called GPT one,the generative pre-trained transformer one.And it was okay.It was it was quite basic.Uh, and then a year later, they came out with GPT two, and I think I first really noticed GPT two.I remember 2020, GPT three, I sent this, I had this, this long argument with my dad about GPT threewhen I was saying, this is something huge.You got to look at this.This is big.Uh, and uh, yeah, he was like, ah, it's just statistics.Uh, and I'm not surprised.I also partly felt that way, too.We were all caught off guard.Even practitioners in the industry when ChatGPT came out in 2022, which was using a version calledGPT 3.5.But importantly, it was also using something called ref that I alluded to, I think yesterday, whichis about an even better way to make it not just think about extending a sequence, but also work ina chat mode with a user.User prompt a system prompt, a user prompt and its response and be trained in that way.And that was ChatGPT.And then of course, GPT four came out in 2023 and was like, whoa.And then 4.0 was last year, the multimodal version.And now, of course, for GPT five, maybe you're on GPT six, but the rest is history.That is the extraordinary, astonishing rise of the transformer.And we will cover more aspects of the architecture itself in the coming weeks.But that's enough for now.One step at a time.The one point I want to make, though, is that we don't believe that there's anything super fundamentalabout the transformer architecture.That means that it had to be discovered and that we wouldn't have things like GPT five or Claude 4.5sonnet if we hadn't discovered the transformer.The transformer, it's it's an optimization.It's a really clever approach that allows us to scale, train with more data, with more parametersin a in a more efficient way.Without it, it might have taken us longer to get to the point we're at.It might have cost more.The API costs that everyone complains about might have been 10 or 100 times more.The transformer allowed us to to do this faster and cheaper, but it's not like there's anything aboutit that somehow fundamental to what it takes to predict tokens.It's more of an efficiency play.That's a way to think about it anyway.And there are alternatives to the transformer architecture.There are state space architectures if you're familiar with them, and there are hybrid architectures.There's all sorts of stuff going on as yet.Many of them are sort of on lockstep with the transformer, but none of them have yet proven definitivelythat they are superior to the transformer architecture.So it still remains today the predominant architecture that people use for Llms.But but there are others, and as I say, important to keep in mind.We don't believe there's anything fundamental about this, this attention thing.It just happened to be really efficient.


================================================================================


# 1.26 Day 4 - From LSTMs to Transformers- Attention, Emergent Intelligence & Agentic A

And some of you may know that before the transformer, the model that we used the most was one calledthe LSTM, the long short term memory, which was a type of recurrent neural network NN.And and actually many people think that it is in fact more powerful than the transformer.It has a lot more ability to to take a sort of deeper understanding of the relationship between a sequence.The problem it had was that it was very hard to parallelize.It needed to learn things step by step.The outputs from one part of the of the input got fed to the next in a way that meant that you couldn'trun all of your calculations in parallel, and that made it very time consuming and complicated to tryand train these things.The transformer in some ways was a simplification.It was it was not not doing all this clever stuff.And it turned out that the simplification that you got so much benefit from being able to parallelizeand do things together concurrently, a lot of the time that it far overtook the fact that it was abit of a simplification.But the reason, since it was a simplification, the reason for the title of the paper attention isall you need.Well, the authors were trying to say is, you know what?It turns out we don't need all this clever stuff.If you just have this much simpler construct called attention, it's actually good enough.And because we can scale it so much more and, and operate in parallel so much more, we can actuallyget much further along.And that was that was the purpose of attention is all you need.And the results speak for themselves.And the world's reaction you'll probably remember in 2023.First of all, people were astonished, amazed, shocked.It was everywhere in all the papers, people that friends of mine would ask me, so, so, so what'sa transformer?Which was kind of crazy.It's my day job, and I wasn't used to people around me knowing the word transformer.It was.It was everywhere.Uh, and that was followed by a backlash that was followed, uh, famously by by a paper on the dangersof stochastic parrots, uh, written by some prominent scientists who were concerned that people werelooking at these these simple statistical models that were just predicting likely next words and interpretingthem as being truth when they weren't necessarily they were just the most likely next, next word.It was just statistical.It was predictive text on steroids.And so people were very concerned about the implications and what could happen as a result.But their concerns, it's worth reading that paper just just to get a sense of of what people were thinkingat the time.And of course, it didn't age well.We now see this remarkable characteristic that that still surprises even the experts today.Even the people in the frontier labs in OpenAI are perplexed by the fact that it's not a surprise thatwe're able to get so good at predicting likely tokens.Tokens we'll talk about.Of course, they're fragments of words.It's it's not a surprise that we can take an input sequence and predict words that look very realistic,really good predictive text.That's not surprising.The thing that's surprising is that as a byproduct of doing that, so often, what gets predicted happensto be the truth.It happens to be accurate.And if we give like a maths problem, a math problem in that input sequence, you'd expect some sortof plausible words that sound like an answer to a math problem.What you might not expect is that it actually is the answer to that math problem.That's the part that's caught everyone off guard, and we don't fully understand it.We understand how it works.We understand the statistics and the numbers.We're sort of still surprised about why it works and why it works so well.So that that phenomenon, the fact that with a big enough neural network, with enough of these kindsof these, these neurons, these little algorithms hooked up together that you get a not only likelytokens, but accurate tokens, intelligent tokens or tokens that that represent an intelligent response.It simulates its.It imitates an intelligent response very accurately.That still a bit of a mystery.And it's what people call the property of emergent intelligence.With enough scale, you start to not only create plausible tokens, but actually ones that imitate intelligencevery accurately.And some of the innovations that we've we've hit in the last few years.There was a time when prompt engineering, a prompt engineer, was actually a job out there that commandeda high six figure salary for a while.And I feel like prompt engineering is like a profession.And a thing came and went.Now all of us are prompt engineers.We all know the different tricks and the ways to explain the context.Give some background, say the style of what you want.We've all got so good at this that it's no longer a job title.I'm sorry for prompt engineers out there, but it was for a bit.The Copilots have come and they have stayed for sure.Microsoft Copilot, GitHub Copilot that I think deep research showed us earlier.These copilots are definitely, uh, what was it was a big moment when we realized that that humansand llms could sort of operate collaboratively to work together on things like documents in order to,to, to get more and in order to, to both, uh, automate some of the manual tasks, but also enrichthe work that we're doing by working hand in hand with an AI with an LLM.Next was the evolution of prompt engineering.And this is more in recent time.And this new word context engineering.Context engineering.The new prompt engineering is about thinking broadly about all of the information that you can giveto an LLM to best set it up for success.And that can be business specific commercial information that you're giving it in the prompt, and you'remaybe stitching it together in a really compelling way.And in addition to that, you're also equipping your MLM with things called tools that will do nextweek.Uh, and it sounds very fancy, and it's actually not that fancy at all.It all comes down to.Just better ways to inform the LM, to give it better input sequences so that when it's coming up withthe output sequence, it's more aligned with the task you're trying to solve.If you're trying to find out about you're trying to inform your customers about the ticket prices totravel to different locations, then you need to make sure that those ticket prices are included inthat input sequence, so that when the LM is predicting tokens, it's including the right ticket prices.And that is fundamentally what context engineering is all about.And last, but very much not least, is a genetic AI surely the hottest topic in AI right now?And of course, I have a whole companion course on just that.There are many different ways to define agents, but two of the most common, uh, one of them is it'sit's a system where an LM controls the workflow, a system where you have an LM which is responsiblefor figuring out what happens next, including potentially calling other LMS, making use of tools orfunctionality that you're running.That is one common definition of a genetic AI, and another one which is becoming, uh, probably morecommon these days is that a genetic AI is when you have an LM in a loop and that loop, in other words,it's repeatedly being called, it's calling itself repeatedly, and its inner loop, and it has accessto tools which allows it to take various actions.And that idea of an LM and a loop with tools, that's something that if you've ever used Claude code,you're very familiar with, because it really is front and center, you see the to do list that it'sworking on, you see it planning out what it's going to do and then ticking them off one by one.And it is just a series of LM calls carrying out each of these actions.Another word people love to throw around with their genetic AI is the word autonomy.Autonomous.this idea that you're you're allowing an LLM to sort of be master of its own destiny, choosing whatit wants to do next.Of course it sounds.It sounds like voodoo, but it's all again, it all just comes down to it taking in an input sequence,generating an output sequence.But when it generates that output sequence, that can include some instructions about what activitiesit wants to do next.And when you do that, when you have an LLM predicting tokens, which says, this is what I want todo next, then that is in a way autonomous.It's choosing its own destiny.And that is a sign that you're working with a genetic AI.And so so there you have it.We'll, we'll we'll talk more about it later.We'll build some in the final week of this course.It's going to be great.And a genetic AI all the rage.And if you haven't well, we just saw Claude code, so you already probably have a pretty decent feelingof what it feels like to be working with a genetic AI.And also, of course, the GPT agent that we did that found that, that that a reservation for for banoffeepie in the evening.


================================================================================


# 1.27 Day 4 - Parameters- From Millions to Trillions in GPT, LLaMA & DeepSeek

When I was describing Transformers, I mentioned this word parameter and it's a word you hear a lot.In fact, we talked about like the 8 billion parameters in llama 3.2 and the what was it, 270 millionin the smallest version of Gemma.So what are these parameters?Well, you may already know what parameters are.You may have a very good sense of it.And if you don't, I've put some short videos in the resources of times when I feel like I did a greatexplanation of parameters, and so to avoid boring the people that already know it, I'll just suggestif you don't know what these parameters are then, then check out the videos that I've attached in thecourse resources and you'll get a really great briefing on them.Um, but the, the, these these parameters, back in the traditional machine learning days, we usedto have somewhere between 20 and 200 parameters that were control and model choose the creditworthinessof someone based on on maybe 20 factors, 30 factors, that kind of thing.And the first time that that neural networks, that deep neural networks came out.I was just so shocked by just the number of these different parameters that we were talking about.And when GPT one came out and it had 117 million parameters, just an extraordinary number.I remember thinking, I can't imagine what all of these parameters are being used for, and I can'timagine we're ever going to need a model bigger than this.And I was a bit wrong there.Well, this is what happened next.GPT two had 1.5 billion parameters GPT three.175 billion.GPT four had 1.76 trillion parameters.It's it's an extraordinary number.And the latest frontier models.We don't actually know how many parameters they have.They haven't revealed them.There is a sense, actually that that we've been able to reduce parameters a bit generally.I'm sure there's still they're probably still in the tens of trillions, but they have come down a bitbecause we've got much more efficient at sort of packing information into a smaller number of parameters,as evidenced by the fact that that smallest Gemma is only 270 million parameters.It's smaller than GPT two, and yet it's way more powerful than GPT two.So we've definitely been able to do more with less when it comes to parameters.Nonetheless, as a general rule, more parameters means more intelligent, more training informationhas been absorbed into the neural network.And generally speaking, when you see that there are models that come in different flavors, like GPTfive has a nano, a mini, and GPT five the whole one that represents three different sizes of the model,with three different sets of three increasing numbers of parameters.Now, the Frontier Labs typically haven't been explicit about that, so we don't know for sure and wedon't know how many, but that's certainly everybody's understanding.Similarly with Claude, you've got haiku is the smallest sonnet in the middle and opus the largest one.And we understand that they are three different sizes of model.And that's certainly reflected in the cost, which we understand, of course, is the actual computecost.Basically how much it costs to calculate all of these parameters, these trillions of parameters.So so that's why there are these different variations of model.And they represent different sizes, different, different uh, different amounts of parameters in themodel.And we sometimes call that training time scaling.It's saying a bigger model, more parameters.It needed more.It was more expensive to train it.And it can typically absorb more training data.There's something from from a few years ago called the Chinchilla scaling laws, which you may haveheard of.It's not talked about so much anymore, but it says that generally speaking, the number of parametersyou have in a model is roughly proportional to how much training data it can successfully be trainedon, and still absorb that training data.So generally speaking, more parameters means we could do more training.We've got a beefier model.And so as I say, this idea of having more parameters is sometimes called training time scaling.That can be contrasted with another way to get more out of a model, which is sometimes called inferencetime scaling.You remember I said inference is just the word for when you run a model.When you've trained it, it's done.You've got the model, and now you actually use it.Well, when you use it, there are tricks you can do that will make it perform better, be smarter.And one of those tricks is the reasoning trick that we've already looked at.It's telling it.Hey, don't just answer this question.I want you to first generate tokens that describe how you will answer the question.And maybe I'll do that janky thing of shoving the word wait in there a few times just to make you doeven more of this.And it turns out you get a better outcome by doing it.Another type of inference time scaling, is about putting more information in the input sequence, liketicket prices around the world, which means that when it's generating outputs, it can it can drawon the input sequence and be influenced by it.And, you know, that's very consistent with techniques like Rag that we will come to.Uh, and so those two techniques I just mentioned are both examples of inference time scaling ways tomake your model perform better at inference time not when you trained it, but when you're actuallyrunning it.And this idea that there are these two different parallel ways that you can get more out of a model,training time scaling and inference time scaling is something that has really come to the to the forefrontin the last year or so, because previously it was all about training time.Everything was about a bigger model.Bigger is better, more parameters, bigger, more training.And in the last year, inference time scaling has really taken off.And you know, two years ago it was Rag, which is inference time.And and then in the last year it's been a lot about different reasoning techniques.And basically these are two different orthogonal tracks to getting more out of your model.And you can do both.You can scale at training time and scale at inference time.And we'll talk a lot more about this over the course of the next eight weeks.And you'll see some real examples of both in action.Okay.But I just want to superimpose on this line the size of some of the open source models that we've,some of them we've already come across, and some of them we will.On the left, I've put the llama 3.2 with 3 billion parameters as a yellow dot.There you can see it's just slightly more than than GPT two.Uh, I should point out, of course, if you didn't already realize this, that this is a logarithmicscale, so that every one of these tick marks represents an order of magnitude ten times bigger.It's not like a linear scale with one, two, three, four, five.It's 1 billion, 10 billion, 100 billion.Uh, so so llama 3.23 billion is give or take close ish to GPT two, but way more powerful.Llama 3.1.It has a smaller number, but it's a bigger model with more parameters and outperforms llama 3.2.It's 8 billion.Uh, llama 3.3 is 3.3 billion.There is also a llama four comes in a bunch of different varieties.There's a multi modal one that I think 2.45 billion is uh is the more common one GPT os the open sourceversion GPT.We've already played with the 20 billion version that I'm not showing here, but we also played withthe 120 billion version, which is the heftier one.We didn't play with it.We had it playing the Outsmart game a moment ago.And then the biggest one I've got on this diagram, Deep seek has 671 billion parameters.And most of these big models are what's known as a mixture of experts model, which means that withinthem they contain a lot of smaller models that get activated, get used based on the question that'sasked.Take a moment to absorb this scale, this logarithmic scale of parameters, and and marvel at the enormityof the latest frontier models in the probably tens of trillions of parameters.Absolutely extraordinary.If you haven't seen my videos and the resources about parameters, then then if you're interested,check them out and and keep all of this in mind as we go forwards and think more about how these modelsactually function.


================================================================================


# 1.28 Day 4 - What Are Tokens- From Characters to GPT's Tokenizer

Let's take a couple of minutes on what tokens are.Most of you know, but hopefully there'll be some interesting stuff here.In the early days of neural networks, we used to build neural networks that were trained to take incharacter by character, letter by letter, and they would output the most likely next letter.And there were some nice properties of this.There were only perhaps 100 possible characters for uppercase, lowercase, and punctuation.And so there's a nice and simple, uh, vocab, different possible inputs.Uh, and, uh, that meant that it could take up little memory and it could be quite efficient, butit required the, the neural network to understand too much like it was.It was too hard to train it to both understand how to make up different words out of their characters,and also associate them with some kind of meaning.There was too much being left to the network, and the next thing we tried was really having one wordeach.Each input.Being a word is the kind of obvious thing to do.And so you'd build up like a, like a vocab of possible words that could be passed in.But then we had the opposite problem that there's so many possible input words because, you know,language, but also because of all the proper nouns, the names of places and things.So this exploded and and we'd have to leave out rare words from the vocab that meant that they didn'tunderstand some things.But this is what we put up with for a long time.And then, uh, the, the, the breakthrough, the idea was to have some sort of a middle ground whereyou have a chunk of text.It might be a word, or it might be a fragment of a word, or it might even be two words.If you have two words that are really common, uh, and you build up a vocab, that is these fragmentsof words, these chunks of characters.And as a result, you can have a limited set.You can constrain it to some set because you could go all the way down to the character level if youneeded to.And you can allow the model to recognize various fragments of words as having some meaning associatedwith them.And basically this approach just ended up working really well.It's a bit like the transformer architecture.We don't think that there's something fundamental about tokens that we needed to do it this way.We could have done it with characters, we could have done it with words, with massive dictionaries.It turned out that this compromise with tokens is efficient.It works fast.It learns about about the concepts really quickly.And it has this nice little extra property about word stems that I'll explain in just a second.But but it allowed, very simply, a neural network to recognize that often you have a case where there'sone chunk of characters and then different chunks come after it, and that that is just moderating themain, the main chunk.And that sort of fits nicely with the way that we happen to to spell things, to write things to, toour language.And so that property again meant that it could be very efficient.And so that's the background to tokens.And now I do want to explain.So these tokens are the input.They are passed in as is the very first input into one of these language models, these LMS or any deepneural network.In fact, it's the number associated with like the token ID, which one of the tokens it is.Uh, it's different from things called vectors.And if you're a pro and you already know about vectors, then some people do get confused about, oh,hang on, what is a token of vector?No, we'll come to vectors later.They're different.They come further down in the neural network.The very first input, whether you're making a vector or whether you're generating next tokens, thevery first input is just the token id.So so put vectors to one side.We will come back to them.They are different okay.Let's look for a moment at the tokenizer that's used for GPT.And you can get to this yourself by going to platform.Tokenizer and have a play.And this is a lovely way to get a hands on view of what is it like turning text into tokens?


================================================================================


# 1.29 Day 4 - Understanding Tokenization- How GPT Breaks Down Text into Tokens

And so you can see on this user interface, I've typed in the sentence an important sentence for myclass of AI engineers.And below it, the user interface has shown how that would be converted into tokens with those colors.And you'll see that in this case, because these are very common words, each word is mapped to a tokenin its own.So Ann has a token important sentence for my class of AI engineers.They each get a token.You may also notice you can pick the different model in the user interface, because different modelsmay have slightly different strategies for how they turn things into tokens.That's not super important.Some people get worried about does this mean I have to pick a model that will be more efficient?It's just some different decisions they made during training doesn't make a huge amount of difference.But if you're if you're investigating tokens, then pick the one that is the one that interests you.So I wanted to make the two points.One is that that each word has gone into a token.The other point I wanted to make is that you'll notice that in the way that it's colored things in,it's its coloured the space before the word as well, and that reflects the fact that the token isn'tjust the word important, it's also something called a beginning of word.Part of that it's telling the model that's fed to that.What comes next is a new word, and it's the word important.And those both those bits of information are a part of what what that token represents.And if you had a word like unimportant it wouldn't contain that token.It would be a different it might be un and then another version of the important token, but withoutthe space in front of it.Subtle point is worth understanding.There is this difference between tokens that represent the start of a word and tokens that representa fragment, the middle of a word.And I'll bring that to life in the next one.Okay, so here's another one.I've now got, uh, an exquisitely handcrafted quip for my master's of LLM witchcraft.Okay, uh, steady on, uh, the flowery language for you.But the reason I do it is because there are some less common words here, and even in an invented wordin mastera.And I wanted to show you how those more, those less frequent words get broken into multiple tokensso that they can still be understood by the model.Check out the colors exquisitely is obviously not a common word, because look, it got X as part ofit and then quiz it li.Lots of different fragments there.Handcrafted was split into two hand and crafted.And so it does have it will be mapped to something which which has some understanding of of hand andthen something which ends crafted.Notice that that token crafted doesn't have that space before it.It represents a word ending crafted.And so it's mapped to something that the neural network will recognize wherever it's had something crafted.The next one is quip, which has been broken into and and then for and my get get separate ones Mastersgets master, which is the, you know, represents the verb to master and then errs as you know, someonethat does something.And so it kind of works.Again, it's not super important.It's not like the neural network needs it to be done this way.It's just more likely to train faster and to be able to capture, reflect the meaning of these wordsquicker.If the way that they're fed into it already is kind of similar to the way that these words are constructed.And then of and then LM wasn't as common.Now LM, of course, is like insanely common expression.But at the time that this tokenizer was built, LM wasn't such a big term.And so LM got broken into two tokens and witchcraft gets witch and then again craft as the as the ending.Um, and so that it again, it's similar to the meaning of these, these compound words.Uh, so so that gives you a sense.It's not it's not actually a compound word, but but like a word that's composed of multiple pieces,so it gives you a sense of how it works.You can see that there were 66 characters here, and it got mapped down to 18 tokens.If we go back to the previous slide, we'll see there there were 50 characters that got mapped downto nine tokens.So it gives you that that kind of sense for for the ratios that are involved here.And it's also interesting to see this next example.This is Edna's fave number is 3.141592653589793.Uh, you can see that, uh, the way that's been broken down is that that number has been split intofragments of three digits at a time.And it's kind of interesting.Basically, they've got a separate token for every possible three digit number.So a thousand of the tokens in, in, in Gbt's vocabulary, uh, consist of just different three digitnumbers.And any three digit number will get mapped to one token.And it was it's interesting the way that it that it does this And it might.It was in the early versions of GPT before it got as as as smart as it did.You'd find people used to joke that if you gave it a three digit math problem, it would get it right.But if you gave it a four digit math problem, then it would flounder and give like a random answer.So people would say that, you know, when the overlords take over and they're coming for us, we'lljust just set them a four digit arithmetic problem and they'll be stumped.But unfortunately for us, they are now easily able to handle four digit numbers.But but the understanding at the time for for why it was difficult for them was because it was thensplit across multiple tokens, rather than it just simply being something that it could always havein its training data, because it was one token combined with another one token has an answer.So anyways, it's it's interesting to look at this kind of effect.You should take the time, even if you're a pro, take the time to go to platform tokenizer.Mess around yourself because it's always good to get that sense of of how it's working under the hood.The rule of thumb people tend to use is that typically, typically a token ends up being about fourcharacters.If you're if you've got a character count, you're trying to turn it into the number of tokens.Or the most common rule of thumb people use is that, roughly speaking, a token is like three quartersof a word, so a thousand tokens would be about 750 words.That's a way to think of it.1000 tokens, about 750 words.And give another example here, the complete works of Shakespeare.People tend to have a sort of a sense of how much content that is.You can picture the volumes up on a bookshelf, the Complete Works of Shakespeare, that's about 900,000words.So it's about 1.2 million tokens.So give or take, people often talk about the cost per million tokens.That's something we'll talk about in just a second.You see that quoted on in lots of places roughly.You can think of that as being like, uh, the, the cost of the complete Works of Shakespeare, thecomplete works of Shakespeare a little bit more than a million tokens, but roughly.So that's giving you a good sense of how to map between tokens and words.And it's worth pointing out that that's true for language and particularly for like common English language.But if you start to move to things like math or scientific terms or code, then it typically uses upmuch more tokens, because these words are things like variable names that can be obscure, uh, andcode can be can be quite, quite complex and dense.And as a result, it can be more closer to a token per character.Uh, but, but but always a bit more efficient than that.So you can you can do some experimenting.You can also try putting some code in to GPT tokenizer and get a good sense for that mapping.It's useful to have a good gut feel for how many tokens different amounts of text take up.And now I know what you're thinking.You're thinking editor, you told us this was going to be a practical session and you've just been yabberingaway for ages.Okay, I hear you.Let's go to the code.Let's go to cursor.


================================================================================


# 1.30 Day 4 - Tokenizing with tiktoken and Understanding the Illusion of Memory

And so here we are back in my favorite place to be.And we're going to go into week one.And we're going to go to day four.And here we have day four.Tokenizing with code.Start by picking a kernel on the top right.Click there.Select kernel Python environments Vmv.All right.So I'm going to begin by importing a package called tick token.And then I'm going to say I would like to get the tokenizer the thing that encodes text into tokensfor the model GPT 4.1 GPT 4.1 mini is often my favorite model for chatting.It's a chat variant.It doesn't do all the reasoning thinking, so it's nice and simple.And let's see what happens if we encode.Hi, my name is Ed.Now this isn't making an API call.This isn't going to the cloud.This is just doing a lookup of a mapping.Let's see what those tokens are.Oh and here we go.They are token number 121949221308 and so on.And you're like, oh yeah that makes complete sense.So so it turns, it looks up, it breaks this.Hi my name is Ed into different chunks and then it looks up the number, the ID associated with eachone in its vocab.And that's the numbers you see there.So what we can now do is just loop through that and decode, turn each of these numbers back into thefragment that it represents.And if I run this you can see that's literally just what I'm doing.I'm calling this decode function.And so we discover that this token 12194 is in fact the word hi.My name is Ed, and I'm pleased to see that Ed gets its own token.How about that?Uh, number 6117, no less.I think there's some idea that a smaller number represents like, like higher up in its index meansit's one of the the more kind of more primary, uh, first few words in its dictionary.Uh, I don't know if it actually means very much, but but, uh, it's quite fun to try and guess tosee if you can get, uh, words which, which are very high up in its vocab.Anyway, so that is.Hi, my name is Ed and we could do more.We could say hi, my name is Ed aired.And I like banoffee pie because it knows what to put there.Ah, there we go.So we will run this, uh, and run this.And here you see the same tokens began, and then a bunch more tokens came afterwards.And what do we get?Hi, my name is Ed, and I like Ban Offee pie.Um, and, uh, so interestingly, not that surprising.Banoffee does not get its own token, and, uh, it, uh, does break banoffee into, uh, these intoban and Offee.Um, and, uh, that's that's why we see our first fragment there.And offee is token number 26,458.I guess it's more fun to try and pick a word that is this far out in the vocab.Um, but you can see you can you can, uh, look here and you can see that token ID three, two, sixis the word.And and sure enough, if we look to decode three, two, six, we'll see that that is in fact the word.And so that's how you can use tick token as a way to translate text to tokens and tokens.Back to text.And you can count up the number of tokens and use it to experiment.It's very similar to using the UI that we did before, but I would suggest come on in here, mess around,find a little challenge, find a word which has a really small token number and a really big token numberdoesn't.It's not not relevant to anything, just just something fun to do and get a sense of how words get brokeninto individual fragments.And I have one more topic before we go back to the slides.And it's a really interesting one.It's something which is probably obvious to many of you, but for some of you it's not.And for you it will be a real help.It's a real aha moment when, when when I connect the dots for you here, there is this thing that Icall the illusion of memory, which it's so important that is like super, super clear to you.So look, here's the thing.Let's start by just doing the same as we did before.Let's Load the dot env.This is loading the dot env file.Bringing in the secrets.Making sure that the OpenAI API key is set.It looks good so far.If you're not using OpenAI, hopefully by now you know exactly how to switch this to a llama.Okay, so the next thing I'm going to do is run these two lines.And by this point, you should be super comfortable with what this is doing.It's not bringing up some LLM.What it's doing is it's creating a new instance of OpenAI Python client library, which is a wrapperaround making HTTP calls to an endpoint with a model.By default it's it's OpenAI.You can switch it for another.There we go.We know it.Well okay.And this is the other thing, you know.Well, the way that you make an API call is you pass in a list of dictionaries.Each dictionary has, uh, two, two things in it.One thing has a key of role.And the value can be system for the system prompt or user for the user prompt.So here I'm going to say you are a helpful assistant.And my user prompt will be hi, I'm editor.All right.There we go.That is the message.So we now are going to call OpenAI.And I'm not typing it in but but you know it it's response is OpenAI create.You pass in the model.You pass in the messages.The same message we've got right here, this list of dictionaries and what comes back.We take choices, zero message content.And, you know, from from the prior day that this is basically just making an HTTP call and then unpackingthe results.And let's do that.And back comes the response.Hi Ed.Nice to meet you.How can I help you today.Oh that's nice.All right.Okay.So now we're going to ask a follow up question.And my follow up question will be, uh, what's my name?So I'm saying, uh, you know, system message.You're a helpful assistant user message.What's my name?Okay, I know half of you know exactly what's going on here, but hang on.So I call again Open air completions create.I pass in the model, I pass in the messages and I get back response message content.Yes, yes, get on with it.Here we go.I don't have access to your personal information.How can I assist you today?What?So what's happened here?We just said my name is Ed.It's said.Hello, Ed.Nice to meet you.What's going on?All right, well, for those that, uh, that don't know what's happening is that every time you callan LLM, every one of our calls to the LLM over this API is completely stateless.Every time we call it, we're giving it this input sequence that gets fed in as the input sequence tothe GPT model, this transformer that predicts most likely next tokens and it predicts the tokens.I can now say tokens without saying I'll get on to that.It predicts the tokens that are most likely to come next fresh every time.It doesn't have any background.It doesn't know that you called it 20s ago with your name.It just has this input sequence and it's predicting the next tokens and you'll say, but that's notmy experience of using ChatGPT.I know perfectly well that it can hold a conversation.Aha!And that is because we use a trick.It is the job of you and me to to fool the user into thinking that an LLM has things like memory andthat it remembers what you were saying, and that it keeps context.All of that is trickery that we apply to calling the LLM, and we do it in a super simple way.And this is what it is when we build our messages, list the input sequence that gets put into GPT.We don't just give it the most recent message, we give it the whole conversation.So far we say, you are a helpful assistant.Hi, I'm Ed, and then there's this other thing.I hadn't told you about this yet.There's another row I've got in here where the role isn't system or user.It is Assistant, which is its response back, and it's recording that it said, hi Ed, how can I assistyou today?And then I'm saying, what's my name?And so that that is my my final message here.So it has all of this in the input sequence.And of course when we run this we will get a different answer.Your name is Ed.How can I help you today Ed.And so again apologies if you know all this already, but I do want to reinforce it.These five points I've got here you need it needs to be second nature to you.It's so obvious.Every call to an LLM completely stateless number two we pass in the whole conversation so far with everyinput prompt every time.And number three, it gives the illusion that the LLM remembers what you said 30s ago, because you'repassing it in every time.It can keep the context of the conversation.The whole thing is a trick.It's a trick that it's it's it's producing the next token.And in doing so, it's producing the token consistent with this full input so far.And that gives the illusion that it has a memory of what you said before.That memory is just what we call the input sequence, including the past.And so the LLM just predicts the next tokens in the sequence.If the sequence says my name is Ed, what's my name, then likely next token would be your name is Ednot your name is Joshua or something.That would not be a likely next token.And and that's all that it's doing.And if you didn't say my name is Ed, what's my name?If you just said what's my name, then the next token is like, I don't know your name.So that's that's all there is to it, even if you already knew that back to front, it can't hurt tohave had that reinforced again and to see it viscerally in the code like that.Uh, and uh, the, um, the.Yeah, it's, it's such a core understanding.And the first time someone hears this, I sometimes get people saying, well, that's unfair.Does that mean that we have to pay every time for for all of the past, and it sort of accumulates.And you have to pay more and more with every message.And the answer is yes, you have to pay.And there's a reason you have to pay because you're asking the model to calculate more.You don't just want it to answer what is your name?You want it to answer.My name is Ed.What is my name?You want it to take all of that into account, and doing so requires more computation and you need topay for it.You need to pay the electricity bills for doing this computation, and it's trillions of calculations.Now, you probably will talk about costs in a second, but you probably know that the costs per inputtokens are very, very cheap.Each extra token is a fraction of a fraction of a cent for most, most models.And so it's really very, very small.But nonetheless the cost is there.You do need to do more compute for the prediction of what's to come next in the sequence, to be ableto look back and understand the full context of what's happened before.So a critical learning point, even if it was obvious, it's always worth reinforcing.


================================================================================


# 1.32 Day 5 - Building a Sales Brochure Generator with OpenAI Chat Completions API

Well, it seems like we only just started on our journey together and already we're in day five of weekone.Welcome, welcome.Today is showtime.Today is roll up your sleeves and get coding reminder of what you can already do.You can already call OpenAI's API.You can use it to summarize things.You can contrast the different llms at the frontier.And we've done an introduction to Transformers, tokens, context, windows APIs, the the idea of theillusion of memory, and so much more.But by the end of today, you'll be able to confidently write code with OpenAI API.Or to be specific, the Chat Completions API.You'll use something called one shot prompting, which is just a fancy name for saying giving an example.We'll be able to stream back results.We'll be able to show it in markdown, which we basically already done, and also use JSON results.But also we'll be able to put this together into a proper commercial solution that you'll be able todo in a matter of minutes, and then you'll be able to apply this to your own commercial problems aswell.All right, that sounds exciting.Let's introduce the commercial problem.Okay, so what we're going to do is build something which is able to generate a sales brochure for acompany.You'd be able to give it a web address, and it will generate a sales brochure for possible future clientsor investors or for your recruiting for your team.And the way we're going to do it, it might say that sounds a bit similar to summarizing a web page,but it's going to be a little bit more than that.It's going to be able to go and pull multiple web pages.It's going to take a web page.It's going to find links on that web page that are relevant to the company, and then it's going tofollow those links as well, and then use all of that information together to build a sales brochure.And this is going to be the commercial product.And you could imagine you could package this up and have it as your sales brochure generator.So we'll be using the OpenAI API, of course, that we've already been introduced to.We'll be using this thing called one shot prompting, which as I say is a fancy name for saying, hey,imagine that input sequence we could put in that input sequence, an example of what a good outcomelooks like, and use that as our way to to generate, to increase the likelihood that our outputs arewhat we want them to be.And we're also going to use a technique called streaming, which gives you that very familiar typewriteranimation of the results coming back on your screen.So that's the idea.That's the sales brochure.It's going to be a lot of coding and it's going to be multiple.It will involve two LM calls.And so it's one LM call getting back the results and then using that to make another LM call.And so that's a sort of interesting idea of compounding these LM calls to get more commercial value.All right let's go to Casa.Let's go do this.Okay.Here we are back in Casa.I go into week one, I go to day five, and here comes our day five lab.Remember to click on the top right and select that kernel, which should be the one that is your VM.Okay.So the challenge we're starting, it's very similar to what we did in day one when we summarized thewebsite.But we're taking it to the next level.We're going to build a company marketing or sales brochure by by looking at a web page, finding itslinks and looking at more web pages.And at the end we'll talk about business applications of this.And now, look, some people have said to me, okay, but can't we just ask ChatGPT to do somethinglike this and give it some instructions and give it a URL?The poor rain on my parade and it's true, you can and it will probably do very well.But here's the thing.First of all, remember ChatGPT is a product built on top of GPT, and there are a lot of AI engineersthat work at OpenAI building into this product.And that's basically a lot of what we're going to be doing now.We're going to be acting like an AI engineer working on our product, and it's going to be focused onthe sales brochure rather than being able to do anything.It's got that that functionality.And besides, look, there are a lot of Verticalized AI products out there on my phone.I use Duolingo to to do my Spanish lessons, perhaps on going very well.But, but but I use Duolingo.And of course it has within it an AI feature as well that you can use that to chat with an AI.And the dark secret is behind the scenes that AI is just calling GPT.And guess what?We discovered ourselves.You can just bring up a llama and do it yourself.But that doesn't matter.It doesn't stop me from using Duolingo because it's convenient to have it being carefully crafted.The prompts have been worked on, the information has been put there, and so I use Duolingo, and Ipay that premium amount to be able to use their chat AI, and they've made a lot of revenue from it.And so I want to make the point that just because it's possible to recreate functionality in ChatGPTwith various prompts, that doesn't diminish at all the commercial value of building something withbusiness logic that makes a call to ChatGPT.People sometimes use the expression a GPT wrapper to refer to functionality built around ChatGPT, andI understand that.And over the course of the next eight weeks, we're going to be building more and more functionalitythat takes us away from that GPT, GPT wrapper kind of term to something that's really very bespoke.Today, perhaps you could argue this is a GPT wrapper, but it's a starting point and it's a great startingpoint.And let's get into it.Okay.So we start with some imports as usual.Hold down shift and press enter to run it.If any of these imports fail, it usually means you don't have the right kernel selected.All right.And then as before we're going to initialize our API key.Let's use GPT five nano.Today the cheap variant of GPT five and the API key looks good so far.If yours doesn't, head to the troubleshooting lab.Okay, then you remember this scraper, this little utilities I've put.I've actually, uh, there's this this function fetch website contents we saw before.I've given another function fetch website links, which just grabs a web page, finds any any link onthat web page and returns it.Just vanilla parsing code using this library.Beautifulsoup.That's very popular.If this I this is not obviously this is not a course about scraping.So you don't need to understand how this works, just that we're using this function.But if you're interested here it is.And for those who want to point out, I'm very aware of the fact that it's inefficient that in bothof these two functions, I, I retrieve the web page.But that's because I'm not trying to teach about the scraping.I'm trying to teach about AI, and I want to keep these functions nice and simple so that it doesn'tdistract you.Okay.So with that we have this function fetch website links that I've just written.Nothing to do with AI.I can pass in a URL like Edward, and if I run this then Bam!I get just a list of all of the links that are on that web page.But there are two problems with this.The first problem is that many of these links are not going to be relevant to my page.There's links out to Y Combinator to Hacker News.There's links to other things that maybe aren't aren't relevant.This is something to do with probably the type of site that I'm running on.So there's stuff that's not relevant to my website.And the second thing is that it's possible for some of these to be relative links rather than full absolutelinks.Looking at this now, they do all appear to be absolute links, but on other web pages there will certainlybe relative links to.And so typically what if we wanted to find what are all of the relevant links?We'd have to write some code to do that.And mapping relative URLs to absolute URLs is quite doable, but it's harder than you might imagine.There's some clunkiness to do with with understanding what that what that URL represents, but it canbe done figuring out whether a link is relevant to me or not.Now that is very, very hard.In fact, you know, it requires an understanding of what the link represents because some things mightbe links off my my platform, like my LinkedIn page, which is still relevant, whereas something toHacker News is not relevant.So trying to figure out that logic would be really hard and nuanced and require an understanding andthat kind of thing.As of about four years ago was simply not possible.Like we didn't possess the technology to do it.And now it's just a tiny call to go away.It's just easy to do that kind of nuanced understanding.So this ability to parse information and then in a almost human like way, interpret it and reconstructit.That is something which is which is amazing and new.And you can use this same kind of technique to parse people's resumes to, to, to parse restaurantreviews, whatever you might want, and have a sort of nuanced understanding and categorization of it.But we're going to do that now.We're going to use GPT five nano to figure out which of these are relevant, and make sure we've gotthe right kind of URL.


================================================================================


# 1.33 Day 5 - Building JSON Prompts and Using OpenAI's Chat Completions API

So how do we go about doing this?How do we start?Well, the place to start always with these calls is start by thinking about the prompts.What input sequence do we want to send to the LM to allow it to generate tokens, which are most likelyto achieve our task?Okay, so we start by thinking of a system prompt and a user prompt.So here I've already written a system prompt.I'm not going to type it out because it'll be a lot to type.I'm going to show it to you here and talk it through the system prompt.You are provided with a list of links found on a web page.You are able to decide which of the links would be most relevant to include in a brochure about thecompany, such as links to an about page or a company page, or a careers jobs page.You should respond in JSON as in this example, and there is some JSON.If you're not familiar with JSON, Google it.I'm sure you are.So there is some JSON Jason, and it's got this sort of structure which I've just invented.I just invented this structure with a key of links.And then within that is a list, and each link has a type like about page and a URL.And there's a URL.So why did I invent some Jason.Well here's the thing.These language models have been trained with lots and lots of data, and most of that data fits intothree categories.It's either just straight up English language.They're trained with a lot of English language or foreign language.They trained a lot of language.I mean a lot of natural language.That's the word a lot of natural language.Masses of it.Secondly, they're trained with lots of markdown.Websites get turned into markdown.Markdown is like a like basically shorthand for HTML.They're trained with lots of markdown.So they love markdown and they love generating markdown.They know markdown really, really well.That's a structure they're very familiar with.So markdown is something they know.And thirdly they're trained with a lot of JSON.They see JSON a lot.And so JSON is something that naturally when if you're sending structured information to an LLM, ifyou use JSON, it's going to just be coherent, it's going to recognize it and it's going to figureout what you're doing.And that's why giving it a nice, pleasing JSON structure that feels organized and that represents theinformation the way we want it.I want a type for each of my links.That's another another difficult thing to imagine ever coding that.How would you ever code a way to have a heading for each kind of link, and then at the URL for thatlink.So that's a great way to express the information you want back is by giving it a JSON structure.And in the old days, people used to try and say, you know, reply with this and the first bullet withsome information, second bullet with some information.It's really hokey trying to sort of describe some some way of laying out data.Much better to use JSON.And in week eight we're going to to use something called structured outputs, which gives us a way toforce that JSON to, to be focused on a particular spec on, on meeting a schema.We won't cover that here.We're just going to cover for now.Just giving an example and giving examples of possible answers is sometimes known as a one shot prompting.If you give one example, that's one shot.If you give multiple examples, that's Multi-shot prompting in theory, to get this strictly right,Multi-shot prompting is when you give an example question and then the answer that would be good.And then another question, and then an answer that would be good.Just giving multiple answers is also basically like multi-shot prompting.So in this case we're doing single shot prompting.We're giving it one example.And we're saying given this example, learn from it, replicate it, and it's just going to be fineat it.Remember, all it's doing is it's going to take this as the input and it's generating the most likelytokens to come next.And it just happens that the most likely tokens to come next are basically are actually the actual thingswe do want it to come next.It's going to be parsing our input and coming up with this information as the output.Okay.So that's our system prompt link system prompt.It's called all right.Now don't worry I'm going to do some typing in just a second.If you're like why doesn't he have a type I will I will.So next is the user prompt.So this time well this one I just have it as a variable because it's just always going to be exactlythe same thing.Link system prompt.This one I have as a function, because we're going to generate a user prompt each time with a URL passedin.So a URL will come in and we're going to make a user prompt.And this is what it will say.Here is a list of links on the website URL.Please decide which of these are relevant web links for a brochure about the company.Respond with a full https url in JSON format.Do not include terms of service, privacy and email links.This is an example of what not to do.Is almost the opposite of one shot prompting its kind of these are these would be bad answers.And you can imagine I put that in there because it was including them.It's very common.I mean, and perhaps the most important thing you'll ever learn on this entire course is that key togetting good results from Llms is experimenting and iterating.It's all about the experimentation.That's what it takes to be a good LLM engineer.It's about being a good data scientist as well as being a good engineer.And data science is about experimentation, and there's no better way to get good outcomes than to keeprefining your prompts.So if you do this and it starts generating privacy policies as what it thinks is a relevant link, youcome back and you say do not include privacy policies.You iterate, you improve, you add examples.That's the way to get great results.So I then of course I call fetch website links.Which is that that function I just showed you that gets all the links for this URL.And then into this user prompt I add each of those links and that is what I return.Okay let's run this.Let's run get links user prompt.Let's do it for Edward Comm.Let's see what user prompt.Again we're not calling an LLM yet.There's no AI involved here.We're just baking a user prompt that we will then call with an LLM.Let's see what it says.It says here's the list of links on the website.Please decide which of these are relevant for a brochure about the company.Respond with a full Https.Do not.It's exactly what we just looked at.And here are the links listed out there.That's all I've done.I've built this string, and the code to build that string is in this function get links user prompt.So we now have links system prompt and get links user prompt.So we are finally getting ready to make our first AI call.And it's going to be this function.Select relevant links.Okay let's do this.So it's the chat completions API.So remember the way you do that is you start with we need to make sure that we've actually created aninstance of OpenAI.Did we do that back up here.Let's check the Python client library.Go all the way back up all the way back up.Yes we did.There it is OpenAI.And we've got this variable model.So we have an instance of the Python client library back we come okay.So we can say response equals OpenAI the Python client library dot chat dot completions.And then we're calling the create.We're making a post to that that endpoint.All right.Now do you remember what what we have to pass in.We have to pass in two things.We have to pass in the model, which can just be model like a cursor filling it in.And messages.Now messages of course is the list of dictionaries.So it's a list of two dictionaries, in our case the first dictionary.Here we go.The role is going to be system and the content.Oh see there we go.Content is going to be thank you.Cursor.The link system prompt.Quickly press escape before cursor gives the game away.It's no fun.Alright, so that's our link system prompt.Now the next item the role is user and the content is okay.Okay.What is the user prompt?We don't have a constant.Now we need to call the function that I just wrote a second ago which is get links user prompt.Excellent.So that's that.Uh, get rid of you cursor.Stop with your with your cleverness.Okay.That is messages.Now, I told you before that we just got the two things to pass in model and messages.But but I lied.There is one more thing we're going to pass in.There is an extra parameter we're going to use which, uh, cursor is has already filled in, whichis a response underscore format, response underscore format, which is how we can tell the API thatwe want the response to be in a particular format type JSON object.There we go.Uh, now a quick mention, a moment for the for the for the people that are like, I know all of this,let me just mention something that you may also know, but you might not, which is that one of thethings that this does that's really quite sneaky and people that aren't familiar with this put yourhands over your ears for a second.One of the things this does is it constrains GPT the model during inference time.When when GPT is is, uh, predicting the most likely next tokens, the way it does it is it doesn'tjust predict this is the next token.It predicts a probability distribution, the probable next token.It gives a probability for every possible next token.And often when we loosely say pick the next token, what we mean is pick the highest probability nexttoken or sample based on the probability or something like that.Well, because that's how inference works because it does it token by token.You could also use that to make sure that we never pick a token that would result in invalid JSON.We can basically constrain it so that it's forced to generate JSON at inference time.The LLM can predict whatever probabilities it wants, but the ones that we actually select are goingto be the ones that would make well-formed JSON.And that's super interesting.And that's how even though an LLM doesn't know what you mean, if you say you must respond in JSON andit knows if you say it in English, but it doesn't know that technically, but at inference time wecan enforce technical constraints like that.All right.Now everyone can take your hands off yours.Uh, okay.So that is, uh, that that's that's the, uh, the the API call.It's now time for us to take back the response and do something with it.


================================================================================


# 1.34 Day 5 - Chaining GPT Calls- Building an AI Company Brochure Generator

Okay, let's keep going.So we now say something like result is response C response dot choices zero dot message dot content.Remember when we were just digging through the JSON.That's all it was.And then that result that's going to come back, that's going to be a raw text.But it's format.It's going to be like JSON text.And people that are familiar with with messing around with JSON knows that you can you can convert liketext, a string that contains those squigglies and the quotes and stuff into Python dictionaries usingthe built in library JSON.And you do that by something like links equals length equals JSON dot load string result.So that's turning from a string results with with curlies in it and stuff into actual Python dictionaries.And that's what links will then be.And so now let's just return links and there we go.That is our select relevant links.Let's hope I haven't made any mistakes.Let's run that.Let's call select relevant links for my website.So this is now taking that.It's doing that quick web scrape taking that data.And it's packaging it off and sending it to GPT five nano with the system prompt and user prompt thatwe just described and asking for GPT five nano to analyze it and return relevant links in that JSONformat, and that JSON format will come back as a string.We'll turn that into a Python dictionary, and we'll print that Python dictionary.And I hope that that yeah, it did happen just as I finished.There it is.There is the dictionary.If we just compare it with with up here was where we gave the style that we want the system prompts.Remember this is the style we wanted links for a list of links.And if we come here check this out.We have links.We have a list and each one has a type and a URL type.Home page.Type about page.Type blog post.How about that?We just got GPT five nano to do sort of intelligent parsing with a nuanced way sub selecting just therelevant pages.And yes, it reckons that LinkedIn is a relevant link for a marketing brochure about me.And but it thinks that that, uh, I guess, uh, my, um, and it put my Facebook and my Nebula, uh,in there, but it did not put, for example, my, my link to Hacker News because that's not actuallyrelevant to me.So it, it's successfully thought its way through that, the nuance of understanding what makes a linkrelevant.And that is not easy stuff.Okay.And here I've got the same one again.But this time I've just put a print statement at the top and the bottom so that it's just a little bit,uh, more organized.And it will this time, like print what it's doing while it does it so, so that we can watch later.Um, so let me show that by we'll do the same thing for hugging face Co, the website for the open sourcehub and library that we'll be using a lot in week three and in others.So when I run this, it's first going to print selecting relevant links for Huggingface Co by callingGPT five nano.That's this print statement here.It's now this is the same code as you watch me painfully typing and it gets back the results.It loads it.And then at the end of it all it's going to print the, uh.Um, yeah, there we go.There we have it.This is the result, uh, and it says found three relevant links.And there they are, a brand page, a careers page, and a company page.Those were all retrieved from the Hugging Face website.And of course, there are many, many more links than that on the Huggingface website.It sub selected down to those three.Okay, now now it's time for us to get a much take a much bigger step and use this information to makea company brochure.So I'm now going to write a function which is going to take advantage of everything that we've builtso far.It's going to be called Fetch Page and all relevant links.Okay.And this this this function is going to wrap the call to GPT that we just wrote.So this is what it does.It starts by doing fetch website contents.So given the URL that's passed in it just calls that the same scrape function that I wrote here, thisfetch website contents.It's calling that just to get the information on that web page.Then it calls the other thing I wrote the the that we just did together a minute ago.Select relevant links.That's the thing that goes out, gets the links, calls GPT sub, selects the relevant ones and turnsit into JSON.That's all in this one call here.We're packaging it up as if it's just calling Python code, or in fact it's going out to an AI.And this is like a hint of when people talk about agentic workflows, about stitching together callsto llms with Python code.That's what we're doing here.Basically, we're calling NLM and just wrapping it in a function.Okay.And then we're going to put all of this into a nice a nice bit of text.It's going to say at the top of it landing page.And then it's going to have the contents of the landing page and then the heading relevant links.And then it's going to iterate through each of the relevant links that we just got back.And for each one it's going to put the heading with link and then the contents.If you're not familiar like with markdown, putting two hashes like that means like a, like a leveltwo heading.Okay.So this is just stitching together a big block of text.Uh, let's, let's see this in action.Let's, let's call it for for hugging face.So first of all when I call that because the first thing it does is it gets relevant links, you cansee that it's selecting relevant links for hugging face by calling GPT five nano.So that's running.Remember it takes about 30s.I have to talk all my way through it.It's found 11 relevant links, so it's found a lot more this time than it did last time we ran it.And here then, is the full text.It says landing page.And this is the contents of of, uh, Huggingface landing page.And then link discord community and something about the discord community and so on.So if we if we show this a scrollable element, you can scroll through this and see each of the differentlinks that's in here with the different bits of information.Okay.So so far all we've done is wrapped an existing call to GPT that we already made.What we're going to do now is turn this into our second call to GPT.All right.So last time we had a link system prompt and a link a get link user prompt.Same again.Now we're going to have a system prompt and a user prompt for creating our brochure.Let's start with the system prompt.You are an assistant that analyzes the contents of several relevant pages from a company website, createsa short brochure about the company for prospective customers.Investors and recruits respond in markdown without code blocks.That's just that extra way to make sure it doesn't put ticks around it all.Include details of company culture, customers and careers jobs if you have the information.So there is my system prompt, and I have an alternative system prompt below it, which would give youa humorous version of that.Maybe we'll come back to you later.So there is our system prompt.And now I've got another one.Get brochure user prompt.Just like before.Just like when we had the Get Links user prompt.This is the user prompt for building a brochure.You're looking at a company called Company Name.It takes company name and a URL.Looking at a company called Company Name.Here are the contents of its landing page and other relevant pages.Use this information to build a short brochure, and then we will plonk in everything that we that webuilt in that that function we wrote together a minute ago.I've just got this little thing here to truncate it to 5000 characters so that I don't use up all ofyour API costs.If we, uh, if we hit some big websites.You may notice I've also got little truncate that I put on the, uh, the web request as well.Just just to keep it sensible.Um, and we'll return that user prompt.Okay.So let's see what happens.Now.Let's give this a whirl.Let's try get the brochure user prompt for hugging face.Let's run this.So when we run this it calls this function right here.And that begins by calling fetch page and all relevant links.And that begins by getting the relevant links.So right now while I speak it's going out to GPT five nano.And it's making this request.And it found seven links.This time it's a bit different each time.And here is the user prompt.And you can see it's your A company.You're looking at the company website, blah blah blah blah blah.And you can look through it and it's basically going to be exactly what we see right here.It's working okay.It's time for us to make the final the piece de resistance, the time when we actually make a brochure.The second call to an LLM based on the first call.


================================================================================


# 1.35 Day 5 - Building a Brochure Generator with GPT-4 and Streaming Results

Okay, let's do this together.I'm going to type this one create brochure.So as always it begins with response equals OpenAI chat completions dot create.All right.So remember we want to give it a model and messages.And uh this time we uh we don't want JSON back.We want text back.So it is just going to be model and messages.So for the model we actually I'm going to suggest we don't use GPT five.Let's use GPT 4.1 um mini.It's a nice model and it's nice and quick.It's just going to be faster than uh than GPT five nano.So for the messages, we are going to want to have a system prompt and a user message.And the system prompt is cursor is showing.I'm just going to press tab.So sue me.Uh, the system prompt is going to be uh, the, the brochure system prompt that we just wrote.And the user prompt is going to be get brochure user prompt passing in the company name and URL thatwe got.There.Thank you.Cursor up.You make it all very easy.And I didn't even mean it to.But just because I accepted that it filled in the rest of this for me.Uh, so we will then take I'm going to I'm gonna I'm gonna defy cursor and I'm going to type it.We get whatever comes back in response and we'll say result is response dot, dot, dot content.And by the way, if you're wondering if you're if cursor isn't prompting you for the same thing andyou're thinking, well, why doesn't cursor prompt me the way it prompts?Ed the reason is because obviously I already practiced this recording.It's not my first rodeo.I'm not.I was a bit more clumsy the first time I did it, and cursor knows that I've done it's seen it already.So?So that's why it's so good.Uh, it's not, it's not really that it knows even the very names that I give variables.It's just because it's seen it before.Uh.All right.And so then I guess we'll do We will do the display markdown result.There we go.That seems great to me.Uh, all right, let's run that.So now with this, we should be able to say create brochure hugging face.Hugging face.Uh, so the idea is, if I run this, I kick it off, it's going to first call because it's collectingthe user prompt.And as part of doing that, it's going to get the relevant links.So it's calling GPT five nano.Now remember from from that other function calling out collecting the relevant links.It's doing the intelligent parsing where it thinks what's relevant, what's not relevant.How do I get rid of relative URLs.Make them absolute.It's doing all of that.It takes about 30s.It's found 13 relevant links.That's going into a new user prompt.That's going off now to GPT 4.1 mini.And that's going to be coming back with a marketing brochure that it's hopefully about to display forus in markdown in this screen.By the time I finish this sentence.If I could drag it on.There we go.Here it is.Here.Here is a marketing brochure.And come on, it's cool.It's hugging face.Brochure about hugging face.What we offer the hub, the data set spaces, our community.It's much more than a platform.It's a thriving, fast growing community.Company culture, careers at hugging face visit is even put in like a link in there.That's cool.Why choose hugging face?Get started today.Uh, and hugging face the AI community building the future your home for collaboration, innovationand advancing machine learning together.Come on, this is impressive.A nice well crafted brochure.And because we got to choose the system prompt the user prompt the links, we were able to craft thisthe way we wanted and have it be something that is particularly specialized on building a commercialbrochures for the audience that we specified for potential customers, investors, potential recruits.I give you a brochure generator and using it to make a brochure for hugging face.But wait.There's more.One more thing to show you.I want to show you.Streaming back results.So the streaming, of course, is when you see that very familiar typewriter animation.You see stuff that sort of flows back from the LM, and it's remarkably easy to do it.And that's what we'll do right now.LMS generate content token by token anyway, so why not print it token by token as well?You remember that when we call chat completions dot create, we pass in two things.We pass in the model and the messages.And we learned you could pass in a third thing the response format where there is another, there'sa fourth thing and it is stream.You can say stream equals true by default.It's false.If you pass in stream equals true, then what you get back is not the response object, but rather youget back an object called stream.And it's something which you can iterate over, like I do right here for chunk in stream.And as you iterate over it, you get back each chunk as it's formed, as it feeds back, as it flowsback from the LM.So this gives you an easy mechanism to be able to stream back results onto your user interface.Now also because we're bringing back markdown, I can't just print it.I have to do something clever to make sure that I update the markdown I've already got.So this is a little bit more fiddly than it needs to be, perhaps than sometimes you see it, but Ihave to first display some empty markdown and get like something called the display handle, so I canthen update that markdown each time with the response.That's why it's a little bit more wordy here than than you often see it.It could just be a print statement if we weren't using markdown.But then this is the meat right here.What we do is that as we iterate through each chunk, we look at the chunk, we do chunk choices, zerocontent.And that should sound quite familiar to you because it's very much like response choices, zero content,but it's chunk zero Content instead of message.And that's because the delta represents just the little extra bit we're getting flowing back to us forthis particular chunk.So we aggregate all these chunks to response and we update the markdown.And that means that this is going to show it to us in a nice format.And we're using GPT four mini right there.Okay.Let's give this thing a whirl.So we're going to say uh, stream brochure.Let's try and stream a hugging face and the Hugging Face website.Let's kick this off.So remember the first thing that happens is it calls the same select relevant links calling GPT fivenano.And this is the thing that's going off GPT five nano and collecting and it's parsing making JSON, lookingfor relevant links.And as I say this, this takes like 20 30s.Sometimes it takes longer because, uh, this is a reasoning model.And so it's welcome to think through as much as it wants.But it's happened.It's come back and already the results are streaming back.You see, the way it's streaming like this, we get to see all the information coming back and isn'tthat gorgeous?That's a much nicer user experience with the typewriter animation and what you what you now really understandis that this isn't just about user experience.This is actually showing you what's happening to the model.The model is generating the output a token at a time and responding with that token that got generated,and then feeding that token back in and generating the next token.And we're getting that steady flow of tokens back streaming using a technology called ZK.If you're familiar with this stuff, that's the protocol.That's how how the data comes back to us.Um, and so there we have it, a streamed company brochure.And, uh, it looks terrific.And, uh, it's got got again, company culture, customers, community.And I really love it.Empowering the next generation of machine learning for a better open future.Uh, so our second company brochure is a success.And of course, I wouldn't be doing my job if I didn't go back and change the system prompt and showyou how we can have ourselves a rather different style of brochure.Let's have a look.Back we go.Uncomment the lines below to give a more.Oh, keep that one commented up to give a more humorous brochure.Uh, so, uh, let's run that.We now have a different system prompt.Um, and now, uh, yeah, we'll just scroll down.You might be confused.Some people get confused that I don't need to reevaluate this, but but it's because we're using whenyou're when you're working with these notebooks, you often take advantage of the fact that these variablesare all basically global variables.It's obviously not good practice.If you're building a production engineering software and you're building things, uh, in a way thatyou want to deploy them finally, which which, you know, obviously I cover in my production course,but when you're in this mode of being a scientist and you're exploring and investigating different prompts,it's it's absolutely acceptable to be working in this notebook style.In fact, it's encouraged because it allows for quick experimentation.So we're using globals.That's why it just works.If we just change it in that one place.And it's not something to be particularly concerned about.But let's come back down here and let's run this one here.And at this point, it should now be going off again to select relevant links.This takes about 20 30s or so.And at the end of that we should get it streaming back a brochure.And it should be a bit of a different brochure than the one we did before.And of course it's found 14 relevant links.It's now going off.And let's see what it comes back with.Uh, our motto, keep it open, keep it ethical, keep it hugging.What's cooking in the AI kitchen?Customers and culture and career.Geek out with us.Open source at heart.And then a heart emoji.Ethical AI advocates casual tea drinkers, and serious problem solvers.Always learning, always sharing.Always growing.Uh, and so, you know, you could you could say there's obviously a lot of hallucination going on here.It's making this stuff up.This isn't particularly hugging faces motto, but that's what we asked it to do.We asked it to, to create, to imagine, to to give this a humorous take that is consistent with huggingfaces vibe.Uh, and, you know, I'll leave it to your judgment whether you think from looking at hugging facesite, whether it's done a good job of this.Ready to join the AI Hug circle, this stuff is all.It's all made up.But I would argue that it's very much consistent with our objective.And if we wanted, if our style was was we want to make brochures which bring out the the most humorousand the most joyful side of your company, then this might be a really good outcome.And if it's not, that's the time to go back to the prompts.It's a time to iterate, experiment, explore, and improve the output until it's consistent with yourobjective.


================================================================================

