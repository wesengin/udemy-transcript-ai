# 1.6 Day 1 - Installing UV and Setting Up Your Cursor Development Environment

Welcome back to PC and Mac people.We have come together.Both of you have completed the first step of the setup instructions.We are now embarking on the remaining steps and they are hopefully super easy.You've got cursor up and running, you've got LM engineering in block capitals on the top left likethis.If you have this this chat screen over here, you can get rid of it by clicking here and just draggingover to the right till it goes away.So you're left with this.If you're not seeing the files on the left here, then you can go to the view menu and select explorer.And it will come up.And you should be right here.Now one thing you'll notice is that the some of the setup instructions, the readme should be here.You should be able to click on the readme and it comes up.But when it comes up it comes up looking like this.It's what they call markdown.It's a particular type of formatted text.The way to see it in all of its formatted glory is to right click on it and say Open Preview.And when you do that, you will see it like this in all its glory.And you can use this now.You don't need to go to the website anymore.You can use this to follow the instructions and to find all the links you need.You'll also have all the guides you'll have.Everything that was on GitHub is now locally on your computer right here.Congratulations.So we're going to start now by installing and using this thing called Uuv which is wonderful.And I already told you this, but it's wonderful because it's fast, it's bulletproof and it's easy.And and that's why we love it.It's it's taken the data science world by storm.It's now used in all sorts of ways.It's used for MCP, it's used by crew AI, it's used by by lots of people.And increasingly it's becoming the standard for these things.It's something which makes sure that we're all using exactly the same version of Python, and that wehave exactly the same dependencies and that they work on our computer, and it's really great.Enough of the sales pitch.Let's actually use it.Now, the first thing we need to do is to bring up a terminal window within cursor.Cursor has built into it a terminal window which is easier to use than than doing it using the Mac orPC's built in ones.So the way that you do it is you can go to the the view menu and select from there.Uh, the terminal option, it's it's on the view menu.But there is also a shortcut.And that shortcut is to press the control button and find the the back tick.Not a normal apostrophe but a back tick, which on my keyboard is right next to the number one.Um, and control not not command.So control and backtick and up comes a little terminal like this in LM engineering.So that's a thing to know.When you're in this terminal, it's worth knowing you can open a second terminal by pressing the plusbutton here.And now you can see we've got two terminals.And we can get rid of any of them like that.So it's worth understanding.You can have multiple terminals running.You can also open a new terminal by pressing Shift Control Backtick.And now that just opened a second terminal right here, and you can type exit in a terminal to get ridof it as well.And that gives you the basics of using terminals within cursor.Okay.And now within the terminal I'm going to type UV minus minus version to check that I have UV installed.And you probably don't.So when I do that I get a version and you might get an error.And if you get an error it means time to install UV.So the link to UV installation guide is in the setup instructions.I've got it right here.Um, it is uh, the, the installation uh, it's Doctor Astral is the company behind UV UV getting startedinstallation.It's linked in the setup guide.Uh, all of Uv's documentation is absolutely first class.If you want to be a UV expert, just spend a few moments on their docs.It's really easy to use.Installation.Super simple.If you're on Mac.Then you just use this command right here.If you're on windows, then start by just using this command right here.If you have any problems with it at all.If this doesn't work for you, there's lots of other ways.The second way I recommend using is winget.Winget is also very simple to use, so that might be another one.But but do run this to install it.And the way you run it of course is you would copy this or I'll use the Mac version.You would copy this and then go back to the cursor terminal and then paste that command in there andrun it.I'm not going to do that, of course, because I have already done it and I have you've installed.But once you do that, it will install and once it's installed.And this is important, this is a little trap.You need to then open a new terminal to use it, because you need to pick up the environment variablechange.So if you type UV minus minus version you might still get an error.You'd have to open a new terminal like this for it to work.And if it still doesn't detect UV, you might even need to restart your box.But you shouldn't need to.And if after you restart your box you still have a problem, then you might need to try one of the otherinstallation approaches.If you're on a mac and you get errors to do with permissions, a problem with your your bash profile,then that's a sign that you've installed things in the past using a command called sudo, which meansthat you've you've got your profile files with two high permissions, which is a setup problem on yourMac.I've got instructions for how to fix that.So to look for it or email me if you're not sure.But that's very, very fixable.But it's a standard Mac issue, which is very fixable.Anyways, at this point you should be at a point where you can type UV minus minus version and you doindeed see a version of UV.And the next command I'm going to ask you to run is UV self update, which just makes sure that you'reon the very latest version of UV, which you probably are because you just installed it, but if youdidn't just install it like me, then it will update you.You can see I went from naught .8.1 5 to 0 .8. 22 and you might be on something even more than that.But now we have the latest.Congratulations.Okay, it's ready for showtime now.We're now going to run a two word command and it is as follows.It is UV space sync sync.And I just want to be clear that in the the prior version of this course I used Anaconda a bulletproofpowerful data science package management system, which is very, very bulletproof and powerful.But it takes some significant time.And in fact, I would tell people that it could take up to an hour or maybe more to build your fullspec environment.So keep that in mind.We're now going to do the equivalent with UV using UV sync.And here we go.Off it goes.I've just pressed it to run and uh, let's um, uh, you, you can say yes for that, but it's not necessary.Uh, it's just run that that whole UV sync process ran.It might take a little bit longer for you because it needs to download the packages from the internetthe first time, and mine were already cached locally, but nonetheless, it should be a matter of minutesat the most under ten minutes for sure.It is order of magnitude faster than Anaconda.Uh, it's it's really amazing.The whole environment has just been built.And for people familiar with virtual environments, it's basically built this dot vmv this this virtualenvironment file folder right here.Um, and within this, all of the packages that we depend on, it did everything and it did it in aflash.Uh, how does it do it so fast?Well, it's written in rust.It's a super fast, uh, and it's just really great.So we now have a full spec environment.It's built.And step two of the setup is complete.Congratulations.You have your own UV environment.


================================================================================


# 1.7 Day 1 - Setting Up Your OpenAI API Key and Environment Variables

Now.Steps three and four of the set of instructions are optional.I strongly recommend that you get an account with OpenAI if you don't have one already, so that wecan use the most powerful models on the planet, and OpenAI is a good one to start with.But you don't need to.It requires a payment $5 up front, and you might not want to spend that.And you can use free models throughout this course.You can use Gemini or you can use llama.And there are detailed instructions for how to do it.And I will have an example lab as well.But assuming that you do want to go ahead and set up an OpenAI account, you may already have one aswell.Let's do that.That is step three, and feel free to skip this or just watch what I do if you don't want to.So you go to a web browser.I've got an incognito tab here.So because I'm already logged into OpenAI, but you go to platform OpenAI, that is where you come tothe kind of the API side of OpenAI, not to use ChatGPT, but to have your own account to use the API.and that's an important point.Some people are confused about the fact that there is ChatGPT the product which you can interact withand which has a paid plan for like $20 a month and an even higher tier paid plan.And then separate to that, there is an API, the OpenAI API, for connecting to the model directly.And people, some people are confused about that and say, hey, if I'm paying for one, why can't Ihave the other?Luckily, I've written a guide about exactly that.There's a guide called Technical Foundations, which you'll find in the guides folder.It takes you through some of the foundational information to help you navigate through some of theseconcepts.And I explain very clearly the difference between the product ChatGPT and using the API from an AI engineer'spoint of view.So do please read that if you've got any confusion.But assuming that that's that's obvious to you, then then great, come here.Come to platform OpenAI.And the first thing to do is to click sign up.And you can then come in and use like your your Google auth any or Microsoft account or an Apple account,or just create an account with an email address.I think people that are coming internationally outside the US, you might need to click a button likecreate organization or something first.You might need to click through some screen before you get to create your account, but it should bepretty obvious you should get here.Sign up for an account.So far, it's completely free.The money will come in just a second.The $5 spend.Uh, let's let's, uh, come back once you've set up your account.Okay.So this is what you see once you come to platform and once you've signed in with an account, and thenavigation is just a little bit confusing, but you have to be aware that at the top here there's dashboard.And over on the right there's settings.And the first thing to do is to go over to settings, which is where the billing information is, andgo to settings and then come down to billing right here.And this is where you get to put a balance on OpenAI.OpenAI is a pay as you go model, which means you put down an amount of money and then you draw downagainst that over time and you only spend.Tiny amounts of money on this course.We will spend fractions of a cent, typically for each of our projects.Except where I tell you otherwise, you should always track your your spend always come back here.The nice thing about this is that because it's a pay as you go model, you're always paying up front.You're not likely to get some kind of surprise as long as you only top up with a small amount.There are some some other services like AWS where it doesn't work that way, which I don't like.This one is better because you get to be in complete control of your spend.Nonetheless, you should come back and have a look at it.You should always have auto recharge off.You don't want it to automatically recharge.Don't press that.Enable auto recharge button.You don't want it automatically billing to your credit card, unless this is something that you're inproduction and you know exactly what you're doing.So the first thing you do when you come in here, you should see zero credit balance.And you will want to press the Add to credit balance button.And you'll want to to type in an amount there.And you'll want to select a payment method.And then once you've done that, you would then add that amount and sorry, $5 is the minimum.You don't need to spend $10.$5 is the minimum payment amount.You will then draw down and that lasts a year.So you've got you've got plenty of time to spend that $5.You need to make sure that your payment methods includes a valid credit card that you can charge against.Otherwise, OpenAI will throw various errors.So make sure that you add a proper payment method which it can use.And I should mention a lot of students do have problems with payments not being accepted, getting declinedthrough through OpenAI.And it's worth pointing out that almost always most of the time, any time that I've been aware of thatproblem is with your credit card not accepting OpenAI's charge as an international vendor, OpenAI ishappy to take your money if you let it.It's unlikely to be a problem with OpenAI.It's more likely that your credit card company, for whatever reason, is refusing an internationalcharge.And so you might need to go into your to your bank account and select something which enables that kindof charge.Perhaps something to look out for.Otherwise, if you if you really can't get past this and can't get a balance on there, then you cancontact OpenAI support.They're actually really responsive.Okay.But let me assume you've done this.This is now showing $5 and it's looking great.You've got a good payment method associated with your account.We're ready to set up an API key.Okay.So you're still in settings.You're still seeing this menu up here.And the first thing to do is to select API keys up here making sure you're under the organization heading.You're right up here API keys.And here you go.You can see that that I have some API keys.And you're going to make one yourself.Right now these are your API keys.And it's super important to be careful about what you do next.You're going to press the Create new Secret key button like so.You're going to want to create a new secret key.It is owned by you not service account.You can give it whatever name you want, but something that you remember, like, uh, LM engineeringkey.And now be careful with this for the project.Select default project because you don't want to have any extra constraints.If people if you do anything clever here that that constrains it in some way, then you might get errorslater.So just have default project, which means any project can access this and leave permissions at all.And then you're going to press the Create secret key button.And when you do that which I will do it now it will come up with a key like this.And fear not I'm going to delete this key right afterwards.Aha.Uh this key is your key with OpenAI.It's your way to link your code to using OpenAI.And it begins ESC hyphen, hyphen.And then this long thing right here.And that is your secret key.And that's something which we're going to be using in our code.And if you get this wrong, if you type, if you, if you change one of these numbers or one of theseletters, it's like a password.Everything has to be right.If something's wrong, it's not going to work.So copy it into your clipboard by pressing copy.It's now copied.So when you press paste, it's going to paste.And in a minute we're going to paste it somewhere.And please now don't don't paste it somewhere else first and then copy it again or anything that mightmess with the key.Let it be in your clipboard in pristine shape.We're not going to change it.We're going to paste it exactly as it is into our actual inter cursor.So keep it like this.In fact, you don't even need to press the done button yet.You can keep this screen up here.We're going to go back to cursor and put this in cursor right away.Let's go do that.So here I am back in cursor.By the way if you have one of your terminal screens appearing like this, you can close it just by pressingthe X button there.So you've got nice clean screen.Okay.What we're going to do now is create a file called the file, which is a file that contains your secrets.And it's super important that you do this carefully.It's got to be right or things ain't going to work.So?So this needs to be right.The EMV file is a new file we're going to create, which has to be at the top level.It has to be a file within LM engineering not inside any of these directories.So what we can do is we can get this little blank space just here underneath the bottom of this listof files.You can right click there and say new file.And you can see it's now giving me a new file.And you can just double check.It's at the same top level as all these other top level files.And I'm going to call it it's going to be called exactly a period dot like that.And then the letter E and the letter n and the letter v e v and it can't be called dot EMV dot txt.It can't be called EMV in your name.It can't be called EMV dot it.In case you didn't get the message, it's got to be called EMV and only that and it's got to be in theproject root in this top folder.And there it is.That is the right place to be Okay.And now what I'm going to put in here is going to be my open AI key.And here's how you do it.You have to type open AI underscore API underscore key.And it's got to be exactly this open AI underscore API underscore key.It's got to be in block capitals.It's not this it's not open API key.It's open AI API key.Now you wonder why I'm going on about this.I get more than a thousand emails every couple of during every few weeks.I get that many emails from people that have mistyped these words.So don't do that.Spell it open AI API key exactly like this.Uh, then equals it's open ai API key equals.And then you're going to press paste.And when you do paste it's going to paste in that key that you copied from the open AI screen.Let's check it out.Look it's open OpenAI.API key equals sq.Proj dash.And then this long funny thing.And there's nothing crazy at the end of it.We haven't pasted in an extra space.An extra empty.We've been careful.It is exactly this.Now, that white blob just to the right there means that I haven't yet saved this file.It's not been saved.It's not on the disk, which means it's empty as far as my computer is concerned.So I have to press command S Ctrl s on a PC, command S on a mac or file save to save it, and the whiteblob goes away.And that means it's now saved.This is now in the file I get, I get, I get hundreds of emails from people that haven't saved thefiles.Saved the file.You'll see that there is a little stop sign next to it.I also get hundreds of emails from people that are worried about that stop sign.But that stop sign is actually good news.That stop sign is cursor telling you if you hover over it, it will tell you.It's telling you that AI features are disabled for this file.And the reason for that is that this file contains your secrets.This contains your precious key that connects you to OpenAI and to your credit card.And we don't want that leaking out.And particularly we don't want cursor sending this off to different AI to get some sort of informationon it.So as a result, cursor is saying, hey, hands off.I'm I'm not going to touch this file.I'm not going to do AI suggestions for this file.This file is secretly just for you.So we like to see that stop sign.That is good news.Look out for it in anything with secrets in it.Like the EMV file.All right I think I belabored this one enough.You've got a EMV file.It's called exactly EMV.It's in your project root.In the top level.It says OpenAI API key in block capitals equals.And then it's got this key right here that you have taken from your your actual API key that you tookfrom OpenAI.And you pasted it in here.Congratulations.You've got through the hardest part of the whole course getting the API key in there.Let's move on.


================================================================================


# 1.8 Day 1 - Installing Cursor Extensions and Setting Up Your Jupyter Notebook

Welcome to step five of the setup, the very final step.I'm here in Cursa and just before we get going in Cursa, we have to install some things called Cursaextensions.It might prompt you.It might say, do you want to install the recommended extensions?In which case just say yes because it will get them right.But but if not, then we just need to install them.If you've never done this before and very simple to do, you start by going to the view menu and youselect extensions.And this comes up over on the left where the files were.And you now need to search for Python to bring up the Python extensions, which does things like coloringin code nicely and checking it and the like.Now there are in fact two different Python extensions.One of them is made by someone called Innersphere.Well, Innersphere is actually the name of the makers of Cursa.So this is the one for for Cursa.There's another one made by Mr. Python, which happens to be the going name for Microsoft, better knownas Microsoft, and either of these will do just great.I have the Microsoft one installed.You can have the one you click on it, make sure that you install it, and then you'll have Python installed.And then back to the search and search for Jupyter.And then you'll see that there is Jupyter by this company, Miss Tools AI, better known again as Microsoft.And this has like 4 million downloads and it's great.And Jupyter, which is the name of the particular type of data science interactive work that we'll bedoing Jupyter you want to install as well.Mine's already installed, which is why it says uninstall there.And so make sure that both of those are installed.And once you've done that, finally you go back to the view menu and you go to explorer again to bringback the files on the left.You open the week one folder, the week one directory, and and click on day one dot ipynb, which standsfor IPython notebook, which is one of these kinds of things.And congratulations, you've opened the day one notebook.We're so nearly there.The very, very final step is to go to this thing here that says select kernel.The kernel is the name of the of the Python process, which is going to be running behind all of thisthat we're about to work through.So you have to press this button, select kernel.And it will pop up with this little pop up saying Python environments or existing Jupyter server.It all sounds very high tech.You choose Python environments and you should get this list.You may only have one option on your list, but you should have something where the top option shouldhave a star next to it.It should be called dot env and it should say Python 3.12. something.It might be 3.1, 2.1, 10.11.And it should have this kind of path dot venv bin python this thing and it should say recommended.And this is the one to pick and it should look beautiful.It's referring to that folder right there.You see on the left.That's what this is pointing to.Now, if this doesn't pop up as an option, then go to the troubleshooting notebook that is in the setupfolder.And that is where it will help you out.But it should do.Hopefully you should be able to select it.It's now selected and we have ourselves a Jupyter notebook that's ready to go.These things are called Jupyter notebooks.Uh, these this type of format of thing, this dot ipynb.And that is now set.If you need help, then you go to the setup folder.There is a troubleshooting ipynb.And that is where you'll get more instructions for troubleshooting.But hopefully you now have this open.You're looking at this.You have your your environment set.Your kernel is set.It's finally time to go through our first lab.So these things are sometimes called labs, sometimes called notebooks.They're like Python files which have mixtures of of text, formatted text and code.And you can run the code piece by piece.Each of the pieces are called cells.You may be if you're if you're already in data science, you know this stuff super well.Uh, but, you know, uh, I'm going to explain it to the others.Now I've used I've treat these like, like living, breathing documents with tons of good interestingmaterial.And I do ask that you spend a moment to read through them, particularly this first one, because Igive a real lay of the land here.I explain that we're about to embark on our first LLM project and look, it's a simple one, but you'vegot to start somewhere.And for the pros, you can really rattle through this really fast.But you know, we we have to do something simple later in the course.We're going to be building a complete agentic platform with multiple agents collaborating and doingall sorts of stuff with, with many different llms involved.But we're starting small.That's the way to do it.What we're going to build is something which is a different kind of web browser.It's something which is able to take a web address or URL and find a scrape it, and then make a summaryof that web page and display that summary in a nice format.It's our own little Reader's Digest of the internet.That's what we're building.And and sure, you could, you know, ChatGPT now the product has some things like this built into itthat you could ask it, but we're basically building our own.We're building a product like that ourselves, and we're going to do it using an API call to the underlyingGPT model.Okay.So I do want to point out if you're new to using these things called notebooks, they're also calledlabs, also known as Jupyter Lab.As you now know that I've got lots of guides here that will help you through this.There's I think you already know there's guides to the command line, guides to using git and GitHub.There's some technical foundations to to explain things like endpoints and APIs and environment variables.And then there is this guide all about using notebooks, which you should look at to learn more abouthow to use notebooks.And there's also some Python foundations if you're new to Python as well.Um, also, we'll talk a lot about what to do if you want to use models other than OpenAI, such asusing Gemini, which has some free tiers, or using llama with local models, there is a guide thatgoes through all that as well that you can have a preview look at, but we'll be doing it together.A reminder that I am here to help you.You know that you can reach out to me by email through the platform, and you can always connect withme on LinkedIn.I might have mentioned that something I kind of like, so feel free to do it.Uh, there's the troubleshooting notebook.That's that's right here as well that it is in the setup folder troubleshooting right there.So go to troubleshooting if you have any problems.And as I say if this is stuff you already know about then just speed me up.Put me on two x.Go go through this nice and quickly.Uh.And I'll hope to to throw in some interesting stuff as well.But we will get to more juicy stuff very soon.Now, the way that I run these labs is a bit different to some other courses you might be familiar with,and I make the point here, and I do ask you to read through this.I don't like to sit here typing while you watch.I will do that a little bit because I know people want to see that.I'll do a bit of it, but I feel like that's not a good use of either of our time, particularly ina day with AI tools as well.I prefer to show you some code to explain how it works and why it works, to run it, and for you toalso run it.Put in print statements, change it, make it your own.Investigate it.Experiment with it.That's the best way to learn.Not by sitting there typing things through.Now some people like to type and that's that's all good.Of course you can just delete the cell and then retype it, or create an empty new notebook and startfrom scratch as you wish.Everyone learns differently, but I suggest dig into the code, explore it, experiment with it, anduse this as your starting point for your own projects.So.So make your variations and then it's it's great practice.It also gives you an opportunity to make a variation and post it on LinkedIn and tag me.And that's something that I will happily weigh in and make sure that I amplify your achievement.Look at the fact that these labs will be changing all the time.I constantly update them, sometimes multiple times a week, to have the latest models to have betterexplanations as people ask questions.So please assume that these.The text here is a living, breathing document, and if what you see on the screen doesn't match exactlywhat I'm saying, don't worry about it.What you're seeing is better and I'll try.I won't subtract anything.I'll only add more good stuff.So so welcome.The fact that there could be new content that you're looking here.And finally, I do want to try and emphasize the business value of everything that we build.Whilst I'll try and make it fun and entertaining, I will also come back to talk about the commercialimpact of what we do, and there'll be commercial exercises for you at the end.All right.And then it tells you about installing the cursor extensions, which we've done.Select the kernel which we've done, and it's time now for us to get going with some code.


================================================================================


# 1.9 Day 1 - Running Your First OpenAI API Call and System vs User Prompts

Okay, so now you scroll to this first box right here which says imports.And the way that you run this bit of code known as a cell is you hold down the shift button and youpress return.While you've clicked anywhere in this box, I press this and it runs.Now, if you have any errors running this, if you get an import error or something like that, or ifit just doesn't seem to run, it just sits there.It means your kernel isn't set right.Look on the top right there.Look at what mine says.It needs to look a bit like that 3.12. something.It needs to be referring to your local venv right there.That is what you have to see.And then it should run just fine.There's no output yet.Nothing's happened.It should run.It should show a tick mark, meaning that it's run.You've just done some imports.We'll talk more about what all of this does later, but for now, just run that code.Okay.The next thing we're going to do.So come down to this next block of code right here is we are going to load in that dot EMV file, thatsecret file with our environment variables, our secrets.We're going to load it in.And we're then going to collect this thing called OpenAI underscore API underscore key.And as long as you set it right then it's going to find a key and it's going to be happy.Let me see I hold down shift I hit enter and it says API key found and looks good so far.Now if yours doesn't say that then something's wrong with your EMV file and it's going to give someexplanation.Hopefully.Maybe it doesn't start with the right thing.Maybe you forgot to save your env file.Something went wrong.You can go to the troubleshooting lab to dig into it, or you can have a look yourself and try and figureout why it wasn't able to read in your EMV file, but I'm hoping it did.You're in great shape.You found an API key.All is good.Let's make our first call to OpenAI running in the cloud.Now we're going to call OpenAI in the cloud.And if you don't want to, if you want to use Olama, then wait for tomorrow in day two.We're going to redo this using Olama running locally, so that will be your day.So never fear, it will happen.But for now, what we're going to do here is we are going to, uh, to send this message to OpenAI toGPT two.We're actually going to use GPT five, the new version of GPT.And what are we going to do?We're going to say, hello, GPT.This is my first ever message to you.Hi.Uh, so I've put that in this variable message.Now, the first thing I have to do is I have to put that into another variable called messages, whichis in a special format that OpenAI expects.And you'll see that cursor already fills in what it knows I'm going to do, but I'm going to ignorecursor.I'm going to type it because I know some people like me to type, so I'm going to type.So what you send to OpenAI, the format is a particular format that OpenAI came up with, and we'regoing to talk so much more about this format.So I don't want you to worry about it right now.I'm just going to tell you that it is a Python list and the Python list contains a number of dictionaries.Python dictionaries.In this case, we're only going to have one dictionary.There it is.So it's a list with one dictionary in it.And that dictionary is going to have two keys.One key is called role.And the other key is going to be called content.And that role we're going to put in that in for the key role.We're going to have the value being the word user.And we'll find out why later.And there's going to be another field, another field with key content.And what we're going to put in there is going to be that message.Hello GPT, this is my first ever message to you.Okay, that's all clear.Let's just print what that see what that looks like.It looks like uh, list with one dictionary in it with role user content.Hello, GPT, this is my first ever message to you.Hi.Okay.That's easy.Next up, I'm going to hover down here like this.And I'm going to add in.So I just hit another code block by pressing code right there okay.Now I'm going to type in, um, OpenAI equals open a equals open AI like that.I'm creating a new instance of an object, OpenAI.And again we will cover exactly what that's doing another time.And now I'm going to make my first API call to GPT running on the cloud.And this is how I do it I type response equals OpenAI dot chat dot completions dot create.And that might sound like it's nonsense to you.You will get so used to that that you're going to dream it eventually.But for now, just believe me, that's what we type.It's known as the Chat Completions API.And you say OpenAI completions create.We pass in the name of a model.And the name we're going to go with is GPT five Dash nano, a very cheap version of the latest model,GPT five.At least the latest for me right now.And then the messages I'm going to pass in is going to be this variable messages that we just created.And that is it that is going to call GPT.And with what comes back, what we want to take is we want to take this response and we want to takethe choices variable there.We want to take the first choice message content.There it is.That's what we're going to do.I'm going to now press shift and enter.And when I do that it's actually going to call out to to GPT running in the cloud.Let's do it.Let's see what happens.It takes a little bit of time for for that.And it says, hi there.Nice to meet you and welcome.Thanks for saying hi.I'm here to help with questions, explanations, writing, brainstorming, planning, learning and more.Encoding.What would you like to do today?If you're not sure, here are a few quick ideas.And on it goes.So, a very verbose answer for our very first call to GPT running in the cloud.And it was all pretty easy.And we were running it from code.And if you know all this already, then good for you.Zoom through it.If this is completely new to you, then we're going to talk a lot more about this.It's going to all make absolute sense very soon.But I want you to enjoy your first call to an LLM.Okay.So I have written a little function that is called fetch website contents.And that little function is right here in scraper pi.Now this is what it looks like.This is not a course about web scraping.So I don't plan to tell you about what I've done here.I'm using a lovely package called Beautifulsoup that's very popular.You can look at this function if you wish and use it yourself.But it's very, very simple.And it's doing just what's known as a server side scrape that it just collects that web page.It's not actually rendering this web page.So it's not a very sophisticated web scrape, but it fetches the contents of this URL.So that's what we're using here.And for example, I could pass in my own personal web page https.Com and I could use fetch website contents and see what I get there and what I get if I run.This is a whole lot of stuff from my website.And if I actually if I say like print ad, then we'll get something a little bit more than that.And here it is.This is the contents of my web page with lots of stuff.And it's worth noting that nothing to do with AI at this point.This web scrape is just a simple fetch of a web page.That's it.But now we're going to start to involve AI in this.So I imagine most of you know this already, but some of you might not that there are these two differentkinds of message that you send to an AI, to an LLM, two different kinds of prompt.One is called the system prompt and one is called the user prompt.And the system prompt is where you specify the frame, the overall task they're carrying out.You tell them what tone they should take.You give them context.We'll find out later.When we talk about things like Rag that you can supply information in the system prompt, it's yourway of setting the scene.The user prompt is the actual message coming from an end user to the LM that it should respond to inthe context of whatever you told it in the system prompt.So that's the difference between the system prompt and the user prompt.And people that are very familiar with this can can add all sorts of stuff to that.But but we will get there in good time.So for example, we could have a system prompt that says you are an assistant that analyzes the contentsof a website and provides a short summary, ignoring texts that might be navigation related respondin markdown.It's framing the situation, it's setting the task.It's giving the format, the style of how response should come.That is our system prompt.And we're going to to also define the start of our user prompt, which is.Here are the contents of a website.Provide a short summary in markdown if it includes news or announcements, then summarize these two.That is our user prompt prefix.So far so good okay, so we saw a second ago that when we're sending a message to OpenAI, we do itusing this list of dictionaries.And I showed you one in particular which had role user content.And then we had something like hi GPT.So there's another type of dictionary where you say role not user but system.And in fact this is how you specify the system prompt and the user prompt.Uh, you have a list of dictionaries and we'll have two dictionaries in one dictionary the role is system.And the content will then be the system message.In the second dictionary, the role is user and the content is the user message, the user prompt.And so just just to give you a silly example, here's one.Here's messages role is system content.Let's say not.You're a snarky assistant.Let's say you're a helpful assistant.And then what is two plus two.So this would be messages.And what we can do is then we can say here response is open AI.Look at how cursor fills it in for me.Look at that response is open chat completions dot create model will go with GPT 4.1.Mini know we'll make it 4.1 nano make it a really cheap, simple model.GPT 4.1 is faster than GPT five.It'll be very, very quick.And we pass in this these messages, this this list of two.So let's see what a helpful assistant has to say about this.The helpful assistant.When given this question uh, we need to print the results.Cursor didn't fill that in response.Dot choices zero dot message content.Try it again.Two plus two equals four.So now we're going to change the system message.So it doesn't say you're a helpful assistant.It says you are a snarky assistant.And otherwise exactly the same user message.What is two plus two.We run this.And oh finally a math question.Two plus two equals four.Groundbreaking.I know that's just the effect of putting in snarky there.And now a quick challenge for you in the spirit of of changing things and tweaking the code.Make a different system prompt here.Not a snarky assistant, but something else.Maybe an assistant that speaks like a cowboy, or like a pirate, or an assistant that speaks in pigLatin or in another language.Whatever you want, put it in there.Experiment.Try changing the system prompt several times and get to see how different system prompts with the sameuser prompt will change the tone, character, and mission assigned to the LM.


================================================================================


# 1.10 Day 1 - Building a Website Summarizer with OpenAI Chat Completions API

Okay, it's finally time to put this all together.So before we created our messages, just by setting a variable messages and building that list of dictionaries,what we can also do is write ourselves a little function.This is a function called messages for, and it takes in the contents of a website and it will returnour list of dictionaries.It's going to return two dictionaries.The first of them is role system content.Is the system prompt that we defined up there somewhere.The second one is role user and the content is the user prompt prefix, followed by this website that'spassed in.Okay, that sounds good.Remember the prefix looks up there.Maybe we should put in an extra empty line there.That's probably a good idea.Uh, so that that website will then get get shoved in underneath.All right, let's run this.We've just defined a function that will return this list of dictionaries when we pass in a website.So we have a website.We have one in the variable AD.It contains all of my personal web page.So if I run this cell, we'll see what we get.We get something which is indeed a list of dictionaries.There's one dictionary that's that's got role system content, and it's your assistant analyzes contentsof a website and then role user.Here are the contents of a website.And if I scroll you will see that it's got the full contents of my website.So hopefully this is super simple.We've we've written a function.It can manufacture this list of dictionaries.The list of dictionaries contains a system prompt and a user prompt.The system prompt is telling it.You summarize websites.The user prompt says, hey, here's a website to summarize, and then it has the contents of the website.Okay?And now it's time to bring it together in this lovely function right here.Okay.And just before we run this summarize function, I want to mention I did just make a little tweak tothe system prompt I added in this sentence do not wrap the markdown in a code block.And this is a little trick that you sometimes see that prevents it from putting extra stuff around theresponse.Not super important, but makes the results a bit better.Okay, so this here this summarize.This is the function that brings it all together.We start it takes a URL, a URL like madonna.com.It starts by calling fetch.Website contents that you'll remember is my utility that goes off and pulls a website's contents.We put that in a variable website.We then call OpenAI.Create.That is the chat completions API.That is new to you now, but you will soon know that off by heart you will.You will dream it.You will know it so well.It takes two things.It takes a model.And we'll be using GPT 4.1 mini.And it takes messages, which needs to be the list of dictionaries.Well, we can get that by calling our function messages for for our website that function just here.It returns the list of dictionaries exactly what we want, and with whatever comes back, we call choiceszero content.Let's run that function.It's now been defined and let's give it a try.Let's call it for my website and see what comes back.This is now making the system and user message, putting in a dictionary and dictionaries in a list,sending it to GPT.And before I can finish explaining it, it's come back.We've got an answer.Here is a summary of my website.Our little snapshot of my website about me.It's a bit hard to see because it's all on one line and it's not so formatted, and we can fix thatquickly by making another function called Display Summary.It will call that that function.Here it will call summarize with the given URL.And then it will use this utility display markdown which is a nice way to show markdown elegantly insideone of these labs.So if I run this, then once again it's building the system message, the user message, packaging themup, calling the chat completions API.And with what comes back, I have to actually run display summary for my website.Now it's actually doing that, making a system prompt and user message, sending it off.And what comes back bam is this website summary.Uh, this website is the personal and professional homepage of Edward Donner, a coder and AI enthusiastwho enjoys experimenting with large language models.That sounds like me.Uh, and, uh, it says Nebula, and it's got some projects.It's about my interests and some posts and announcements summarized and how to reach me.Uh, so we've just done it.We've just built something that's able to take a website, scrape it and summarize it, using a callto OpenAI running in the cloud, connected to the API key in your EMV file.Congratulations.Try it with some different websites, experiment with it and hey, you can go to the system prompt andmess with that.How about we get a snarky version of the website instead of the normal version?Any of these things are possible.Go and experiment and explore and have fun.Calling the OpenAI API.Well, let's have some fun!Since I mentioned we can change the system prompt, let's go and do that.Let's go back up to the system prompt here instead of your your helpful assistant.Let's make this.You are a snarky assistant that analyzes the contents of a website and provides a short, snarky, humoroussummary.And then run that.Let's redefine the system prompt.Let's just go straight back down and rerun this line here.To summarize.Edward Donald, let's do the display version, make it the pretty version of it, run that and run displaysummary and see what we get.Edward Donner's nerdy Playground editor is your friendly neighborhood code monkey who's obsessed withplaying DJ badly.Stalking Hacker News like it's the only social interaction he needs.Latest news like you're dying to know so wonderfully.Just with that tiny change, we've been able to get a snarky version.Not just a Reader's Digest version, but a snarky version.And we can also use this to do different kinds of tones and styles and try it with other websites,too.And so I'll keep it on snarky mode.And let's look at what CNN's news of the day is.Uh, it's now collecting all of CNN's website, the news site, and it's coming, uh, the CNN, theglorious smorgasbord of everything news.Welcome to CNN, where you can get your daily dose of global chaos, celebrity heartache breaks andgovernment shenanigans in one place, trying to escape world wars, hurricanes and political drama.Too bad it's all here, front and center.Ha!So, uh, yeah, because nothing says fun like a political stalemate so wonderfully.This has given us a summary of the news, but also a snarky version of it as well.And you can try some different websites here too.What you'll discover is that this is a pretty simplistic solution.The way that I'm scraping web pages will only work for web pages that don't get rendered in the clientweb web pages, that you can just fetch them remotely and you'll get the full content.And that doesn't work for too many websites.But luckily, there's an easy way to build this out yourself if you know about this.If you know about tools like Selenium or Playwright, then you can build this yourself.And many students have.And that brings me to the topic of the community contributions.So over here in week one is a folder called Community Contributions.And it's a place where you can put your own projects.And if I open this up, you'll see that a lot of people have come and done this, which is such a joy.You can come in here and add in your own exercises and your own examples.It could be as simple as changing the system prompt to translate a web page to French or Spanish.Uh, hopefully better than any of my Spanish.Uh, and you can do anything that you want.But use this as an opportunity to make changes and to share them with other students.And there are instructions on how you do that.In the guides.There's a guide on the git and GitHub guide includes instructions for how you submit what's called aPR, a pull request, which lets me merge in your changes into the repo.And I would love that.And I'd love it if you shared it.And it's a great way to add value to the whole community and to get your work seen and get get me tosee it and look at it and comment on it as well.So, uh, I want to mention that that there are so many business applications of what we've built,uh, the fact that we've, we've just experienced calling a frontier model in the cloud, and we'vethen given it some instructions to summarize it, to change the tone, we could translate it.There are so many different ways that you could extend this and use it, and you will see many of themin community contributions.So many things.Someone added it to to to call the Yahoo Finance API to summarize stock movements.There's lots of other stuff in here and you should be inspired by this, but try and take it in yourown direction.Apply it to your business or to some some personal thing that's been on your mind.A pain point that you think you can fix.A simple way that you can make a call that involves collecting a web page and doing something with thatweb page.That's it's such an open canvas.There's so many different commercial directions that you can take this, this idea of summarizationor translation.This is one of the most essential and simple gen AI use cases, and you now know how to do it.And you can apply this to any problem you want.So please do that, apply it to a different business and try it out for yourself.Uh, try making something and and have fun with it above all.


================================================================================


# 1.11 Day 1 - Hands-On Exercise- Building Your First OpenAI API Call from Scratch

And now an exercise for you.You've just hopefully gone in and run this for other web pages, and you've maybe tried tweaking itfor a different business purpose.The exercise is now to come in here and see if you can make your own message to GPT from scratch.So first of all, write a simple system prompt.Now you should think about something completely different to do here, perhaps still related to to summarizinginformation.So as an example, you might want to summarize the contents of an email.Or perhaps you might want to take an email and come up with a nice subject, a possible subject forthat email that might be an example of a business task.So in the system prompt here, describe the task that you want it to do in the user prompt.You'll want to put in here the contents of the email or the thing to be translated or whatever the isgoing to be the prompt with the instruction.Then you now have to do some typing, just like I did some typing.It's your turn to do some typing.Replace this messages equals you're going to want to put in here something which will describe yourmessages to OpenAI.So this is going to be a place where you're going to put in a list of dictionaries, two dictionaries.And it's going to have some things in it like role and content, and cursor is determined to fill itin for me.But don't cheat.Uh, try and see if you can't do it yourself and then cheat if you can't.Of course, it's always good to use AI tools.It's not cheating in the least.And then you have to uncomment this line and have response equals and type in the the the command thatyou're going to give, which I'll tell you very quickly, is OpenAI create.And you pass in the model name and the messages, and then you're going to print the result.And the result will be this word response .0. content.And always look back up in this code and copy and paste to get that example.And when you've done that you should find that that works.And if you've built something like something that can give you the subject of an email or somethingelse, then please consider doing a PR.The instructions are in the in the guide so that I can get to see it myself.And I can share in your joy of your first commercial call to an LLM.And then finally, there's this extra exercise I mentioned for people that are pros, people that alreadyknow this stuff.You'll be aware that, as I say, my my way of web scraping is super simplistic, but you could useselenium or playwright or something similar to do proper web scraping with a web page that gets renderedin the browser, and you'll see that other people have done that.There's a playwright version there.I know that there's several selenium versions.You could look at how other students have done it.But first, if you know this stuff, if you've used selenium or playwright before, have a shot at doingit yourself as well to build a more industrial strength.Web page summarizer.And then afterwards, as I say one more time, please do consider sharing your code.The guide will tell you how to do it.Submit a PR.I will gladly review it and I'd love to include it in the community contributions with 1 or 2 otherpeople that have done this too.Okay, so with that, that's your mission.Enjoy it.Fill this in.If if you if you find that you're struggling with it, please don't worry.We're going to be doing this so many times over the next eight weeks that's going to become second nature.Give it a shot.Give it your best crack.And if you can't get it this time, don't worry.We'll be coming back to this tomorrow.Wow.You're probably thinking, whoa, this if this is if this day is what every day is like, this courseis going to go on forever.No, today was a big, big day.And it's longer than most of our days will be.You've got through a ton of material.You've made your first request to an LLM, and I'm hoping that you've also adapted it for a differentcommercial task.And maybe you've even submitted a PR.So much has got done and and it's just just a great start.Your environment is built and ready for prime time.And tomorrow we're going to start getting a little bit deeper.But already, even though your journey just started, your 2.5% of the way along your journey to theto the path of being 100% a master of LM engineering.You've already used a llama to run a model locally on your box.Perhaps several models.Seems like an age ago that we did that.But we did that.You've written code that calls OpenAI's frontier models.You know the difference between a system prompt and a user prompt.And you've built summarization.You know how to apply to commercial problems.Maybe you've built a couple yourself.Tomorrow is going to be a little bit of a breather.It's going to be an easier day, I promise.We're going to talk about the steps it takes between now and being a master of LM engineering.I'm going to set you up for success.We're going to talk through some of the top frontier models out there, and we'll also do a little bitof a dig into Olama and using Olama locally.I kind of repeat of that lab, but using Olama locally on on our own computers, people that didn'twant to to get OpenAI.That's your chance to use Olama instead.So I'm super excited to see you on day two.You survived all of day one that by far the biggest day of the whole course.So tomorrow much gentler.Can't wait.See you then.


================================================================================


# 1.12 Day 2 - LLM Engineering Building Blocks- Models, Tools & Techniques

Oh, hello.It's you again.You've come back for more?I didn't manage to put you off with the horrors I put you through yesterday.You're back for more with day two of LM engineering.Welcome, welcome, welcome.I promise an easier day today than yesterday.I will always start each of these with a recap on what we did on the previous day, and what's to comeso you can get used to this.It's a bit of repetition, but repetition is good sometimes.Already we have played with Olama and got used to different open source models.We've written code to call open AI.Maybe you didn't.If you're holding back on having that key and you're ready for something new today, which we will do.You've learned the difference between a system prompt and a user prompt.And we did a summarization use case, a core gen AI use case that we built on the very first day.And today we're going to go through the building blocks, the steps it takes to get from where you arenow to where you will be in LM engineering master and recognizing frontier models, the different kindsof frontier models, what that means and how to use them.Let's get into it.So look, the first thing I want to say is that this is predominantly a practical course.This is about getting stuff done.I do teach some theory, but I like to do it in the context of the practice of putting things into action.I am here to to allow you to deliver commercial impact through LMS.Whether you work at a startup, whether you're an entrepreneur and you want to start a startup, orwhether you work at a fortune 500 enterprise or anywhere in between, the goal of these sessions isto teach you how to use LMS in order to have commercial gain, so please always keep that perspective.I'm a practical person.I think that the best way to learn is by doing, and doing is what I have in store for you.And there are going to be these three different dimensions, these three different factors to LM engineeringthat we will build on.One of them is the models themselves, and I mentioned this yesterday, I mentioned that bottom layer,the models open source closed source.We'll look at multimodal models that generate images and audio.The architecture of models and importantly, how you select the right model for a particular task athand.We're going to then build on top of models by looking at different tools, frameworks, libraries,things like the fabulous hugging face which which you will get to love.Long chain gradio.Weights and biases.Modal.And then on top of this are different techniques that you can apply.Taking advantage of these libraries and using these models techniques like calling APIs, which we'vealready done Multi-shot prompting is an easy one, is a very juicy one fine tuning a powerful one,and agent ization a genetic AI.You don't get bigger than that.That is the hottest topic of all right now.And so I've designed this course to appeal to many different levels of background.No matter what your expertise is you're coming to this with, there will be something here to appealto you, but you might find some parts are too easy if you're experienced, in which case, hang inthere.Speed me up.Put me on 2X2X right now and we'll quickly get to meaty projects very soon.you might find some parts are too hard for you, particularly if you're completely new to this field.And one thing to know is that all of us, every single person, one thing we all have in common is thatat some point we've all been new.And so we know exactly how it feels.I've written a bunch of self-study guides in the guides folder.Please spend time with them.They really will help prep you.I do believe that you can learn everything from those self-study guides, and I've had people completethe whole LM engineering course who came at this with zero programming experience.It's not ideal.We do.I do aim this primarily for someone with some coding experience, but you can come in completely raw,learn from self-study guides and you'll be successful.Many people have looked at the guides, ask me for help and take things slowly, carefully, work throughthe practicals, add in print statements and above all, don't worry if you don't get it right away,it doesn't matter.Get some intuition.Keep moving on.You can always come back.You can, you can.Some people have actually watched all the videos and then come back a second time and gone through muchmore carefully.If that works for your style, then that's a great idea.But otherwise you can just work through it slowly the first time.But if it feels just right, if I've got it just right, wonderful, then keep going.So the ideal background to come into this course is someone who has intermediate level Python experience,and if you have that, then you're in really great shape.If you look at this line of code and you know what that does basically give or take, then fantastic.If not, don't worry.As I say, I've got great background material.You can either look at the beginner Python guide or the intermediate Python guide, or both, and getto a point where you feel pretty comfortable and you can pick stuff up as you go.And you know what?Llms are amazing at this cursor is great.It will fill in most things for you, and it can explain any line and what it does and it can help youwith it.So with things like what I've got going on here, like a generator and a set comprehension, that kindof stuff, it can explain it all.It's fancy Python stuff.But I tell you, when you see how it works, it's all pretty easy.All right?And there's some tricks to get the very most out of the next eight weeks that you and I are going tospend together to get the most out of it.First of all, do follow along with the coding.It's often better to watch watch me do do a video, watch me talk through it, and then come back anddo it yourself.Once you know what to expect.Now, I know some people dislike the way that I don't often just type out the code.Most of the time I have the code there and expect you to run it.It's a style.I find that it's very productive and it allows you to learn a lot quickly.Typing everything.In these days with AI tools, it's just not necessary.It's more important to understand what the code does and to be able to change it.Now, if you want, you can start a fresh notebook and type from scratch if you prefer that.And some people do learn best by by typing.And sometimes I do that myself.When I'm learning things, it can help, but most of the time I would urge you look at the code, understandit, change it, add in print statements, mess with it in some way, and see how you can experimentwith it.That, I think is the best way to learn.And that's why I like to drive these labs and then do complete the exercises that I've got in the labs.And you should be making those changes and then pushing it to your own GitHub so that you've got examplesof codes and solutions.And you can also make these changes and submit what's called a PR, a pull request, so that I can mergein your changes, as I mentioned yesterday.And there's instructions on that in the guides.If you're new to that, make sure that you apply the examples that I give to your business, or to someidea you've had about how you can apply it to something you work in.The best kinds of ways to apply these things is to something that feels personal to you.One person on the course that it just blew my mind.He had an elderly relative who was having problems renewing his his medications because he was losingtrack of the different refills, the different pharmacies.And the student wrote a tool, an LM based app that helped his relative to know when to schedule, whichrefills with which pharmacy and the dependencies between them, and so on.And it was so cool.And he posted it about it.And of course it got lots of attention because it was very personal and it was real and tangible, andit solved a concrete problem for the student and his relative.That's the kind of thing you want to look for, ways that you can take what I do and apply it eitherto your personal situation or to your work, your professional life, and make something that's specificto you.And then when you've done something like that, or when you've done any of the exercises or anythingthat feels like you've made progress on your journey or just discovered something or had some insightthat you think is interesting or an experiment, post about it on LinkedIn.Make a post if you wish.If you're comfortable, and tag me in the post and make it relevant in some way so that I can then comein and weigh in and say, you know, this is great.This is this is why it's important, and so on.And in doing so, when I do that, of course, LinkedIn makes sure that that gets then shown to allthe people that I'm connected with that are in large part other students on this course.And that helps amplify the work you're doing.Other people will weigh in.And similarly, if you're on LinkedIn and you see that I've posted about some other student doing something,then please do weigh in because it's so helpful.It helps build up the community.It gives attention to all, all of you and everyone's contributions.And it means that when people are looking, perhaps, perhaps it's a future client of yours, perhapsa future employer of yours, a future boss.They will see this kind of action.They will.They will be aware of your expertise and of the impact you're making, and it just helps to promoteyour visibility.So.So please do do that.And most importantly, stick at it.I, I really do say that that even if there's times when you feel like you're not following it, orthere's times when you feel that's not as relevant to me, maybe speed me up, but just stay the course,get to the end.It's eight weeks we're going to spend together.By the end of it you will have mastered LM engineering.It's going to be worth it.Hang on in there.Let's get through it.Keep me posted on the journey.If you have any problems, reach out to me.I always reply and I'm always quick.All right, let's keep going.


================================================================================


# 1.13 Day 2 - Your 8-Week Journey- From Chat Completions API to LLM Engineer

So I already covered this yesterday.But one more time I want to tell you about the eight weeks I have in store for you because they're sogreat.The first week this week we're here on the left.The first week is about the foundations and the thing called the Chat Completions API, which you alreadyexperienced yesterday.OpenAI.That's this week.Next week is about looking at a ton of different frontier models, calling their APIs, getting supercomfortable with APIs, and then hopefully I'll slip in there some things for the pros so that there'llbe some new new information to to give give you some some extra juice.And we'll also look at things like multimodality.We're going to have such a cool project for you to see that week three is when we go deep on open source.So whilst we've used llama so far already to use open source models, we're going to use Hugging Faceand I'll explain the differences and we'll be building some cool open source projects.Week four is when I cover this super important topic of how do you pick the right LLM?And we will also use Llms to generate code or to port code.And an interesting project.Week five is Rag.Now I know you're super interested in Rag.Everybody is.I do advise you not to skip ahead to week five because you'll miss out on lots of great stuff.Um, but but week five will be fun, and we're going to create a knowledge worker expert that is ableto read through tons of documents and answer knowledgeably about them, which is a very important commercialuse case.Week six is when we start to work on fine tuning.But week six is also a core data science week, when we will work with data and we will work with scrubbingdata sets, data curation.It's stuff that I absolutely love, but I know a lot of people don't don't like it as much, but I reallyenjoy it.I hope you will too.It's going to be very interesting.Week seven is the shocking week, the shocking week with a ridiculous outcome that that, uh, hopefullyyou won't be expecting.I shouldn't give give the game away, but it's when we fine tune an open source model, uh, to, toachieve, to do a business task of our own.And week eight is when we bring everything together.It really pulls together many of the prior weeks.And we build in a genetic platform, and it solves a commercial problem, and we go out with a bang.And that at the end of those eight weeks, you will be able to say, I am an LM engineer.That's the plan.And in a cheap move to keep you engaged all the way through, I've definitely been sure to to sprinkleinteresting content all the way through.There'll be tools and resources.There's going to be basically a code cookbook.I will leave you with sections of the code that you'll be able to use in your own projects.But most of all, I've left the best till the end, and there's no point in skipping ahead to it becauseit builds on everything that came before it.So I uses trickery to get you to stay engaged and see it through to the bitter end.


================================================================================


# 1.14 Day 2 - Frontier Models- OpenAI GPT, Claude, Gemini & Grok Compared

Now, you'll be pleased to hear that today isn't entirely admin and logistics.We're going to cover a topic and then we're going to do a lab.So the topic I want to talk to you about is models themselves LMS, the things that that generate content,the things that do generative AI and they are the best ones on the planet are sometimes known as Frontiermodels, frontier LMS.And they're created by companies that we call the Frontier Labs.And that's an informal word.There's not a strict definition of frontier.It just tends to mean the really big companies and their models.Now typically when you talk about frontier models, you're referring to the closed source models, themodels which you have to pay to use because a company like OpenAI spent hundreds of millions of dollarstraining their models, and they need to get back that money, and it costs a lot of money to run thembecause they're enormous.So those models are sometimes called closed source models.And and frontier Models is another name for them or closed source frontier models.And then contrasting with that, there are open source models Models made by companies like like meta,Facebook that made a range of models called llama that are open source.And these are models that you can access and run yourself.The people who feel strongly about this stuff sometimes say that they're not really open source.They are open weight because meta has made available all of the weights of the model, but they haven'tnecessarily made available the training data and methodologies that was used to build the model.But still, we tend to call them open source models.And when you use the word frontier, it's most common to only be referring to the closed source ones.But sometimes people say frontier and they're also referring to open source.It can be used in either context.So just just stay aware of the context when someone uses a word like frontier model.But most of the time when I say frontier, I'm talking about the big closed source models.Hope that all makes sense.Let's talk about some closed source models, and the first one will not surprise you in the least.OpenAI, the most famous AI lab on the planet, made the most famous model on the planet GPT.And of course, it all hit everyone's radar when GPT when ChatGPT came out, uh, which was GPT 3.5.If you're keeping count in, what, November 2022 around then and ChatGPT, of course, uh, reallymoved the needle.That really changed our perception on what these models were able to do.And since then, OpenAI has released many more of these.GPT right now, for me, we're on GPT five.Maybe by the time you watch this, there's another version.But GPT five is the latest.And along the way, they also came out with a different range of models called the oh models.010304 mini.Um, but now the oh range and GPT range have converged.And GPT five is a kind of combination of both.And we'll cover more of that another time.But that is GPT from OpenAI, the most famous model and the most famous lab on the planet.And the next model to mention is, of course, OpenAI's mortal enemies, anthropic and their models,called Claude and anthropic is also a West Coast US start up.It was founded by people that used to work at OpenAI.The OpenAI turncoats that started the big rival, anthropic.And Claude is the model that a lot of people in the industry love to love.Claude is my favorite and many people's favorite.Claude, of course, comes in different varieties.The very small version is called haiku, the middle version is called sonnet and the bigger versionis called opus.But sonnet also happens to be right now the most powerful one.Right now for me, we're on Claude 4.5 sonnet, and it is very, very powerful.And it's kind of neck and neck with GPT five, as we will see.But Claude, very powerful model.It's also, of course, the model behind Claude code, the coding agent platform.And Claude Claude is great fun to work with.And of course, we'll spend a lot of time with Claude on this course.And Gemini is Google's model.And Gemini, of course, is the big closed source version of Jama, the open source model that we playedwith yesterday.Gemini.It's it's fascinating that Google began this game very much at the back of the pack.They were late to the party.Do you remember Bard?Bard was Google's early model.That was something.They were a laughing stock for Bard.It was just awful, and a lot of people predicted that they'd left it too late, that they weren't goingto be able to catch up with OpenAI and anthropic, but Google absolutely caught up.The Gemini, the latest one for me is Gemini 2.5 Pro, but Gemini three is poised to be released anyday now.I'm kind of hoping it's going to be released while I record these videos.You're going to get like a live reaction moment.Uh, but Gemini three, a lot of excitement for that.If you've already got it, don't worry, I'll have updated the labs.You'll be able to use Gemini three instead of I'll be stuck on Gemini 2.5, but Gemini.Amazing set of models.Also, Google has made a lot of the smaller versions of it available for free for students, so youmight in your region be able to get a deal to get a cheap or even free version of Gemini.So that also goes down well with lots of people.That is Gemini from Google.And is there a fourth one to mention?Is there?Of course there's a fourth one to mention.I know you know the fourth one.And you're like, oh, what about.Yes, yes, yes.There is of course grok.Grok with a K from Elon Musk from X.ai.A lot of people love grok for various reasons, and we will cover, uh, and grok is definitely thesort of the fourth of the big four.And of course, there are a bunch of other big frontier models.There's the model that sits behind perplexity that I know is popular, lots of people.There's the one that the Cohere Canadian company has, um, there's, I guess, uh, Mistral, the FrenchAI company, has both a closed source and an open source model.So so that's another one.So there's lots of others in the kind of next tier.But these are the big four.These are the ones that grab the headlines.And we'll be using all four during the course.But but you don't need to have uh keys to all of them.You can watch me doing them.Uh, but we will get to use them all and compare them.And we will find out in week four how you pick the right one for your task.So that is the closed source.Let's talk about open source.


================================================================================


# 1.15 Day 2 - Open-Source LLMs- LLaMA, Mistral, DeepSeek, and Ollama

And now to talk about the open source side of the picture.Some people might still call these frontier models, but I tend to reserve that for closed source.So the open source guys, the first and foremost would be meta with their llama series of models.And really, meta was the first company to go out there and open source their models in In Anger.It's possible one might.If you're being uncharitable, you might say that meta felt like they had they had slipped.They hadn't been able to keep up with OpenAI and anthropic in terms of being in the headlines for havingthe strongest models on the planet.And so they were looking for a way to really set themselves apart, and they took their stance thatAI models should be open source and they open source llama.And as a result, everyone talks about meta as being one of the the leaders in the field because theyhave promoted open source AI.So, you know, maybe it was an intentional ploy to keep the headlines, but for whatever, whateverit's worth, they've been very successful.Everyone uses Lama will use a lot on the course and it's a fabulous model.Lama for right now is the biggest, most powerful one that they have open sourced.But also Lama 3.2 is particularly notable.And you might have discovered this yourself because it comes in very small varieties.And as a result, it's easy to run it on your local computer.Lama 3.2 comes in a 1,000,000,003 billion size, both nice and convenient to run locally.And sometimes people call these things SL instead of LM, a small language model instead of an LM,but really with 3 billion parameters, it's still pretty large.So that's Lama 3.2, which is the small one, which is super popular, and Lama four is the bigger one,Lama from meta and then Mistral, the French company has come up with a bunch of open source models.One of them is called Mistral because it's a type of model called a mixture of experts, which meansthat it has lots of smaller models within it, and it directs traffic to different specialist modelsdepending on the kind of question you're asking.So that's Mick's trial.Quinn is a model or a set of models from Chinese company Alibaba Cloud, the cloud division of Alibaba.And Quinn is a remarkably powerful model that I very much enjoy using.And it's not as well known and as popular as models like llama, but it's really great, and I recommendthat everyone does give Quinn a shot because it's a super powerful and Gemma is the one that we alreadylooked at.It is the cousin of Google's Gemini.It is the open source cousin, and it comes in a lot of different flavors.And it's most notable for having this absolutely minuscule model with, what was it, 270 million parameters,which is definitely a small language model.And it's still I mean, it's not particularly impressive to use it, but it's still able to, to interactand to, to generate language.So it's pretty impressive.And then Fei is Microsoft's range 54 is the latest at the moment as of recording this.And those are their powerful models, particularly good, I think, with tool calling and with withcommercial work.And we'll see how to compare all of these, as I say, in week four.And then the one that everyone loves to talk about is, of course, deep seek from deep seek AI becauseit took the world by storm about six months ago and it came out.And it is important.I think I already said this.It's important to keep in mind what it was about deep seek that caused so much excitement.It wasn't necessarily that it's a super powerful model, because it's not as powerful as as GPT fiveand as Claude Sun at 4.5 or even close to it.What was impressive is that the Deep Seek AI team were able to make a model that is at the frontierlevel of capability, but at a fraction of the training cost than OpenAI spent.OpenAI spent north of $100 million training GPT and Deep Seek.I think it was about 4 million or something that they said they spent training deep sky.So it represented this, this huge, this step change in efficiency of being able to build a frontierlevel model very quickly and easily.And they open sourced it, which is amazing.The main version has 671 billion parameters, so it's way too big to run on your computer.But they also have made smaller variants of it that you can run locally, that you may have seen whenyou were looking at the models on llama.The way that those are actually built is kind of kind of funny.It's they're not actually deep seek themselves those smaller variations of deep seek.They are in fact versions of Llama and Quen, those two models you see earlier up on the list.They've taken small versions of Llama and Quen, and then they've trained it more with synthetic datathat was generated by the big deep seek.So again, they used big deep seek to generate lots of fake data, synthetic data from deep seeks training.And then they've used that to educate these smaller Lama and Shen models.And that process is known as distillation.There's actually various different ways of doing distillation, but that's one of them.And that's what Deep Sea did, and that's how they made various small variants.And you can use those variants in Lama.And it's great fun.They're available for all for free.And wait, there's one more.There's just one more open source model to mention.And you know exactly what it is.It is, of course, OpenAI's GPT, the open source version of GPT that OpenAI released for me quiterecently, like a month ago or so, six weeks ago.And it made a big splash.And people actually say that OpenAI might have done that because their hand was forced by deep sea,that open sourced this model that was almost at OpenAI's level.So that's one reason, perhaps, that motivated OpenAI to open sourcing GPT and GPT OS.It comes in two sizes, the one that we use, the smaller 120 billion.We used that yesterday.And then there's a much bigger 120 billion version that's far too big to fit on my computer.Maybe it fits on your computer, in which case, anytime you want to lend me your computer, that'llbe great.But it's a very powerful model.It's very impressive indeed.And it's remarkable that OpenAI have open sourced it, which means it's also available for free forany of us.And one final point to make before we go to the lab, there are these three different ways that we canuse models.And we'll be using all three of them at various points, but I just want to make sure that you feelcomfortable that there are these three different things, and one of them is using packaged products,products that have a user interface and have functionality built in.And the most famous of them is ChatGPT.It's not you're not using a model, you're using a product.And it's important to have that distinction.ChatGPT comes with a user interface.It comes with features like memory and web search and stuff that's been built into ChatGPT GPT product.So when you're using it, think of it like it's a product.It's had some some people have worked on the model behind that, but also some people like you and me,some AI engineers have built the user interface, the functionality, the the glue that glues a userexperience to a model that's ChatGPT anthropic has it's one two clawed through through the user interface,and there are a bunch of others.Then you can call large language models that are running on the cloud using APIs.So there are ways that you can make a call to an API.And you can call them directly.So you can call OpenAI like we did yesterday.And there are also frameworks that code you can run locally that will manage then calling OpenAI.And then finally you can also run managed.You can you can call a service like bedrock from Amazon or vertex from Google, vertex AI or Azure'sML from Microsoft.And these are our managed services, which themselves will run these models, and we can call them andthey will call the models.And there are some other kinds of APIs as well that we'll find out.There's one super confusingly that's called grok spelt with a Q that is different to Elon Musk's grokspelt with a k.So we'll use both grok in due course.But but watch out for that one too.That's a different thing.That is it's basically an ability to run open source models super fast on the cloud using proprietaryhardware.So we will we will look at that too.And then there's things like open Router which is very popular, which is a platform which you can connectto.And it will then route your requests to many different model providers.So all of these are different types of cloud APIs being able to call models in the cloud.And it's you can call closed source and open source models this way.But there's something else you can do as well.If you're working with open source models, then you can actually download the code and you can runthe open source model yourself.Not on the cloud, not through some API, but that you are running it.And there are many different ways of doing that as well.But there's two in particular that we'll use on this course, and they're different and it's a bit confusing.And one of them is Olama and one of them is called the Hugging Face Transformers library.The hugging face Transformers library is actually the simplest one to explain, because with that youare just literally taking the code, Python code and C++ code that was used to to write these models.It's a it's a neural network.It's a kind of data science model, which is a large language model.And you run that code yourself in your Python interpreter and and you execute it and you get the outcome.It is just taking the open source code, taking all of the weights, the billions of weights, and runningit yourself in code that is hugging face and it's hugging face is a is an open source vendor, and theTransformers library is the there a library that wraps a lot of this code that you can use very easilyyourself?So that's that's that.So then what is Olama?Well, Olama is itself a product to make it very easy to run fixed open source models in a way that'sbeen packaged and prepared so that in just a couple of steps, like we did yesterday, you can run anopen source model yourself on your computer.So it's packaged.It only takes very specific versions of models.The code has been optimized into very efficient C++, and the weights have been compressed into a specialfile called a GIF file.And you download that onto your computer.And then you run this product called Olama on your computer.And that gives you an API locally on your computer.It's like an API, but you're not calling something out on the cloud, you're just connecting to yourown computer and calling an API there.And then this Olama product is then running your model in a way that's a bit like hugging face.It's just more efficient, more packaged, and just focused on a smaller number of models.The models that you looked at yesterday in the Olama platform.So that's olama packaged, fast, efficient, uh, platform runs on your computer and provides an APIon your local computer for you to connect to.And that's the difference between those two.I hope that makes complete sense.And that is the third category direct inference of an open source model.And if you're new to the word inference, inference is a fancy name for saying running a model.Inference is what you do when you got a model and you've got some new inputs and you want to get theoutput, that is called inference.And we'll be doing a lot of that in the next eight weeks.And on the subject of inference, I think we should go and do some right now.I'm going to take you back to cursor, and we're going to do a sort of repeat of yesterday, but we'regoing to be able to do it for Olama.And I'm going to talk a bit more about what's actually happening to give you some more insight intohow this all works.Let's get to cursor back to the lab.Let's get building.


================================================================================


# 1.16 Day 2 - Chat Completions API- HTTP Endpoints vs OpenAI Python Client

Okay, well, here we are, back in cursor, one of my very favorite places to be.And we're going into the week.One directory on the left.Check the LM Engineering's in block caps here.So you're in the right project.And then go to day two, which is where we find ourselves right now.And I need to remind you before you do anything, you have to go to the top right to where it says Selectkernel for you.You click on that.You click in the Python Environments option.The first option you pick the dot VM.That's Python 3.12 that is matching this VM right here.And that means your kernel is set.Welcome to the day two lab.So I just wanted to mention here before we start that there is a page of resources on my home page andI've linked to it here.I'll try and keep adding useful resources.So do keep that bookmarked.You can go there and check it out right now if you wish.Okay okay so we're now going to go back again to doing what we did yesterday, but do it with a littlebit more discipline.And I want to tell you about this thing called the Chat Completions API.And this is the name of it's the simplest way that you can call an LLM, particularly a frontier LLM.And it's called chat completions because it's a nod to what you're actually doing.When you call an LLM, you're giving it a chat a conversation so far, and you're asking it to predictthe most likely message to come next.And you can think of that as as like completing a chat.That's what the LLM thinks it's doing.It's it's it's like predictive text.It's just trying to predict the most likely words to come next.And as a side effect of that, it happens to be really good at answering whatever questions it's asked.But all it's actually trying to do is to predict the most likely next few words, or as we'll shortlyexplain, the most likely next tokens.Okay, so it was actually this chat completions API.This this approach was invented by OpenAI first and foremost.But it was so popular the structure and style of this type of request became so popular that it's becomeubiquitous.All of the providers offer the Chat Completions API.It is the kind of standard way to interact with an LLM.And we're going to start again with OpenAI.I know some of you are antsy to get off OpenAI and use free models.Your time is coming any second now.Just watch this if you don't want to use OpenAI, and we'll get to you in a second.All right.So first of all, I'm going to do a repeat of last time where we use this thing called load dot EMV,which is going to load in the secrets in our EMV file and check that we have OpenAI key set.So this load EMV function is something which which makes which loads in anything you've saved in yourEMV.And then we'll just check that the key is good.It says API key found and it looks good so far.And if yours doesn't say that, then you know what to do.Check the EMV, check everything, and look in the look in the troubleshooting and ask me if you getstuck.Okay, let's talk about end points.Now, I imagine most of you know exactly what an end point is.It's one of those words people throw around all the time.But you might not.And for people that don't know about it, you should quickly find out.I explain it all in the Technical Foundations guide.There's nothing special at endpoint.It's an http url which you can call to, to to to make an API request by hitting some web address.And that web address would be known as an endpoint.It's a way that you have an API.But if things like API's and endpoints are new to you, then read that guide.Okay.There's an endpoint in particular which might interest you, which I want to show you right now.And it's an endpoint, an API endpoint offered by open AI.So in order to call this endpoint, which I will just do, we first have to set a couple of things whichare very common for any HTTP request.There's HTTP headers, the headers that go down in that request.You normally you probably if you know anything about web requests.You specify what content type, what type of thing you want to come back.And that's a way of saying, I want Jason to come back, please.And then this is a fairly standard way of, of, of sending down a secret in an HTTP message that authorizesyou, you put in a header called authorization and the value of that header, there's the word bearerand then a space and then some secret that identifies you to the third party.And in this case we are going to stuff the OpenAI API key.The thing that I got just here, we're going to stuff that into the header.And then I've got something here called payload.Payload is just a chunk of JSON.And it's going to be a dictionary a dictionary with two keys.One of them is model, which is going to be GPT five nano, the tiny version of GPT five.The latest model, the one of the strongest on the planet right now.And then the second field in this dictionary has a key of messages.The value is, you know, this is a list of dictionaries.It's a list you can see there.It is a list.Each dictionary, let's put that on another line, has a key role with value user a key content.And the value is tell me a fun fact okay.Nothing special here.Let's look at this.Uh, there is the payload JSON.It's is just what I said.Model GPT five nano messages, a list of dictionaries, role user content.Tell me a fun fact.Okay, that's a chunk of JSON.Uh, headers and payload.What we're now going to do is we're going to send that to an endpoint.This is the endpoint API.V1.It's chat completions.And sometimes when you make a Post request people think of that as like creating a new resource forpeople that are like rest, rest people.Uh, that's that's sometimes called creating a resource.So you could think of this as being like chat completions create some familiar.Uh, so chat completions create.We're passing in our headers with our API key.And the payload is this JSON blob right here.So let's run that.And what comes back is a bunch of JSON.We asked for JSON back.Let's see what we get back.Let's see what kind of JSON we get.Here it is.Uh so it's a chunk of JSON as like an ID object, various stuff.And then it has a field called choices and Choices.In this response, choices is a list.And the first item of that choices with index zero is something which has a field message.And that message is itself a JSON dictionary.And that has something called content, which is fun fact.There are possible unique games, more possible unique games of chess than there are atoms in the observableuniverse.About ten to the power of 120 There is a fun fact.And so that fun fact came back in the JSON response from our call to GPT five to that endpoint, thatURL that I just gave you.And so there's obviously there's another way I could do this.Let me get a new code thing here.Let's say, uh, um, we said response JSON.That was the JSON that we just were just looking at right up here.Uh, I spelt response wrong.That's that's what you get if you make me type Response.json.So we could say, okay, let's look in the choices field.I guess it knows what I'm doing here.Look at the first element in there.Element index zero at the field message at the field content okay.You see that.That's you know what that's going to do.We're just going to choices the first one message content.And let's see what that prints.It prints that that very fun fact of course that we just looked at uh, the more possible unique gamesof chess than there are atoms in the observable universe.Okay, but you know what?Uh, so so this is a perfectly good way to call OpenAI in the cloud using an HTTP endpoint.And it's fine.We could do this.We could we could always type this.But it is kind of messy fussing around with JSON, looking in dictionaries at keys and things like that.It's kind of hokey, and it would be a real pain if every time we wanted to call GPT or any frontiermodel, we had to stitch together these HTTP requests, call that slash chat completions with a postor create and then be be sort of navigating our way through JSON objects like this.It would be a pain.It would be nice if there were a better way to do it.Yes, yes, I know you get the joke.You know what I'm gonna say?There is a better way to do it.And OpenAI created this better way.They made a package called OpenAI, and that package is what's known as a Python client library.Python client library is nothing fancy at all.A Python client library that you often use for APIs all over the place for you.Name it for APIs, for sending emails, for APIs to do so much.Um, and these things are typically very lightweight libraries that manufacture an HTTP request to anendpoint.And with what comes back, it turns it into Python objects so that you're not messing around with thiskind of stuff with with these, uh, ploughing your way through JSON dictionaries.But you can just write some nice, elegant Python code, and that is all the OpenAI library is.It's a Python client library that wraps a call to an HTTP endpoint.It's very simple.It's completely open source.You can open it, look at it, look at all the code.It's perfectly simple.Some people, the first time they come across this, think that when you're dealing with OpenAI, theobject and the code, that somehow we're sort of running GPT and we have some some fancy code from OpenAI.Not at all.It's vanilla code that just wraps making a web request.And we'll we'll quickly use that code now, but it's going to be much more familiar to you.


================================================================================

